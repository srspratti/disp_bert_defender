{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFjvtMt8wsST",
    "outputId": "bb81eb3f-c2aa-406b-cd47-97a12a910d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/bert_defender_master_vastai_new_instance\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBNAxf8JxchI",
    "outputId": "b3b288fa-12b9-45e2-f7fb-0fbf908d2cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'bert_defender_master_vastai_new_instance'\n",
      "/root/bert_defender_master_vastai_new_instance\n"
     ]
    }
   ],
   "source": [
    "cd bert_defender_master_vastai_new_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haOwVP_qyA1D",
    "outputId": "d5ed96f1-a3a7-4342-ad26-6266415cf8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     bert_eval_epoches.py    enumerate_attacks.py\n",
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/                  bert_generator.py       file_utils.py\n",
      "bert_classifier.py            bert_model.py           \u001b[34;42mmodels\u001b[0m/\n",
      "bert_config.json              bert_random_attacks.py  optimization.py\n",
      "bert_defender_modified.ipynb  bert_utils.py           sample\n",
      "bert_discriminator.py         \u001b[34;42mdata\u001b[0m/                   \u001b[01;34mtmp\u001b[0m/\n",
      "bert_eval.py                  \u001b[34;42memb\u001b[0m/                    tokenization.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhj4N3zezAIk",
    "outputId": "2ab9ace5-3c89-4bbe-d7f0-08fb3f9d866d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 25 13:46:12 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:46:00.0 Off |                  N/A |\n",
      "| 30%   37C    P8     9W / 350W |      6MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   32C    P8    12W / 350W |      6MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1 MB 23 kB/s s eta 0:00:01    |███▎                            | 81.9 MB 21.4 MB/s eta 0:00:34     |██████▋                         | 165.0 MB 23.0 MB/s eta 0:00:28     |███████                         | 177.6 MB 20.6 MB/s eta 0:00:31     |████████▍                       | 210.1 MB 7.6 MB/s eta 0:01:18     |████████▋                       | 215.4 MB 7.6 MB/s eta 0:01:18     |████████▊                       | 219.9 MB 7.6 MB/s eta 0:01:17     |██████████                      | 252.8 MB 18.3 MB/s eta 0:00:31     |██████████████▉                 | 372.2 MB 22.8 MB/s eta 0:00:19     |███████████████▉                | 398.3 MB 16.0 MB/s eta 0:00:26     |██████████████████████▌         | 565.5 MB 12.5 MB/s eta 0:00:20     |███████████████████████         | 580.7 MB 10.5 MB/s eta 0:00:22     |████████████████████████        | 600.5 MB 9.6 MB/s eta 0:00:22     |█████████████████████████       | 629.5 MB 11.8 MB/s eta 0:00:15     |██████████████████████████████▍ | 764.2 MB 10.6 MB/s eta 0:00:04     |███████████████████████████████ | 781.3 MB 25.2 MB/s eta 0:00:01     |███████████████████████████████▌| 792.4 MB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.8.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-uziBBezoQ0",
    "outputId": "a1a262f6-56c9-4236-a4d3-5b925a9d0669"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9LVvTU180CDC",
    "outputId": "49a9d74c-66e9-4aef-d8e9-0452f69e0ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7L8NN5P16fx",
    "outputId": "b6c369e8-dc26-4665-83dd-04625135d455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWou01Hj-c1Z",
    "outputId": "dd28d40a-afae-4de2-fa72-516e742cf921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.17.39.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.21.0,>=1.20.39\n",
      "  Downloading botocore-1.20.39-py2.py3-none-any.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 25.7 MB/s eta 0:00:01     |█████████████████▉              | 4.1 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.6-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.39->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.39->boto3) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.39->boto3) (1.15.0)\n",
      "Building wheels for collected packages: boto3\n",
      "  Building wheel for boto3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for boto3: filename=boto3-1.17.39-py2.py3-none-any.whl size=128779 sha256=4dd7d80690e80e10ca7427c2016bd3ad5a416c38bd3a983a629a68649d180e80\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/99/a1/0dd065b28a3306b87a96ce2f63b54c16d90ba8a3ea956f8905\n",
      "Successfully built boto3\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.17.39 botocore-1.20.39 jmespath-0.10.0 s3transfer-0.3.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MHA27HN-tKP",
    "outputId": "cb3199cf-b0f7-41a0-8b7d-f53a3f4cf6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hnswlib==0.5.1\n",
      "  Downloading hnswlib-0.5.1.tar.gz (29 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hnswlib==0.5.1) (1.19.5)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.5.1-cp36-cp36m-linux_x86_64.whl size=1291241 sha256=d387cc3b27ac7db5c8d1bd587fdd03527c7648929d76df7e437484245a9909f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/4c/2a/9b6a94e03a1827a2b54493fd5cf4c3c4c351d9e4ed3a132ba4\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: hnswlib\n",
      "Successfully installed hnswlib-0.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hnswlib==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV064Cit_kxn",
    "outputId": "2217fd3c-a407-4f45-d984-9bb8137a5d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2021.3.17-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 105.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=6dabd784a918cd4e5b0f2134a242e50ab22d5c54504756cfe465a758b57f2996\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-1.0.1 nltk-3.5 regex-2021.3.17 tqdm-4.59.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 81.5 MB/s eta 0:00:01    |████████▉                       | 7.2 MB 81.5 MB/s eta 0:00:01     |██████████████████████████▍     | 21.3 MB 81.5 MB/s eta 0:00:01     |██████████████████████████████▍ | 24.5 MB 81.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=5eed6cb75e8fb1d9cf33364dfca0979bfc487040796d3727378fcd6ee41d112b\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.24.1 scipy-1.5.4 sklearn-0.0 threadpoolctl-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Mon Mar 22 19:25:35 2021\n",
      "Driver Version                            : 460.39\n",
      "CUDA Version                              : 11.2\n",
      "\n",
      "Attached GPUs                             : 2\n",
      "GPU 00000000:46:00.0\n",
      "    Product Name                          : GeForce RTX 3090\n",
      "    Product Brand                         : GeForce\n",
      "    Display Mode                          : Disabled\n",
      "    Display Active                        : Disabled\n",
      "    Persistence Mode                      : Enabled\n",
      "    MIG Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Accounting Mode                       : Disabled\n",
      "    Accounting Mode Buffer Size           : 4000\n",
      "    Driver Model\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Serial Number                         : 1324020038945\n",
      "    GPU UUID                              : GPU-8ad6666a-677c-9f0e-3b58-90220b309ddc\n",
      "    Minor Number                          : 3\n",
      "    VBIOS Version                         : 94.02.27.00.0A\n",
      "    MultiGPU Board                        : No\n",
      "    Board ID                              : 0x4600\n",
      "    GPU Part Number                       : 900-1G136-2510-000\n",
      "    Inforom Version\n",
      "        Image Version                     : G001.0000.03.03\n",
      "        OEM Object                        : 2.0\n",
      "        ECC Object                        : N/A\n",
      "        Power Management Object           : N/A\n",
      "    GPU Operation Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    GPU Virtualization Mode\n",
      "        Virtualization Mode               : None\n",
      "        Host VGPU Mode                    : N/A\n",
      "    IBMNPU\n",
      "        Relaxed Ordering Mode             : N/A\n",
      "    PCI\n",
      "        Bus                               : 0x46\n",
      "        Device                            : 0x00\n",
      "        Domain                            : 0x0000\n",
      "        Device Id                         : 0x220410DE\n",
      "        Bus Id                            : 00000000:46:00.0\n",
      "        Sub System Id                     : 0x147D10DE\n",
      "        GPU Link Info\n",
      "            PCIe Generation\n",
      "                Max                       : 3\n",
      "                Current                   : 1\n",
      "            Link Width\n",
      "                Max                       : 16x\n",
      "                Current                   : 16x\n",
      "        Bridge Chip\n",
      "            Type                          : N/A\n",
      "            Firmware                      : N/A\n",
      "        Replays Since Reset               : 0\n",
      "        Replay Number Rollovers           : 0\n",
      "        Tx Throughput                     : 0 KB/s\n",
      "        Rx Throughput                     : 0 KB/s\n",
      "    Fan Speed                             : 0 %\n",
      "    Performance State                     : P8\n",
      "    Clocks Throttle Reasons\n",
      "        Idle                              : Active\n",
      "        Applications Clocks Setting       : Not Active\n",
      "        SW Power Cap                      : Not Active\n",
      "        HW Slowdown                       : Not Active\n",
      "            HW Thermal Slowdown           : Not Active\n",
      "            HW Power Brake Slowdown       : Not Active\n",
      "        Sync Boost                        : Not Active\n",
      "        SW Thermal Slowdown               : Not Active\n",
      "        Display Clock Setting             : Not Active\n",
      "    FB Memory Usage\n",
      "        Total                             : 24268 MiB\n",
      "        Used                              : 414 MiB\n",
      "        Free                              : 23854 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 11 MiB\n",
      "        Free                              : 245 MiB\n",
      "    Compute Mode                          : Default\n",
      "    Utilization\n",
      "        Gpu                               : 0 %\n",
      "        Memory                            : 0 %\n",
      "        Encoder                           : 0 %\n",
      "        Decoder                           : 0 %\n",
      "    Encoder Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    FBC Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    Ecc Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    ECC Errors\n",
      "        Volatile\n",
      "            SRAM Correctable              : N/A\n",
      "            SRAM Uncorrectable            : N/A\n",
      "            DRAM Correctable              : N/A\n",
      "            DRAM Uncorrectable            : N/A\n",
      "        Aggregate\n",
      "            SRAM Correctable              : N/A\n",
      "            SRAM Uncorrectable            : N/A\n",
      "            DRAM Correctable              : N/A\n",
      "            DRAM Uncorrectable            : N/A\n",
      "    Retired Pages\n",
      "        Single Bit ECC                    : N/A\n",
      "        Double Bit ECC                    : N/A\n",
      "        Pending Page Blacklist            : N/A\n",
      "    Remapped Rows                         : N/A\n",
      "    Temperature\n",
      "        GPU Current Temp                  : 22 C\n",
      "        GPU Shutdown Temp                 : 98 C\n",
      "        GPU Slowdown Temp                 : 95 C\n",
      "        GPU Max Operating Temp            : 93 C\n",
      "        GPU Target Temperature            : 75 C\n",
      "        Memory Current Temp               : N/A\n",
      "        Memory Max Operating Temp         : N/A\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 16.14 W\n",
      "        Power Limit                       : 375.00 W\n",
      "        Default Power Limit               : 350.00 W\n",
      "        Enforced Power Limit              : 375.00 W\n",
      "        Min Power Limit                   : 100.00 W\n",
      "        Max Power Limit                   : 400.00 W\n",
      "    Clocks\n",
      "        Graphics                          : 0 MHz\n",
      "        SM                                : 0 MHz\n",
      "        Memory                            : 405 MHz\n",
      "        Video                             : 555 MHz\n",
      "    Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Default Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Max Clocks\n",
      "        Graphics                          : 2100 MHz\n",
      "        SM                                : 2100 MHz\n",
      "        Memory                            : 9751 MHz\n",
      "        Video                             : 1950 MHz\n",
      "    Max Customer Boost Clocks\n",
      "        Graphics                          : N/A\n",
      "    Clock Policy\n",
      "        Auto Boost                        : N/A\n",
      "        Auto Boost Default                : N/A\n",
      "    Processes                             : None\n",
      "\n",
      "GPU 00000000:81:00.0\n",
      "    Product Name                          : GeForce RTX 3090\n",
      "    Product Brand                         : GeForce\n",
      "    Display Mode                          : Disabled\n",
      "    Display Active                        : Disabled\n",
      "    Persistence Mode                      : Enabled\n",
      "    MIG Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Accounting Mode                       : Disabled\n",
      "    Accounting Mode Buffer Size           : 4000\n",
      "    Driver Model\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Serial Number                         : 1323920020675\n",
      "    GPU UUID                              : GPU-d45bf8ec-5a88-7fa3-7baa-c0196ebd87c8\n",
      "    Minor Number                          : 2\n",
      "    VBIOS Version                         : 94.02.27.00.0A\n",
      "    MultiGPU Board                        : No\n",
      "    Board ID                              : 0x8100\n",
      "    GPU Part Number                       : 900-1G136-2510-000\n",
      "    Inforom Version\n",
      "        Image Version                     : G001.0000.03.03\n",
      "        OEM Object                        : 2.0\n",
      "        ECC Object                        : N/A\n",
      "        Power Management Object           : N/A\n",
      "    GPU Operation Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    GPU Virtualization Mode\n",
      "        Virtualization Mode               : None\n",
      "        Host VGPU Mode                    : N/A\n",
      "    IBMNPU\n",
      "        Relaxed Ordering Mode             : N/A\n",
      "    PCI\n",
      "        Bus                               : 0x81\n",
      "        Device                            : 0x00\n",
      "        Domain                            : 0x0000\n",
      "        Device Id                         : 0x220410DE\n",
      "        Bus Id                            : 00000000:81:00.0\n",
      "        Sub System Id                     : 0x147D10DE\n",
      "        GPU Link Info\n",
      "            PCIe Generation\n",
      "                Max                       : 3\n",
      "                Current                   : 1\n",
      "            Link Width\n",
      "                Max                       : 16x\n",
      "                Current                   : 16x\n",
      "        Bridge Chip\n",
      "            Type                          : N/A\n",
      "            Firmware                      : N/A\n",
      "        Replays Since Reset               : 0\n",
      "        Replay Number Rollovers           : 0\n",
      "        Tx Throughput                     : 0 KB/s\n",
      "        Rx Throughput                     : 0 KB/s\n",
      "    Fan Speed                             : 0 %\n",
      "    Performance State                     : P8\n",
      "    Clocks Throttle Reasons\n",
      "        Idle                              : Active\n",
      "        Applications Clocks Setting       : Not Active\n",
      "        SW Power Cap                      : Not Active\n",
      "        HW Slowdown                       : Not Active\n",
      "            HW Thermal Slowdown           : Not Active\n",
      "            HW Power Brake Slowdown       : Not Active\n",
      "        Sync Boost                        : Not Active\n",
      "        SW Thermal Slowdown               : Not Active\n",
      "        Display Clock Setting             : Not Active\n",
      "    FB Memory Usage\n",
      "        Total                             : 24268 MiB\n",
      "        Used                              : 414 MiB\n",
      "        Free                              : 23854 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 7 MiB\n",
      "        Free                              : 249 MiB\n",
      "    Compute Mode                          : Default\n",
      "    Utilization\n",
      "        Gpu                               : 0 %\n",
      "        Memory                            : 0 %\n",
      "        Encoder                           : 0 %\n",
      "        Decoder                           : 0 %\n",
      "    Encoder Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    FBC Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    Ecc Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    ECC Errors\n",
      "        Volatile\n",
      "            SRAM Correctable              : N/A\n",
      "            SRAM Uncorrectable            : N/A\n",
      "            DRAM Correctable              : N/A\n",
      "            DRAM Uncorrectable            : N/A\n",
      "        Aggregate\n",
      "            SRAM Correctable              : N/A\n",
      "            SRAM Uncorrectable            : N/A\n",
      "            DRAM Correctable              : N/A\n",
      "            DRAM Uncorrectable            : N/A\n",
      "    Retired Pages\n",
      "        Single Bit ECC                    : N/A\n",
      "        Double Bit ECC                    : N/A\n",
      "        Pending Page Blacklist            : N/A\n",
      "    Remapped Rows                         : N/A\n",
      "    Temperature\n",
      "        GPU Current Temp                  : 24 C\n",
      "        GPU Shutdown Temp                 : 98 C\n",
      "        GPU Slowdown Temp                 : 95 C\n",
      "        GPU Max Operating Temp            : 93 C\n",
      "        GPU Target Temperature            : 75 C\n",
      "        Memory Current Temp               : N/A\n",
      "        Memory Max Operating Temp         : N/A\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 13.63 W\n",
      "        Power Limit                       : 375.00 W\n",
      "        Default Power Limit               : 350.00 W\n",
      "        Enforced Power Limit              : 375.00 W\n",
      "        Min Power Limit                   : 100.00 W\n",
      "        Max Power Limit                   : 400.00 W\n",
      "    Clocks\n",
      "        Graphics                          : 0 MHz\n",
      "        SM                                : 0 MHz\n",
      "        Memory                            : 405 MHz\n",
      "        Video                             : 555 MHz\n",
      "    Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Default Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Max Clocks\n",
      "        Graphics                          : 2100 MHz\n",
      "        SM                                : 2100 MHz\n",
      "        Memory                            : 9751 MHz\n",
      "        Video                             : 1950 MHz\n",
      "    Max Customer Boost Clocks\n",
      "        Graphics                          : N/A\n",
      "    Clock Policy\n",
      "        Auto Boost                        : N/A\n",
      "        Auto Boost Default                : N/A\n",
      "    Processes                             : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIO5U77A1QcJ",
    "outputId": "3653c367-8f1e-49ac-cbb9-bcbdd2ef4d30",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/29/2021 02:40:06 - INFO - bert_utils -   device: cpu , distributed training: False, 16-bits training: False\n",
      "03/29/2021 02:40:07 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   *** Example ***\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   tokens: apparently reassembled from the cutting-room floor of any given daytime soap .\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   *** Example ***\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   tokens: `` they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes . ''\n",
      "03/29/2021 02:40:07 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 26 30 14 31 32 4 33 34 35 7 36 37 38 39 40 41 42 43 12 44 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -   ***** Running training *****\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -     Num examples = 6919\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -     Num token vocab = 14815\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -     Batch size = 8\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -     Num steps = 21600\n",
      "03/29/2021 02:40:08 - INFO - bert_utils -   Loading word embeddings ... \n",
      "03/29/2021 02:40:19 - INFO - bert_utils -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/29/2021 02:40:19 - INFO - bert_utils -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp32epdqan\n",
      "03/29/2021 02:40:22 - INFO - bert_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/29/2021 02:40:25 - INFO - bert_utils -   Weights of BertForDiscriminator not initialized from pretrained model: ['discriminator.weight', 'discriminator.bias']\n",
      "03/29/2021 02:40:25 - INFO - bert_utils -   Weights from pretrained model not used in BertForDiscriminator: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/865 [00:00<?, ?it/s]\u001b[A/root/bert_defender_master_vastai_new_instance/optimization.py:132: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "\n",
      "Iteration:   0%|                              | 1/865 [00:08<1:55:54,  8.05s/it]\u001b[A\n",
      "Iteration:   0%|                              | 2/865 [00:16<1:56:29,  8.10s/it]\u001b[A\n",
      "Iteration:   0%|                              | 3/865 [00:24<1:56:00,  8.07s/it]\u001b[A\n",
      "Iteration:   0%|▏                             | 4/865 [00:32<1:55:33,  8.05s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 5/865 [00:40<1:55:29,  8.06s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 6/865 [00:48<1:54:51,  8.02s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 7/865 [00:56<1:54:46,  8.03s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 8/865 [01:04<1:54:24,  8.01s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 9/865 [01:12<1:54:12,  8.01s/it]\u001b[A\n",
      "Iteration:   1%|▎                            | 10/865 [01:20<1:54:13,  8.02s/it]\u001b[A\n",
      "Iteration:   1%|▎                            | 11/865 [01:28<1:54:02,  8.01s/it]\u001b[A\n",
      "Iteration:   1%|▍                            | 12/865 [01:36<1:53:44,  8.00s/it]\u001b[A\n",
      "Iteration:   2%|▍                            | 13/865 [01:44<1:53:23,  7.99s/it]\u001b[A\n",
      "Iteration:   2%|▍                            | 14/865 [01:52<1:53:21,  7.99s/it]\u001b[A\n",
      "Iteration:   2%|▌                            | 15/865 [02:00<1:53:25,  8.01s/it]\u001b[A\n",
      "Iteration:   2%|▌                            | 16/865 [02:08<1:53:09,  8.00s/it]\u001b[A\n",
      "Iteration:   2%|▌                            | 17/865 [02:16<1:53:14,  8.01s/it]\u001b[A\n",
      "Iteration:   2%|▌                            | 18/865 [02:24<1:53:15,  8.02s/it]\u001b[A\n",
      "Iteration:   2%|▋                            | 19/865 [02:32<1:53:09,  8.03s/it]\u001b[A\n",
      "Iteration:   2%|▋                            | 20/865 [02:40<1:53:17,  8.04s/it]\u001b[A\n",
      "Iteration:   2%|▋                            | 21/865 [02:48<1:53:04,  8.04s/it]\u001b[A\n",
      "Iteration:   3%|▋                            | 22/865 [02:56<1:52:56,  8.04s/it]\u001b[A\n",
      "Iteration:   3%|▊                            | 23/865 [03:04<1:52:44,  8.03s/it]\u001b[A\n",
      "Iteration:   3%|▊                            | 24/865 [03:12<1:52:26,  8.02s/it]\u001b[A\n",
      "Iteration:   3%|▊                            | 25/865 [03:20<1:52:23,  8.03s/it]\u001b[A\n",
      "Iteration:   3%|▊                            | 26/865 [03:28<1:52:09,  8.02s/it]\u001b[A\n",
      "Iteration:   3%|▉                            | 27/865 [03:36<1:52:01,  8.02s/it]\u001b[A\n",
      "Iteration:   3%|▉                            | 28/865 [03:44<1:52:01,  8.03s/it]\u001b[A\n",
      "Iteration:   3%|▉                            | 29/865 [03:52<1:51:53,  8.03s/it]\u001b[A\n",
      "Iteration:   3%|█                            | 30/865 [04:00<1:51:55,  8.04s/it]\u001b[A\n",
      "Iteration:   4%|█                            | 31/865 [04:08<1:51:52,  8.05s/it]\u001b[A\n",
      "Iteration:   4%|█                            | 32/865 [04:16<1:51:33,  8.04s/it]\u001b[A\n",
      "Iteration:   4%|█                            | 33/865 [04:24<1:51:18,  8.03s/it]\u001b[A\n",
      "Iteration:   4%|█▏                           | 34/865 [04:32<1:50:55,  8.01s/it]\u001b[A\n",
      "Iteration:   4%|█▏                           | 35/865 [04:40<1:50:42,  8.00s/it]\u001b[A\n",
      "Iteration:   4%|█▏                           | 36/865 [04:48<1:50:39,  8.01s/it]\u001b[A\n",
      "Iteration:   4%|█▏                           | 37/865 [04:56<1:50:33,  8.01s/it]\u001b[A\n",
      "Iteration:   4%|█▎                           | 38/865 [05:04<1:50:53,  8.05s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 39/865 [05:13<1:51:02,  8.07s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 40/865 [05:21<1:50:52,  8.06s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 41/865 [05:29<1:50:52,  8.07s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 42/865 [05:37<1:50:56,  8.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 43/865 [05:45<1:50:06,  8.04s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 44/865 [05:53<1:49:54,  8.03s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 45/865 [06:01<1:49:59,  8.05s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 46/865 [06:09<1:49:32,  8.03s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 47/865 [06:17<1:49:21,  8.02s/it]\u001b[A\n",
      "Iteration:   6%|█▌                           | 48/865 [06:25<1:49:25,  8.04s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 49/865 [06:33<1:49:16,  8.03s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 50/865 [06:41<1:49:02,  8.03s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 51/865 [06:49<1:48:50,  8.02s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 52/865 [06:57<1:48:45,  8.03s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 53/865 [07:05<1:48:33,  8.02s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 54/865 [07:13<1:48:30,  8.03s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 55/865 [07:21<1:48:32,  8.04s/it]\u001b[A\n",
      "Iteration:   6%|█▉                           | 56/865 [07:29<1:48:36,  8.06s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   7%|█▉                           | 57/865 [07:37<1:48:34,  8.06s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   7%|█▉                           | 58/865 [07:45<1:48:25,  8.06s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 59/865 [07:53<1:48:23,  8.07s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 60/865 [08:02<1:48:13,  8.07s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   7%|██                           | 61/865 [08:09<1:47:39,  8.03s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   7%|██                           | 62/865 [08:17<1:47:03,  8.00s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 63/865 [08:25<1:47:03,  8.01s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   7%|██▏                          | 64/865 [08:34<1:47:23,  8.04s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▏                          | 65/865 [08:42<1:47:37,  8.07s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▏                          | 66/865 [08:50<1:47:29,  8.07s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 67/865 [08:58<1:47:09,  8.06s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▎                          | 68/865 [09:06<1:47:10,  8.07s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▎                          | 69/865 [09:14<1:47:14,  8.08s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▎                          | 70/865 [09:22<1:47:06,  8.08s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▍                          | 71/865 [09:30<1:47:19,  8.11s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Iteration:   8%|██▍                          | 72/865 [09:38<1:46:47,  8.08s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Discriminator train \n",
    "!python bert_discriminator.py \\\n",
    "--task_name sst-2\\\n",
    "--do_train\\\n",
    "--do_lower_case\\\n",
    "--data_dir data/sst-2/\\\n",
    "--bert_model bert-base-uncased\\\n",
    "--max_seq_length 128\\\n",
    "--train_batch_size 8\\\n",
    "--learning_rate 2e-5\\\n",
    "--num_train_epochs 25\\\n",
    "--output_dir ./tmp/disc/\n",
    "#--no_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/28/2021 17:59:16 - INFO - bert_utils -   device: cpu n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "03/28/2021 17:59:17 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/28/2021 17:59:17 - INFO - bert_utils -   loading embeddings ... \n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   loading p index ...\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   tokens: that loves its characters and communicates something rather beautiful about human nature\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 7459, 2049, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 2008, 103, 2049, 3494, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2008, 7459, 103, 3494, 1998, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [7459, 2049, 103, 1998, 10639, 2015, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2049, 3494, 103, 10639, 2015, 2242, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3494, 1998, 103, 2242, 2738, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1998, 10639, 2015, 103, 2738, 3376, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [10639, 2015, 2242, 103, 3376, 2055, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2242, 2738, 103, 2055, 2529, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2738, 3376, 103, 2529, 3267, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3376, 2055, 103, 3267, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2055, 2529, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the same throughout\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 12580, 8510, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 3464, 103, 8510, 2000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3464, 12580, 103, 2000, 3961, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [12580, 8510, 103, 3961, 1996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [8510, 2000, 103, 1996, 2168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2000, 3961, 103, 2168, 2802, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3961, 1996, 103, 2802, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1996, 2168, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   tokens: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 21 18 22 23 24 18 25 26 27 28\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 1996, 5409, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 2006, 103, 5409, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 0, 0, 0, 0] [2006, 1996, 103, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 0, 0] [1996, 5409, 103, 18856, 17322, 2015, 1996, 0, 0, 0, 0, 0, 0, 0, 0, 0] [5409, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 103, 1996, 16587, 0, 0, 0, 0] [7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 103, 16587, 2071, 0, 0] [18856, 17322, 2015, 1996, 103, 2071, 2852, 24225, 0, 0, 0, 0, 0, 0, 0, 0] [1996, 16587, 103, 2852, 24225, 2039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [16587, 2071, 103, 2039, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2071, 2852, 24225, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 21 18 22 23 24 18 25 26 27 28 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -   ***** Running training *****\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -     Num examples = 5\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -     Num token vocab = 59\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -     Batch size = 8\n",
      "03/28/2021 17:59:27 - INFO - bert_utils -     Num steps = 0\n",
      "train_features:  <class 'list'>\n",
      "train_features 1st element :  64\n",
      "train_features length:  5\n",
      "Type of ngram embeddings <class 'list'>\n",
      "args max ngram length:  64\n",
      "length of ngram_embeddings list:  5\n",
      "printing it:    64\n",
      "printing it:    64\n",
      "printing it:    63\n",
      "printing it:  [[0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0.0109, 0.0054, 0.0025, -0.063, -0.0564, 0.0296, -0.0682, 0.0355, -0.0558, 0.0059, -0.0383, 0.0515, 0.0712, -0.0886, 0.1198, -0.0334, 0.0043, 0.0388, 0.0457, -0.0428, 0.0495, -0.0072, 0.0304, -0.0174, 0.0421, 0.0032, -0.039, -0.0322, -0.0113, 0.0603, 0.0074, 0.0102, 0.0481, -0.0294, 0.0271, 0.005, -0.0374, 0.0533, -0.1492, 0.0262, 0.0029, -0.0423, 0.0086, -0.0379, -0.0297, -0.0117, 0.0891, -0.5834, -0.0196, 0.038, 0.0106, -0.023, -0.028, 0.0464, 0.009, -0.0389, -0.033, -0.1084, 0.0001, 0.0224, 0.1006, 0.0005, 0.0152, -0.0146, -0.0575, -0.0436, -0.0181, -0.0158, -0.0427, 0.0685, -0.0116, 0.029, 0.0035, -0.1194, -0.0918, 0.0197, 0.1237, -0.0245, -0.3306, -0.0037, -0.0307, -0.025, 0.0059, 0.1157, -0.0114, -0.0106, -0.0137, 0.0109, 0.0819, 0.052, 0.0564, 0.0283, -0.0403, 0.0134, -0.0875, -0.0785, -0.0019, 0.0188, 0.0821, -0.095, -0.0039, 0.0945, -0.0765, 0.1313, -0.0581, 0.0405, -0.0103, 0.0653, 0.0289, 0.0409, -0.0256, -0.0071, 0.0012, -0.0567, -0.2729, 0.0702, -0.0213, 0.0021, 0.1257, 0.0059, 0.0772, 0.0403, -0.0276, -0.101, -0.0033, -0.0749, -0.0207, -0.0789, 0.0385, -0.0658, -0.1691, -0.0115, -0.0056, 0.1989, 0.1064, -0.0167, 0.0653, 0.0628, 0.3884, -0.0158, -0.0042, 0.0209, -0.0511, -0.0213, -0.0188, -0.009, 0.0099, -0.023, -0.0427, 0.0403, 0.0124, -0.0669, -0.0269, -0.0124, 0.0256, -0.041, -0.0619, 0.0162, -0.1176, -0.2902, 0.0084, 0.1346, -0.0781, 0.0389, -0.0321, -0.0553, -0.0213, -0.0142, 0.0333, 0.0863, -0.1304, 0.1478, -0.5894, 0.0465, 0.0001, 0.0436, -0.0236, 0.0123, 0.0241, 0.02, -0.2658, -0.0006, -0.0608, 0.3386, -0.0067, 0.0549, 0.0206, -0.0162, 0.0078, -0.0411, 0.0081, 0.0067, -0.0451, 0.1383, 0.0427, -0.0006, -0.0881, 0.0111, 0.0129, -0.0609, -0.0975, -0.0001, 0.1469, 0.0451, 0.0174, -0.0055, -0.0166, -0.0333, 0.0994, 0.055, -0.0201, -0.333, -0.0084, 0.155, -0.0631, 0.0423, 0.0196, -0.0049, 0.0698, -0.0575, -0.1492, 0.026, 0.0842, -0.0507, 0.1491, 0.039, 0.0039, 0.2166, -0.0059, 0.013, -0.0529, -0.0177, 0.1542, -0.1744, 0.0889, -0.074, -0.0335, 0.0167, -0.0347, 0.0433, -0.064, 0.0335, -0.0394, 0.0444, 0.3293, -0.0081, -0.0297, 0.0637, 0.0484, -0.3762, 0.0191, -0.0618, 0.0386, 0.0234, 0.0003, 0.0235, 0.033, 0.0034, -0.0156, -0.1348, 0.0016, 0.0838, -0.0008, -0.0768, -0.0085, -0.0156, 0.0373, -0.0186, 0.0309, 0.0197, -0.0242, 0.042, 0.0556, 0.0171, 0.0195, -0.1021, 0.0454, -0.031, -0.0079, 0.0583, -0.0137, -0.0559, -0.0094, 0.0233, 0.04, 0.0042, -0.0218, -0.0945, -0.0154, -0.029, -0.0141, 0.008, 0.1221, 0.0558, -0.1314], [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259], [0.0143, -0.0543, -0.1051, 0.0771, -0.0517, 0.159, 0.0023, 0.0041, 0.1165, -0.2022, -0.0472, -0.094, 0.0627, -0.0514, 0.1361, -0.0619, 0.0916, -0.1894, 0.3032, -0.0725, -0.0115, -0.0403, 0.0834, 0.0448, 0.0543, 0.1341, 0.0924, -0.1008, 0.0358, 0.0204, 0.0382, -0.1205, -0.1139, 0.0288, 0.1009, 0.084, -0.0333, -0.1662, 0.0438, -0.0211, 0.119, -0.0011, -0.1672, -0.1107, -0.0335, 0.0294, 0.0254, -0.018, -0.0518, 0.0907, 0.196, 0.0684, -0.7173, -0.1312, 0.0114, -0.0293, 0.0415, -0.0909, -0.0204, 0.019, -0.0496, 0.0192, 0.0837, -0.018, -0.0228, -0.2671, 0.1097, 0.1041, -0.0912, -0.0907, 0.001, -0.0216, -0.0183, -0.1755, 0.0833, 0.0609, 0.1786, -0.1091, -0.1792, 0.0464, -0.033, 0.0056, -0.062, -0.2019, -0.0117, -0.1002, -0.0035, -0.1013, 0.148, -0.0883, 0.0249, 0.0818, -0.1695, -0.1593, -0.0177, 0.0438, 0.018, -0.0386, 0.0306, 0.0984, -0.178, 0.1209, 0.0429, 0.0647, 0.0041, -0.0447, 0.2673, 0.0551, -0.0338, 0.0052, -0.2155, 0.1378, -0.071, -0.1129, -0.082, -0.1429, -0.1168, 0.1869, 0.0499, -0.2497, 0.0585, 0.0677, -0.1465, 0.1205, 0.1833, 0.2309, -0.1143, 0.1156, -0.0714, 0.1972, 0.0479, -0.0762, 0.0756, 0.1547, 0.1854, -0.4435, -0.0086, 0.0655, -0.0631, -0.2148, 0.0738, 0.1081, -0.1484, 0.2601, 0.0103, -0.1344, 0.0201, -0.0311, -0.0352, 0.069, 0.102, -0.1283, -0.0474, 0.1745, 0.0474, 0.1155, -0.0341, 0.2, -0.0765, 0.005, -0.0102, 0.0832, -0.0961, 0.03, 0.0569, 0.1179, 0.0911, -0.1376, 0.0509, -0.1254, 0.2475, -0.1036, -0.151, -0.0602, -0.0197, -0.0109, 0.2656, 0.1874, -0.124, -0.125, 0.0859, 0.0337, -0.145, -0.0752, -0.0396, 0.0002, -0.1337, -0.0621, 0.1229, 0.0378, 0.0574, 0.1162, -0.0134, -0.0242, -0.0272, 0.0871, -0.0428, -0.0971, 0.1347, 0.0422, -0.1023, 0.1704, -0.0811, 0.1585, 0.2222, 0.1999, 0.2117, -0.0486, 0.0009, 0.0811, 0.041, 0.0708, -0.0067, -0.0618, -0.0936, -0.0176, -0.103, -0.0052, 0.1627, 0.079, 0.082, -0.2438, -0.1182, -0.016, 0.1215, -0.0091, -0.0517, -0.03, -0.0589, -0.2315, -0.1286, -0.1199, 0.2866, 0.0122, 0.1046, 0.0232, 0.0164, -0.0132, -0.2337, -0.0218, -0.0405, -0.019, -0.0144, 0.0227, 0.0262, -0.0486, 0.0407, 0.0409, -0.0732, 0.4246, 0.103, 0.0808, -0.1494, 0.1471, 0.2057, -0.0533, -0.1855, 0.0616, 0.0348, 0.121, 0.271, -0.0521, 0.1651, 0.025, -0.0339, -0.07, 0.1155, -0.0905, -0.2679, 0.0552, -0.0485, 0.0351, 0.0582, -0.0744, -0.0191, 0.0294, 0.0786, -0.0611, -0.0638, 0.1417, 0.0177, -0.1093, -0.0368, -0.1943, -0.024, 0.1895, 0.088, -0.0517, -0.2148, 0.0574, 0.0077, 0.1147, 0.0485, -0.05, 0.176, -0.1182, 0.0062, 0.1549, -0.0883, -0.0034], [-0.1516, 0.0261, -0.1214, -0.0554, 0.1189, -0.0318, 0.0663, 0.007, 0.0326, 0.2224, -0.1209, -0.023, -0.0409, -0.1631, 0.1822, -0.1053, 0.127, -0.0679, 0.0279, -0.0594, -0.2264, 0.1447, 0.1304, -0.0832, 0.0398, 0.1593, -0.0254, 0.0029, -0.0111, 0.0802, -0.0959, -0.0435, 0.1217, -0.0023, 0.0125, 0.0479, -0.1467, -0.0639, 0.0946, 0.0305, -0.0139, -0.0694, -0.0195, -0.0004, -0.0273, -0.1561, 0.0345, -0.0749, 0.1438, 0.0614, -0.0707, -0.0107, -0.6549, 0.2256, 0.0823, 0.0814, -0.1385, -0.3124, 0.2189, 0.0438, 0.2011, 0.0777, -0.1083, -0.1452, 0.1372, 0.0356, -0.1708, -0.0939, -0.1239, -0.1943, -0.053, -0.0218, 0.1476, -0.2356, 0.0103, -0.0558, -0.0901, -0.0388, 0.0276, -0.0392, 0.0675, 0.0791, -0.1464, -0.1893, -0.0815, 0.0636, -0.0182, -0.2243, 0.2415, 0.0856, -0.009, -0.0234, 0.1402, -0.0363, -0.042, -0.3344, 0.2943, 0.0946, -0.2424, 0.0413, 0.1328, 0.181, -0.0299, 0.1109, 0.0168, 0.2168, -0.0956, 0.0227, 0.0275, 0.2631, -0.0734, 0.1344, -0.18, -0.2053, 0.1573, 0.0621, -0.0574, 0.0598, 0.1778, -0.4325, 0.0107, -0.0715, -0.2109, 0.1728, 0.0044, -0.2923, -0.1781, -0.0571, -0.0941, 0.1887, -0.0802, -0.1051, 0.1524, 0.0908, -0.0678, -0.0649, 0.1076, -0.0581, 0.213, -0.0256, 0.196, -0.1287, -0.1607, 0.0976, -0.1216, 0.0457, -0.0005, -0.1337, 0.0939, 0.081, -0.0775, -0.0077, -0.1135, 0.0251, -0.0352, 0.0153, 0.1876, 0.2257, 0.1956, -0.0751, 0.2021, -0.1221, 0.0264, 0.1743, 0.2078, -0.0412, 0.0684, 0.1525, -0.3519, -0.1587, -0.1308, 0.0614, -0.0619, 0.0761, 0.0409, -0.0189, 0.2209, -0.1827, -0.1356, 0.2158, 0.1883, 0.0224, -0.0051, -0.02, 0.0268, 0.0857, -0.1256, -0.0379, -0.2222, 0.0388, -0.2013, -0.1436, -0.2269, 0.1731, 0.0511, 0.3046, 0.0562, -0.1082, 0.2207, -0.0739, -0.1245, 0.2508, 0.0067, -0.1141, 0.2315, 0.0051, -0.0013, 0.0404, -0.1076, -0.0516, 0.0938, 0.1965, -0.1102, -0.0337, 0.1085, -0.0337, -0.2225, 0.0302, -0.1936, -0.199, 0.1831, 0.0049, 0.0237, -0.2115, -0.122, -0.1782, 0.0604, -0.1211, 0.0262, -0.1189, -0.2392, -0.0616, 0.3307, -0.3224, 0.0041, -0.0311, 0.0268, -0.0678, -0.2845, 0.0837, -0.0811, -0.0565, 0.1536, -0.007, 0.009, -0.0248, -0.0004, 0.102, -0.1167, 0.3847, 0.0886, 0.0723, 0.1374, 0.0931, 0.0807, -0.2107, -0.1093, 0.1742, 0.0211, 0.0566, 0.015, 0.114, 0.0128, -0.0062, -0.7042, 0.0187, -0.01, -0.0832, -0.1644, 0.1549, -0.2233, 0.0695, -0.0517, 0.0135, 0.0209, 0.0536, -0.1664, -0.0425, 0.1485, 0.0098, 0.222, -0.0455, -0.0158, -0.2935, 0.0842, 0.0782, 0.0276, 0.0811, 0.1665, -0.0177, -0.009, 0.2296, -0.0601, 0.0667, -0.0205, 0.0278, 0.0135, 0.1152, 0.068, -0.1444], [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259], [-0.0455, 0.2169, -0.045, -0.1449, 0.0315, -0.0419, 0.2311, 0.2965, 0.0666, 0.0629, 0.0198, 0.113, -0.2449, 0.0175, 0.0883, 0.0854, 0.0701, -0.1096, -0.028, -0.1094, -0.3251, 0.0504, -0.0819, 0.0199, -0.0759, -0.0156, 0.0021, 0.2765, -0.0474, -0.1035, 0.0427, -0.0551, 0.0641, -0.0611, -0.0518, 0.1525, -0.0606, 0.0785, 0.0682, 0.1768, -0.0766, 0.0349, 0.0267, 0.0824, 0.0422, 0.0942, -0.0785, -0.075, 0.1675, -0.0544, 0.2628, 0.0023, -0.6394, 0.1559, 0.1765, 0.0651, -0.0255, 0.0073, 0.0251, 0.0821, -0.0506, 0.0795, 0.0042, -0.0591, -0.0546, 0.0919, -0.1361, 0.1442, -0.0495, -0.1155, 0.0202, 0.0177, 0.0534, 0.0331, -0.0241, -0.2127, 0.1117, -0.0832, -0.0365, 0.2927, -0.0237, 0.147, -0.0489, -0.2541, 0.2297, 0.0248, 0.0782, -0.2193, 0.072, -0.084, -0.1034, -0.1727, -0.0788, 0.0693, -0.0286, -0.1217, 0.0104, -0.0005, 0.0687, 0.0051, -0.1218, -0.1055, -0.103, 0.1064, -0.253, 0.074, -0.0645, 0.0713, 0.0915, 0.0479, -0.0029, 0.0341, -0.0275, -0.1296, -0.0062, -0.0279, -0.0489, -0.0853, 0.0262, -0.369, 0.0512, -0.2761, 0.0339, 0.0558, -0.0562, 0.1133, -0.123, 0.0184, 0.1064, -0.1874, -0.0688, 0.1317, -0.0355, -0.0461, -0.1048, 0.0028, -0.0095, -0.0501, 0.0169, -0.066, 0.0848, 0.0103, -0.1681, 0.0017, 0.0057, -0.1019, -0.0982, 0.0543, 0.1098, 0.2491, -0.0535, 0.0186, 0.0757, -0.0243, 0.1771, 0.0988, 0.0174, -0.0504, 0.1588, -0.1371, 0.1396, -0.2422, -0.0725, 0.0042, 0.016, 0.0036, 0.0601, -0.0851, -0.1195, -0.2016, -0.0865, 0.0556, -0.0251, 0.0856, -0.058, -0.0954, 0.2181, 0.0978, 0.1717, -0.048, 0.0292, 0.1121, -0.1031, -0.0349, 0.1859, 0.0179, -0.0794, 0.1225, -0.3095, 0.0048, -0.1176, -0.0507, -0.1813, 0.0849, 0.0489, 0.3092, 0.1104, 0.106, 0.1833, -0.1318, -0.1152, 0.0651, 0.2678, -0.0722, 0.0947, -0.1414, 0.0365, -0.1402, -0.009, 0.0396, -0.0218, -0.0268, 0.0875, 0.0372, -0.045, 0.1391, -0.0775, 0.1772, -0.3194, -0.083, 0.072, -0.0003, 0.075, -0.0854, 0.0564, 0.1475, -0.1642, 0.0939, -0.0234, -0.1082, 0.0726, 0.004, 0.2956, -0.07, -0.1524, -0.0799, 0.0644, 0.1934, -0.2858, 0.1708, 0.0498, -0.1527, -0.0686, 0.1355, 0.2039, -0.0426, 0.0943, 0.1789, 0.0321, 0.3679, 0.174, 0.1584, -0.1019, -0.1073, 0.4471, 0.2862, -0.0626, 0.0859, -0.1308, 0.2145, -0.0207, -0.0537, -0.079, 0.0873, -0.7401, 0.1595, -0.039, -0.0173, -0.0115, 0.0917, -0.1477, 0.006, 0.1513, -0.0682, -0.0532, 0.0731, -0.0389, -0.0997, -0.0045, 0.0345, -0.0181, 0.0577, -0.1213, -0.1275, -0.025, 0.0256, -0.0919, 0.1334, -0.0633, 0.1814, -0.0024, 0.0277, -0.0574, -0.2292, -0.0546, -0.0158, -0.1111, 0.0665, -0.0223, -0.115], [-0.0452, -0.1182, -0.0588, -0.0678, 0.0659, -0.0131, 0.0226, 0.0341, 0.0073, -0.0948, 0.0425, 0.1141, -0.0269, 0.0057, -0.0998, 0.0446, -0.0107, -0.046, -0.0858, 0.1738, -0.013, 0.0453, -0.1062, 0.1329, 0.0352, 0.0535, 0.0053, 0.1091, -0.0301, -0.0369, -0.0765, -0.0133, -0.0308, 0.1176, 0.0911, 0.0312, 0.049, -0.0843, 0.1036, -0.0138, 0.0216, -0.0034, 0.0214, -0.0578, -0.1358, 0.0589, 0.0092, 0.0299, 0.0109, -0.0453, 0.0458, 0.0886, -0.5181, 0.0412, 0.0049, -0.0017, 0.0471, -0.1153, -0.2281, -0.1125, 0.0473, -0.0311, -0.0006, -0.0075, -0.1309, -0.0087, 0.0068, 0.0634, 0.0376, 0.0001, 0.0125, -0.0212, -0.0006, -0.1096, -0.056, 0.0224, -0.0342, 0.06, -0.0298, 0.2647, 0.0342, 0.0083, -0.006, -0.3098, -0.0, 0.0341, -0.0454, 0.0164, -0.5405, 0.0691, -0.0402, 0.0763, -0.0947, -0.0367, -0.0085, 0.0024, 0.0071, -0.1708, -0.0697, -0.0473, -0.1802, -0.0478, 0.002, 0.0089, 0.0125, 0.0277, -0.1764, 0.0552, 0.0754, -0.0075, -0.0053, 0.0124, -0.013, 0.0183, -0.2257, -0.004, 0.1549, 0.0242, -0.0479, -0.2813, -0.0232, -0.0071, 0.0019, 0.0709, 0.0374, 0.2256, -0.1018, 0.232, -0.037, 0.0355, -0.0028, -0.0077, -0.0419, 0.0492, 0.0353, -0.2993, -0.0554, -0.0579, -0.0191, -0.0706, 0.006, -0.0416, 0.0982, 0.3701, 0.004, 0.1287, 0.1455, 0.0185, -0.0102, -0.1441, -0.1069, 0.0642, 0.087, 0.0936, 0.0824, -0.042, -0.1653, 0.0034, -0.0284, -0.0616, 0.0449, 0.0362, 0.0063, -0.1503, 0.1723, 0.014, 0.0642, 0.1124, 0.1234, 0.0643, -0.1091, 0.0408, 0.1279, -0.0234, 0.1254, 0.1637, 0.2048, -0.7466, 0.0269, -0.019, 0.0579, 0.0034, 0.093, 0.1473, 0.0041, -0.3608, -0.1192, -0.1101, 0.6456, 0.0752, -0.07, -0.055, 0.0097, 0.0175, 0.0034, 0.0171, -0.0259, -0.0431, 0.2949, 0.0442, -0.0568, -0.0246, -0.0825, -0.0895, -0.0354, 0.0302, -0.0512, -0.1258, -0.0261, 0.039, 0.0185, 0.0146, 0.0754, -0.0044, -0.0718, 0.0796, -0.0583, 0.0494, -0.0567, 0.0146, 0.0428, -0.0613, 0.0115, -0.0007, 0.0787, -0.0534, 0.0449, 0.1126, 0.0175, 0.0473, 0.2723, -0.0599, 0.2096, -0.0041, -0.0395, -0.4713, 0.0233, -0.0937, -0.2639, 0.0466, -0.1424, 0.001, -0.0245, -0.0115, -0.0302, 0.0034, -0.0401, 0.0055, 0.013, 0.356, 0.069, -0.0326, -0.06, 0.0033, 0.23, 0.0421, -0.1639, 0.0208, 0.0544, 0.0217, -0.049, -0.0096, -0.0298, 0.0323, -0.7646, -0.0145, 0.1061, -0.1425, -0.0049, -0.0913, 0.0375, -0.0735, -0.0046, 0.0877, 0.0725, -0.0258, -0.0401, -0.0095, -0.0906, -0.1003, 0.2793, -0.0684, 0.0113, -0.1215, -0.0354, -0.047, -0.0268, 0.0227, -0.1376, 0.0929, 0.0013, 0.0696, 0.1652, -0.068, 0.0175, -0.0667, -0.0633, 0.2623, 0.0747, 0.025], [-0.0326, 0.0195, 0.0364, -0.1522, -0.0004, -0.1664, -0.1484, -0.0419, -0.267, -0.0867, -0.0967, 0.1412, 0.049, -0.1076, 0.1205, 0.1043, 0.0353, 0.1153, -0.1742, -0.2729, -0.4423, -0.0395, 0.2611, -0.0571, -0.0127, -0.1157, 0.0721, -0.0517, -0.2769, -0.0741, 0.1076, 0.1005, 0.0459, 0.2092, -0.215, 0.1929, -0.0161, -0.2282, -0.0651, -0.0653, 0.1085, -0.1041, -0.0867, -0.033, 0.0878, -0.2261, -0.0212, -0.0315, -0.2479, 0.0607, -0.12, 0.0408, -0.8369, -0.1042, -0.1361, 0.0352, 0.0124, 0.1088, 0.0509, 0.0047, 0.0219, 0.0814, -0.0404, -0.0903, -0.0295, -0.0709, -0.0619, -0.0012, -0.1265, 0.1139, 0.0489, -0.0884, -0.0212, -0.0213, 0.2818, 0.0265, 0.0981, -0.5025, -0.2206, -0.0274, 0.0092, 0.0006, 0.1065, -0.282, 0.1825, -0.2823, -0.0556, 0.0994, 0.0301, 0.2853, -0.1554, -0.0155, 0.1182, -0.2357, 0.0372, 0.0412, -0.0469, -0.1763, -0.0437, -0.061, -0.1358, 0.0631, 0.0861, -0.0006, -0.0699, -0.0475, -0.1164, 0.1694, 0.0653, 0.1353, 0.0626, -0.056, -0.036, -0.0176, -0.2072, 0.1721, 0.093, 0.1898, -0.1265, -0.292, 0.1351, -0.1685, 0.0872, 0.0529, 0.1249, 0.1556, -0.0189, 0.1228, -0.0232, 0.0005, 0.0116, -0.0448, 0.1589, 0.0311, 0.2283, 0.2629, 0.1098, -0.2181, 0.0711, 0.0021, 0.0404, -0.0385, 0.1605, 0.1123, 0.021, -0.0397, -0.2564, -0.2977, 0.1308, -0.0305, -0.1551, 0.2805, -0.0386, -0.0466, -0.2589, 0.179, -0.0679, 0.1405, -0.212, 0.0176, -0.0757, -0.26, -0.0182, 0.0827, -0.1437, 0.03, -0.0128, -0.1292, 0.1008, 0.035, 0.2977, 0.2411, -0.0608, -0.1024, 0.1391, -0.1677, 0.3231, -0.1087, 0.0269, -0.0593, 0.1357, -0.0963, -0.1021, -0.1126, -0.0657, -0.4809, 0.0893, -0.1207, -0.1273, -0.0757, -0.0831, 0.2692, -0.0536, -0.1892, 0.0861, -0.1484, 0.0321, 0.1093, 0.0251, 0.1879, -0.0447, 0.0748, 0.1338, -0.0289, 0.0128, 0.0625, 0.173, -0.1717, -0.2312, 0.0046, 0.0477, 0.1302, 0.0672, -0.1898, -0.0803, 0.1801, -0.0964, 0.1186, 0.0623, 0.0322, 0.0902, -0.072, -0.0564, 0.1925, -0.2658, -0.1864, -0.105, 0.0615, -0.0711, 0.0126, 0.0044, 0.13, 0.1656, -0.1325, 0.0699, -0.0184, 0.0919, -0.0046, -0.1545, -0.0473, -0.0922, 0.1955, -0.0724, 0.4568, 0.0782, 0.2548, 0.0745, -0.1355, -0.0421, 0.372, 0.1428, 0.0172, 0.1141, -0.1431, 0.0519, -0.1439, 0.0232, 0.0016, -0.0564, 0.0573, 0.0371, 0.0915, 0.0967, -0.1458, -0.1127, -0.212, -0.0881, -0.1117, -0.1259, -0.0701, -0.0795, 0.1875, 0.0183, -0.0661, -0.0428, -0.0551, 0.0231, -0.297, -0.0825, -0.0378, 0.0166, -0.0777, -0.0733, 0.1054, -0.0045, 0.1439, 0.0198, 0.3949, -0.0875, -0.1, 0.1736, -0.0556, -0.0239, -0.1806, -0.2101, 0.1066, -0.1304, 0.0249, 0.0083, -0.1229], [-0.0657, 0.046, 0.0251, -0.0332, 0.0141, -0.221, -0.0121, -0.0384, 0.0709, -0.0542, 0.06, 0.0671, 0.0604, 0.1536, -0.0443, 0.0414, 0.0221, 0.0859, -0.0448, -0.0593, -0.0176, -0.036, 0.0366, -0.296, 0.0502, -0.2343, 0.047, -0.0906, 0.0382, -0.2112, 0.0204, -0.0113, 0.2169, -0.0023, 0.0375, 0.0031, 0.0218, 0.0073, -0.0524, 0.0786, 0.0447, -0.0189, -0.1872, 0.0252, -0.0965, -0.0778, -0.1135, -0.0199, 0.0901, 0.0191, -0.0134, -0.001, -0.6779, -0.0296, -0.0169, 0.0048, 0.0055, 0.0217, 0.0459, 0.1198, -0.1142, -0.0131, 0.1316, -0.034, 0.1073, -0.0355, -0.0246, 0.0521, -0.055, 0.0158, 0.03, 0.0168, 0.0335, 0.1986, 0.072, -0.0165, 0.0566, -0.1098, -0.0796, -0.0808, -0.0356, 0.0266, 0.1186, -0.2048, -0.0201, -0.1187, -0.0025, -0.0004, -0.0488, 0.0154, -0.0453, -0.0443, -0.0276, -0.1272, -0.0878, 0.007, 0.0324, -0.0797, -0.0007, -0.0803, -0.1543, -0.01, 0.0158, 0.0556, -0.1504, -0.0414, 0.216, -0.0864, 0.149, -0.0059, 0.0869, 0.2644, -0.023, 0.0089, -0.0612, 0.0043, 0.0118, 0.1146, -0.017, -0.3391, 0.117, 0.0525, 0.0654, 0.188, 0.0733, 0.1355, -0.0028, -0.0488, -0.1325, -0.0084, 0.0245, -0.0142, -0.0509, 0.0929, 0.0061, -0.2864, 0.1034, -0.0612, -0.0258, -0.0505, -0.0298, 0.0362, 0.0213, 0.2663, -0.0181, -0.059, 0.0939, -0.0895, 0.0385, -0.0212, 0.1066, 0.1416, 0.0303, 0.0044, -0.085, -0.0252, -0.0332, 0.0527, -0.032, -0.0311, -0.0576, 0.0001, 0.0041, 0.0065, -0.1985, 0.0077, 0.1188, 0.008, 0.0286, -0.0599, 0.1298, 0.0204, 0.0183, 0.1724, 0.0711, 0.0252, 0.1688, -0.0282, 0.0433, 0.0193, -0.0003, 0.0717, 0.0823, 0.0143, 0.0575, -0.2815, -0.0972, -0.1434, 0.0879, -0.0272, -0.041, 0.0076, 0.0508, 0.0021, 0.0043, 0.0017, -0.0012, -0.0067, 0.1853, 0.0582, 0.0609, -0.0207, 0.0806, 0.0253, 0.0604, 0.0744, -0.1165, -0.0971, -0.0967, 0.0329, 0.0521, 0.0893, 0.0125, -0.0923, -0.0302, 0.014, -0.1939, -0.0287, 0.0101, 0.1531, 0.0165, -0.1121, 0.0018, 0.1513, 0.0316, -0.094, -0.0563, -0.0422, 0.0181, 0.1168, -0.1048, 0.0031, 0.2662, 0.0742, 0.0081, 0.1066, -0.0511, 0.0307, -0.1781, 0.0602, -0.0484, 0.0319, 0.0762, 0.0056, -0.0201, -0.0665, -0.0476, 0.0214, -0.0166, 0.3347, 0.0167, 0.0944, -0.0148, -0.022, -0.2036, 0.0307, -0.1116, -0.0157, 0.0638, 0.017, -0.1067, 0.0272, -0.0475, -0.0247, -0.0591, -0.0323, 0.1388, 0.0192, -0.0207, 0.0467, 0.0291, -0.0942, -0.0741, -0.0043, -0.0332, 0.051, -0.0713, 0.0526, 0.004, -0.0196, -0.1, -0.1013, -0.0433, -0.07, -0.0107, 0.0362, -0.0375, 0.1955, -0.1428, 0.0313, -0.0333, 0.0039, 0.0837, 0.0162, -0.1025, 0.0986, -0.0369, 0.2393, 0.0885, -0.0732], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "printing it:    64\n",
      "printing it:    64\n",
      "maximum no. of cols:  300\n",
      "maximum no. of rows:  64\n",
      "bert_generator.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_ngram_embeddings = torch.tensor(ngram_embeddings_padded, dtype=torch.float)\n",
      "03/28/2021 17:59:28 - INFO - bert_utils -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/28/2021 17:59:28 - INFO - bert_utils -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmphb0ui5lq\n",
      "03/28/2021 17:59:31 - INFO - bert_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/28/2021 17:59:34 - INFO - bert_utils -   Weights of BertForNgramClassification not initialized from pretrained model: ['converter.weight', 'converter.bias']\n",
      "03/28/2021 17:59:34 - INFO - bert_utils -   Weights from pretrained model not used in BertForNgramClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"bert_generator.py\", line 432, in <module>\n",
      "    main() \n",
      "  File \"bert_generator.py\", line 315, in main\n",
      "    loss = model(ngram_ids, ngram_masks, ngram_embeddings) \n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\", line 155, in forward\n",
      "    \"them on device: {}\".format(self.src_device_obj, t.device))\n",
      "RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Modified : Run this : \n",
    "#Generator Train : Embedding Estimator train \n",
    "!python bert_generator.py \\\n",
    "--task_name sst-2\\\n",
    "--do_train\\\n",
    "--do_lower_case\\\n",
    "--data_dir data/sst-2/\\\n",
    "--bert_model bert-base-uncased\\\n",
    "--max_seq_length 64\\\n",
    "--train_batch_size 8\\\n",
    "--learning_rate 2e-5\\\n",
    "--num_train_epochs 25\\\n",
    "--output_dir ./tmp/gnrt/\\\n",
    "--no_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate attacks - DISP random attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/28/2021 23:13:46 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "line : ['sentence', 'label', 'flaw labels']\n",
      "line : ['that loves its charcatres and communicates something rather handsome about human ntaure ', '1', '\"1,2,3,8,911\"']\n",
      "line : ['remains utterly satisfied to remain the smea throughout ', '0', '\"3,5,6\"']\n",
      "line : ['on the bad revenge-of-the-nerds clichés the filmmakers could dredge up ', '0', '\"2,3,4,5\"']\n",
      "line : [\"that 's far too tragic to merit such basic treatmnet\", '0', '\"7,8,6\"']\n",
      "line : ['\"demonstrates that the director of such hollywood blockbusters as patriot sports can still turn out a small , personal film with an emotional wallop . \"', '1', '\"1,2,3,4,5,9,10\"']\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   tokens: that loves its charcatres and communicates something rather handsome about human ntaure\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the smea throughout\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:13:46 - INFO - bert_utils -   Loading word embeddings for generating attacks... \n",
      "output_file ./data/sst-2/add_1/disc_for_attacks_outputs.tsv\n",
      "attacks:   0%|                                            | 0/5 [00:00<?, ?it/s]examples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "example:  1\n",
      "tok_id :  1\n",
      "tok_id :  29\n",
      "tok_id :  30\n",
      "tok_id :  31\n",
      "tok_id :  32\n",
      "tok_id :  16\n",
      "tok_id :  33\n",
      "tok_id :  34\n",
      "tok_id :  35\n",
      "tok_id :  36\n",
      "tok_id :  0\n",
      "flaw_ids:  torch.Size([1, 128])\n",
      "examples:  [[37 38  1 18 39 40 34 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n",
      "  58 59 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "example:  37\n",
      "tok_id :  37\n",
      "tok_id :  38\n",
      "tok_id :  1\n",
      "tok_id :  18\n",
      "tok_id :  39\n",
      "tok_id :  40\n",
      "tok_id :  34\n",
      "tok_id :  41\n",
      "tok_id :  42\n",
      "tok_id :  43\n",
      "tok_id :  44\n",
      "tok_id :  45\n",
      "tok_id :  46\n",
      "tok_id :  47\n",
      "tok_id :  48\n",
      "tok_id :  49\n",
      "tok_id :  50\n",
      "tok_id :  51\n",
      "tok_id :  52\n",
      "tok_id :  53\n",
      "tok_id :  54\n",
      "tok_id :  55\n",
      "tok_id :  56\n",
      "tok_id :  57\n",
      "tok_id :  58\n",
      "tok_id :  59\n",
      "tok_id :  37\n",
      "tok_id :  0\n",
      "flaw_ids:  torch.Size([1, 128])\n",
      "examples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "example:  13\n",
      "tok_id :  13\n",
      "tok_id :  14\n",
      "tok_id :  15\n",
      "tok_id :  16\n",
      "tok_id :  17\n",
      "tok_id :  18\n",
      "tok_id :  19\n",
      "tok_id :  20\n",
      "tok_id :  0\n",
      "flaw_ids:  torch.Size([1, 128])\n",
      "examples:  [[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "example:  1\n",
      "tok_id :  1\n",
      "tok_id :  2\n",
      "tok_id :  3\n",
      "tok_id :  4\n",
      "tok_id :  5\n",
      "tok_id :  6\n",
      "tok_id :  7\n",
      "tok_id :  8\n",
      "tok_id :  9\n",
      "tok_id :  10\n",
      "tok_id :  11\n",
      "tok_id :  12\n",
      "tok_id :  0\n",
      "flaw_ids:  torch.Size([1, 128])\n",
      "examples:  [[21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "example:  21\n",
      "tok_id :  21\n",
      "tok_id :  18\n",
      "tok_id :  22\n",
      "tok_id :  23\n",
      "tok_id :  24\n",
      "tok_id :  18\n",
      "tok_id :  25\n",
      "tok_id :  26\n",
      "tok_id :  27\n",
      "tok_id :  28\n",
      "tok_id :  0\n",
      "flaw_ids:  torch.Size([1, 128])\n",
      "attacks: 100%|███████████████████████████████████| 5/5 [00:00<00:00, 552.06it/s]\n"
     ]
    }
   ],
   "source": [
    "!python bert_random_attacks.py \\\n",
    "--task_name sst-2 \\\n",
    "--do_lower_case \\\n",
    "--data_dir data/sst-2/add_1/dev_attacks.tsv \\\n",
    "--bert_model bert-base-uncased \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./data/sst-2/add_1/\n",
    "#--output_dir_attacks ./data/sst-2/add_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inference\n",
    "We first attack the test data using 5 differernt methods to drop the model performance as much as possible. The codes related to attacking the test sets would be availble soon!\n",
    "\n",
    "During inference phase, we use the pre-trained discriminator to identify the words that have been attacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/28/2021 23:20:21 - INFO - bert_utils -   device: cpu , distributed training: False, 16-bits training: False\n",
      "03/28/2021 23:20:22 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/28/2021 23:20:22 - INFO - bert_utils -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/28/2021 23:20:22 - INFO - bert_utils -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpmzrbtcrb\n",
      "03/28/2021 23:20:26 - INFO - bert_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   Weights of BertForDiscriminator not initialized from pretrained model: ['discriminator.weight', 'discriminator.bias']\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   Weights from pretrained model not used in BertForDiscriminator: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "data_dir value :  data/sst-2/add_1/test.tsv\n",
      "tsv in data_dir\n",
      "line : ['Sentence', 'label', 'flow_ids']\n",
      "line : ['that loves its characters and communicates something rather beautiful about human nature ', '1', '\"101, 3464, 12580, 8510, 2000, 3961, 1996, 15488, 5243, 2802, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['remains utterly satisfied to remain the same throughout ', '0', '\"101, 2006, 1996, 2919, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 1996, 16587, 2071, 2852, 24225, 2039, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', '0', '\"101, 1036, 1036, 16691, 2008, 1996, 2472, 1997, 10514, 6776, 5365, 27858, 2015, 2004, 16419, 2998, 23616, 2319, 2145, 2735, 27178, 1037, 2235, 1010, 3167, 2143, 2007, 2019, 6832, 2813, 7361, 1012, 1036, 1036, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : [\"that 's far too tragic to merit such superficial treatment \", '0', '\"101, 2008, 7459, 2049, 25869, 11266, 6072, 1998, 10639, 2015, 27941, 4160, 2738, 8502, 2055, 2529, 23961, 21159, 2063, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \"', '1', '\"101, 2008, 1005, 1055, 2521, 2205, 13800, 2000, 7857, 2107, 3937, 7438, 2213, 7159, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "ex_index;  0\n",
      "example.flaw_labels is not None: block \n",
      "i in enumerate tokens:  0\n",
      "i in enumerate tokens:  1\n",
      "i in enumerate tokens:  2\n",
      "i in enumerate tokens:  3\n",
      "i in enumerate tokens:  4\n",
      "i in enumerate tokens:  5\n",
      "i in enumerate tokens:  6\n",
      "i in enumerate tokens:  7\n",
      "i in enumerate tokens:  8\n",
      "i in enumerate tokens:  9\n",
      "i in enumerate tokens:  10\n",
      "i in enumerate tokens:  11\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   tokens: that loves its characters and communicates something rather beautiful about human nature \n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_ids: 101 2008 7459 2049 3494 1998 10639 2015 2242 2738 3376 2055 2529 3267 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_labels: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids: 101 3464 12580 8510 2000 3961 1996 15488 5243 2802 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids_cut: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   chunks: 1 1 1 1 1 1 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "ex_index;  1\n",
      "example.flaw_labels is not None: block \n",
      "i in enumerate tokens:  0\n",
      "i in enumerate tokens:  1\n",
      "i in enumerate tokens:  2\n",
      "i in enumerate tokens:  3\n",
      "i in enumerate tokens:  4\n",
      "i in enumerate tokens:  5\n",
      "i in enumerate tokens:  6\n",
      "i in enumerate tokens:  7\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the same throughout \n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_ids: 101 3464 12580 8510 2000 3961 1996 2168 2802 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_labels: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids: 101 2006 1996 2919 7195 1011 1997 1011 1996 1011 11265 17811 18856 17322 2015 1996 16587 2071 2852 24225 2039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids_cut: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   chunks: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "ex_index;  2\n",
      "example.flaw_labels is not None: block \n",
      "i in enumerate tokens:  0\n",
      "i in enumerate tokens:  1\n",
      "i in enumerate tokens:  2\n",
      "i in enumerate tokens:  3\n",
      "i in enumerate tokens:  4\n",
      "i in enumerate tokens:  5\n",
      "i in enumerate tokens:  6\n",
      "i in enumerate tokens:  7\n",
      "i in enumerate tokens:  8\n",
      "i in enumerate tokens:  9\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   tokens: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   token_ids: 21 18 22 23 24 18 25 26 27 28 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_ids: 101 2006 1996 5409 7195 1011 1997 1011 1996 1011 11265 17811 18856 17322 2015 1996 16587 2071 2852 24225 2039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_labels: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids: 101 1036 1036 16691 2008 1996 2472 1997 10514 6776 5365 27858 2015 2004 16419 2998 23616 2319 2145 2735 27178 1037 2235 1010 3167 2143 2007 2019 6832 2813 7361 1012 1036 1036 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   flaw_ids_cut: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   chunks: 1 1 1 1 8 3 1 1 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "ex_index;  3\n",
      "example.flaw_labels is not None: block \n",
      "i in enumerate tokens:  0\n",
      "i in enumerate tokens:  1\n",
      "i in enumerate tokens:  2\n",
      "i in enumerate tokens:  3\n",
      "i in enumerate tokens:  4\n",
      "i in enumerate tokens:  5\n",
      "i in enumerate tokens:  6\n",
      "i in enumerate tokens:  7\n",
      "i in enumerate tokens:  8\n",
      "i in enumerate tokens:  9\n",
      "ex_index;  4\n",
      "example.flaw_labels is not None: block \n",
      "i in enumerate tokens:  0\n",
      "i in enumerate tokens:  1\n",
      "i in enumerate tokens:  2\n",
      "i in enumerate tokens:  3\n",
      "i in enumerate tokens:  4\n",
      "i in enumerate tokens:  5\n",
      "i in enumerate tokens:  6\n",
      "i in enumerate tokens:  7\n",
      "i in enumerate tokens:  8\n",
      "i in enumerate tokens:  9\n",
      "i in enumerate tokens:  10\n",
      "i in enumerate tokens:  11\n",
      "i in enumerate tokens:  12\n",
      "i in enumerate tokens:  13\n",
      "i in enumerate tokens:  14\n",
      "i in enumerate tokens:  15\n",
      "i in enumerate tokens:  16\n",
      "i in enumerate tokens:  17\n",
      "i in enumerate tokens:  18\n",
      "i in enumerate tokens:  19\n",
      "i in enumerate tokens:  20\n",
      "i in enumerate tokens:  21\n",
      "i in enumerate tokens:  22\n",
      "i in enumerate tokens:  23\n",
      "i in enumerate tokens:  24\n",
      "i in enumerate tokens:  25\n",
      "i in enumerate tokens:  26\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -   ***** Running evaluation *****\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -     Num examples = 5\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -     Num token vocab = 60\n",
      "03/28/2021 23:20:28 - INFO - bert_utils -     Batch size = 32\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Evaluating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[Atmp_eval_loss:  tensor(0.0508)\n",
      "s:  tensor([[[ 1.8103, -2.6382],\n",
      "         [ 1.7207, -2.5305],\n",
      "         [ 1.7368, -2.5833],\n",
      "         ...,\n",
      "         [ 2.5132, -2.7787],\n",
      "         [ 2.5367, -2.8217],\n",
      "         [ 2.5126, -2.7510]],\n",
      "\n",
      "        [[ 1.6936, -2.2109],\n",
      "         [ 1.7135, -2.3332],\n",
      "         [ 1.5097, -2.4624],\n",
      "         ...,\n",
      "         [ 2.5018, -2.5897],\n",
      "         [ 2.4819, -2.5864],\n",
      "         [ 2.4841, -2.6118]],\n",
      "\n",
      "        [[ 2.0756, -2.3852],\n",
      "         [ 1.9839, -2.7622],\n",
      "         [ 1.3004, -2.5930],\n",
      "         ...,\n",
      "         [ 2.4156, -2.4598],\n",
      "         [ 2.3951, -2.4340],\n",
      "         [ 2.3850, -2.4106]],\n",
      "\n",
      "        [[ 1.6574, -2.3127],\n",
      "         [ 1.1455, -2.4098],\n",
      "         [ 1.3560, -2.4831],\n",
      "         ...,\n",
      "         [ 2.3563, -2.6931],\n",
      "         [ 2.3835, -2.7447],\n",
      "         [ 2.3113, -2.6461]],\n",
      "\n",
      "        [[ 2.1178, -2.4871],\n",
      "         [ 1.6803, -2.3309],\n",
      "         [ 1.8489, -2.7761],\n",
      "         ...,\n",
      "         [ 2.2513, -2.2960],\n",
      "         [ 2.1729, -2.2031],\n",
      "         [ 2.1658, -2.1776]]])\n",
      "len of logits:  5\n",
      "shape of logits:  torch.Size([5, 128, 2])\n",
      "type of logits:  <class 'torch.Tensor'>\n",
      "type of logits:  tensor([[[ 1.8103, -2.6382],\n",
      "         [ 1.7207, -2.5305],\n",
      "         [ 1.7368, -2.5833],\n",
      "         ...,\n",
      "         [ 2.5132, -2.7787],\n",
      "         [ 2.5367, -2.8217],\n",
      "         [ 2.5126, -2.7510]],\n",
      "\n",
      "        [[ 1.6936, -2.2109],\n",
      "         [ 1.7135, -2.3332],\n",
      "         [ 1.5097, -2.4624],\n",
      "         ...,\n",
      "         [ 2.5018, -2.5897],\n",
      "         [ 2.4819, -2.5864],\n",
      "         [ 2.4841, -2.6118]],\n",
      "\n",
      "        [[ 2.0756, -2.3852],\n",
      "         [ 1.9839, -2.7622],\n",
      "         [ 1.3004, -2.5930],\n",
      "         ...,\n",
      "         [ 2.4156, -2.4598],\n",
      "         [ 2.3951, -2.4340],\n",
      "         [ 2.3850, -2.4106]],\n",
      "\n",
      "        [[ 1.6574, -2.3127],\n",
      "         [ 1.1455, -2.4098],\n",
      "         [ 1.3560, -2.4831],\n",
      "         ...,\n",
      "         [ 2.3563, -2.6931],\n",
      "         [ 2.3835, -2.7447],\n",
      "         [ 2.3113, -2.6461]],\n",
      "\n",
      "        [[ 2.1178, -2.4871],\n",
      "         [ 1.6803, -2.3309],\n",
      "         [ 1.8489, -2.7761],\n",
      "         ...,\n",
      "         [ 2.2513, -2.2960],\n",
      "         [ 2.1729, -2.2031],\n",
      "         [ 2.1658, -2.1776]]])\n",
      "Type of flaw_logits:  <class 'torch.Tensor'>\n",
      "shape of flaw_logits:  torch.Size([5, 128])\n",
      "Length of flaw_logits:  5\n",
      "flaw_logits:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "type chunks:  <class 'numpy.ndarray'>\n",
      "len of chunks:  5\n",
      "type of logits:  <class 'numpy.ndarray'>\n",
      "len of logits :  5\n",
      "max_seq_length sbplshp :   128\n",
      "max_batch_size sbplshp :  5\n",
      "Type of flaw_logits logit_converter:  <class 'list'>\n",
      "Length of flaw_logits logit_converter :  5\n",
      "flaw_logits logit_converter :  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "length of flaw_ids:  5\n",
      "tmp:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len of tmp:  13\n",
      "length of flaw_ids of i :  128\n",
      "flaw_ids[i]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tmp:  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len of tmp:  9\n",
      "length of flaw_ids of i :  128\n",
      "flaw_ids[i]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tmp:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len of tmp:  11\n",
      "length of flaw_ids of i :  128\n",
      "flaw_ids[i]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tmp:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len of tmp:  11\n",
      "length of flaw_ids of i :  128\n",
      "flaw_ids[i]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tmp:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len of tmp:  28\n",
      "length of flaw_ids of i :  128\n",
      "flaw_ids[i]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "i in write output file: 0\n",
      "flaw_logit in write output file:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i in write output file: 1\n",
      "flaw_logit in write output file:  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i in write output file: 2\n",
      "flaw_logit in write output file:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i in write output file: 3\n",
      "flaw_logit in write output file:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i in write output file: 4\n",
      "flaw_logit in write output file:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:02<00:00,  2.50s/it]\u001b[A\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -   ***** Eval results *****\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -     eval_acc = 1.0\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -     eval_f1 = 0.0\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -     eval_loss = 0.050849396735429764\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -     eval_precision = 0.0\n",
      "03/28/2021 23:20:33 - INFO - bert_utils -     eval_recall = 0.0\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:04<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Modified - Run this : \n",
    "# Discriminator do_eval or Testing \n",
    "!python bert_discriminator.py \\\n",
    "--task_name sst-2\\\n",
    "--do_eval\\\n",
    "--eval_batch_size 32\\\n",
    "--do_lower_case\\\n",
    "--data_dir data/sst-2/add_1/\\\n",
    "--data_file data/sst-2/add_1/test.tsv\\\n",
    "--bert_model bert-base-uncased\\\n",
    "--max_seq_length 128\\\n",
    "--train_batch_size 16\\\n",
    "--learning_rate 2e-5\\\n",
    "--num_train_epochs 5\\\n",
    "--output_dir models/\\\n",
    "--single\n",
    "#--no_cuda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then, we recover the words with a pre-trained embedding estimator. Note that we use small-world-graph to conduct a KNN-based search for closest word in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "03/28/2021 23:25:06 - INFO - bert_utils -   device: cpu n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "03/28/2021 23:25:06 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/28/2021 23:25:06 - INFO - bert_utils -   loading embeddings ... \n",
      "03/28/2021 23:25:06 - INFO - bert_utils -   loading p index ...\n",
      "line : ['Sentence', 'label', 'flow_labels']\n",
      "line : ['that loves its characters and communicates something rather beautiful about human nature ', '1', '\"0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['remains utterly satisfied to remain the same throughout ', '0', '\"0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', '0', '\"0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : [\"that 's far too tragic to merit such superficial treatment \", '0', '\"0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "line : ['\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \"', '1', '\"0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\"']\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   tokens: that loves its characters and communicates something rather beautiful about human nature\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   flaw_labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   ngram_labels: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   label: 1 (id = 1)\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the same throughout\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   flaw_labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   ngram_labels: 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   label: 0 (id = 0)\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   *** Example ***\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   tokens: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   token_ids: 21 18 22 23 24 18 25 26 27 28 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   flaw_labels: 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   ngram_labels: 21 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   label: 0 (id = 0)\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -   ***** Running evaluation *****\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -     Num examples = 5\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -     Num token vocab = 60\n",
      "03/28/2021 23:25:07 - INFO - bert_utils -     Batch size = 8\n",
      "Epoch:   0%|                                              | 0/2 [00:00<?, ?it/s]\n",
      "Evaluating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[Atype of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [-0.02516527  0.24916412  0.3998569  -0.23923235 -0.57313037  0.23637517\n",
      " -0.12364212 -0.18356186 -0.38671654 -0.08198071  0.00504356 -0.01520404\n",
      " -0.05239039  0.10258828 -0.01321339 -0.34907737  0.08511557 -0.07757989\n",
      " -0.142928    0.33181986  0.38751063  0.29767674  0.11068425  0.12816764\n",
      "  0.12853038  0.19553284 -0.01948262  0.16845407  0.03636056 -0.2659368\n",
      " -0.04427266  0.22136036  0.12581639  0.06439314  0.07736353  0.21209386\n",
      "  0.00596261 -0.27591133  0.12307896 -0.06437595  0.14343481 -0.29236093\n",
      " -0.3069529   0.13641028  0.13416664  0.14421406 -0.03581997  0.34600294\n",
      "  0.5144738  -0.00931658  0.05124003 -0.45484844 -0.13879119 -0.06497459\n",
      "  0.17421782  0.00164915  0.02641115  0.13810585 -0.05776872  0.14049831\n",
      " -0.09182833 -0.11005676  0.01325397 -0.08442435  0.11624003  0.07054516\n",
      " -0.3866742  -0.188944    0.527753   -0.22653924 -0.3020229  -0.2977597\n",
      " -0.1965852   0.00242713  0.01238304 -0.13519126 -0.15592445  0.17707714\n",
      " -0.01581664 -0.14522372 -0.35727313 -0.0147486   0.6980421  -0.43508402\n",
      "  0.22065194  0.22031556 -0.21345422 -0.06318586  0.31312627 -0.04768696\n",
      " -0.07188527 -0.5738614   0.0204555   0.02176018  0.09950343 -0.18873551\n",
      "  0.14229368 -0.1733753  -0.09104138 -0.21838251  0.03723061 -0.11721139\n",
      " -0.390864   -0.0858929  -0.03331336  0.12132877  0.32431898  0.01303755\n",
      " -0.51795876  0.36595538 -0.02191404  0.0882332  -0.35935056  0.04371496\n",
      "  0.06673934  0.08389764 -0.21040536  0.5428419  -0.06963897 -0.37808874\n",
      "  0.1649264   0.03648494  0.167811    0.17693652  0.2767917  -0.22913782\n",
      " -0.33357134  0.12096795 -0.26630497 -0.38928625 -0.22101253  0.00752818\n",
      "  0.02932585 -0.15174538  0.07809828  0.33832386  0.26529053  0.2857201\n",
      " -0.07473852 -0.10681389 -0.01231853  0.15041478  0.15657127 -0.09774049\n",
      "  0.14620726 -0.1124313  -0.13763207  0.31200495 -0.00145514 -0.29758704\n",
      "  0.30086094  0.2043736  -0.15638913 -0.16617797 -0.40120697 -0.00898075\n",
      "  0.23008251 -0.33877403 -0.04520994 -0.50723165  0.21282586 -0.47749728\n",
      "  0.02700224  0.04290943  0.07148087  0.05542506 -0.09016421 -0.52489287\n",
      " -0.15433721 -0.01619327 -0.1474307  -0.00642302 -0.22069003  0.01951836\n",
      " -0.10989461  0.15663171  0.08708128 -0.16288035  0.13180156 -0.5158615\n",
      "  0.2103127  -0.20139752 -0.06911213  0.09347916  0.00319468 -0.3511462\n",
      " -0.40258956 -0.18879478  0.22814861 -0.13847135 -0.08560754 -0.25429794\n",
      " -0.38209513 -0.48271245  0.06825742 -0.19024132  0.45320433  0.02710956\n",
      "  0.32618198 -0.02637635 -0.29545     0.12441663 -0.28907603 -0.15422137\n",
      "  0.43376675 -0.02145367  0.17513432  0.0207199   0.17017542  0.1184039\n",
      " -0.10758483  0.04324656 -0.25094444 -0.5591091  -0.3352175   0.269404\n",
      " -0.1311256  -0.12432057  0.11117594  0.2782664  -0.24566767 -0.38207623\n",
      "  0.47370848  0.2720871  -0.19764653  0.38387707  0.17022018 -0.02775275\n",
      " -0.22353882 -0.29023898 -0.13855289  0.5966962  -0.03231415 -0.07415814\n",
      "  0.1124548  -0.25214753 -0.06421391 -0.00188933 -0.3044581   0.11597399\n",
      "  0.00550048  0.11973835 -0.11468352  0.4773858  -0.22352266 -0.30214152\n",
      " -0.08148243  0.00835735 -0.52755284 -0.2326869   0.12682258  0.06490064\n",
      "  0.05251478 -0.2086519  -0.180887   -0.3396042   0.3113378  -0.02283674\n",
      " -0.2483803  -0.13775484  0.03029785  0.03477698 -0.32697842 -0.09973275\n",
      "  0.01736699 -0.04897258 -0.22569336  0.15190239  0.24787468 -0.17074168\n",
      "  0.11509636 -0.12791637 -0.02841067 -0.33321086 -0.01258434  0.14409114\n",
      "  0.05495374  0.13071942 -0.13894661 -0.30732194  0.29185495 -0.21139453\n",
      " -0.17329963 -0.11058154 -0.03582744  0.3539535   0.15271713  0.40181318\n",
      " -0.29598916  0.07342582 -0.2089979   0.07490982  0.02562637  0.30220908\n",
      "  0.06813669 -0.00893016  0.00632936 -0.24720986  0.1242802   0.04459104]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 8.07699487e-02  1.40965670e-01  3.48128945e-01 -4.67206776e-01\n",
      " -5.33704579e-01  1.72188982e-01 -2.30286971e-01 -1.98527470e-01\n",
      " -5.25932848e-01 -6.75831735e-02 -6.03986718e-02 -1.52559564e-01\n",
      " -7.57612986e-03  4.90799025e-02  9.04098302e-02 -2.22025692e-01\n",
      "  8.86393934e-02 -1.05206653e-01 -1.13846444e-01  4.43126559e-01\n",
      "  4.50670838e-01  5.77401042e-01  1.17444195e-01  1.80874422e-01\n",
      "  1.33661836e-01  1.15509562e-01 -5.32268360e-03  3.09692115e-01\n",
      "  6.47975281e-02 -2.46387541e-01 -1.06047183e-01  1.97544873e-01\n",
      "  2.02268317e-01 -3.74158286e-02  9.24251601e-02  2.52405047e-01\n",
      "  2.80704256e-02 -2.16375977e-01  6.45043403e-02 -2.33748987e-01\n",
      "  1.12161063e-01 -3.78633767e-01 -4.20060724e-01  1.87954172e-01\n",
      "  2.45746877e-02  2.15151697e-01  3.37965339e-02  5.30394793e-01\n",
      "  3.39965641e-01 -1.27288088e-01  1.21277064e-01 -4.98113930e-01\n",
      " -5.32597639e-02 -1.05449259e-01  1.75490350e-01  8.88804905e-03\n",
      "  3.55077051e-02  2.08594859e-01 -1.99686661e-01  1.51860164e-02\n",
      " -4.78492904e-04 -2.15501353e-01 -2.58637946e-02 -1.11928448e-01\n",
      "  1.93139687e-01  8.47996473e-02 -3.91025513e-01 -1.52222365e-01\n",
      "  5.63496530e-01 -2.34452263e-01 -3.28294426e-01 -3.04961532e-01\n",
      " -1.74960434e-01  2.92306878e-02  1.57290652e-01 -1.17634781e-01\n",
      "  1.13528920e-02  3.67641389e-01 -1.68098241e-01 -3.40002775e-01\n",
      " -3.80353659e-01 -2.44809046e-01  6.02940202e-01 -4.59516138e-01\n",
      "  1.36401281e-01  3.67914677e-01 -2.67573327e-01 -3.94198764e-03\n",
      "  4.01695430e-01 -1.08654015e-01 -2.68532574e-01 -6.58498645e-01\n",
      " -2.12270748e-02  2.29570307e-02  8.17921534e-02 -2.28002727e-01\n",
      "  8.20674598e-02 -2.16249809e-01 -3.57034415e-01 -4.25045848e-01\n",
      "  3.54246758e-02 -9.83245149e-02 -3.96805137e-01 -9.09962207e-02\n",
      " -1.38133671e-02  1.01637870e-01  3.06014180e-01  1.78258251e-02\n",
      " -3.64345551e-01  3.77356470e-01 -3.70552093e-02  1.65264606e-01\n",
      " -2.66833603e-01  1.45107359e-01  1.64184168e-01  1.25782803e-01\n",
      " -1.68527454e-01  5.70707619e-01 -4.89670634e-02 -2.73313165e-01\n",
      "  6.61933497e-02  1.03246972e-01  7.60614350e-02  2.77694970e-01\n",
      "  3.51292670e-01 -3.25666845e-01 -3.37126523e-01  2.27236718e-01\n",
      " -2.80517191e-01 -3.79696906e-01 -3.92471075e-01  2.40841433e-02\n",
      " -7.64836445e-02 -1.90673709e-01  7.77026117e-02  3.41129750e-01\n",
      "  2.40928695e-01  3.79775405e-01 -5.96460421e-03 -4.92316559e-02\n",
      " -1.09518491e-01  3.09955955e-01  1.16021626e-01 -9.43695903e-02\n",
      "  4.19727415e-02 -2.38228589e-01 -2.88439929e-01  4.02637094e-01\n",
      " -3.29446048e-02 -3.09213042e-01  3.80252391e-01  2.26530328e-01\n",
      " -1.73038989e-01 -2.92254329e-01 -3.28542560e-01  2.50695962e-02\n",
      "  3.30068886e-01 -2.37250209e-01 -6.34395033e-02 -4.53645170e-01\n",
      "  1.39942884e-01 -5.17637670e-01 -1.77552868e-02  6.58894181e-02\n",
      "  1.09127805e-01  8.53415653e-02  5.27929999e-02 -4.10912007e-01\n",
      " -1.61161035e-01 -6.02867603e-02 -1.33380339e-01 -3.91771533e-02\n",
      " -1.94612309e-01 -1.15063652e-01 -1.59374774e-01  1.01673245e-01\n",
      " -1.02561802e-01 -2.20194370e-01  8.69166572e-03 -4.96234506e-01\n",
      "  3.57230037e-01 -1.61751285e-01 -6.37898147e-02  1.87594369e-01\n",
      "  8.09346661e-02 -3.22843045e-01 -5.12897730e-01 -1.72383055e-01\n",
      "  2.05635563e-01 -2.70710707e-01 -1.47457734e-01 -3.14764798e-01\n",
      " -4.02980357e-01 -3.53352994e-01  4.07850146e-02 -3.45256388e-01\n",
      "  5.03271222e-01  7.36780092e-02  3.47338110e-01 -2.23455206e-01\n",
      " -3.22343767e-01  1.78422779e-01 -2.77693033e-01 -1.55639201e-01\n",
      "  4.36369091e-01  1.44455433e-01  1.40698016e-01  5.04687428e-02\n",
      "  4.07707393e-01  1.26972109e-01 -1.56253040e-01 -6.13982566e-02\n",
      " -2.45961800e-01 -6.36222482e-01 -3.60596716e-01  1.54085636e-01\n",
      " -3.04467268e-02 -9.67061967e-02 -8.23096931e-02  3.16350400e-01\n",
      " -2.49421850e-01 -4.14792120e-01  5.10048389e-01  3.72303665e-01\n",
      " -1.89480603e-01  4.30972844e-01  1.24783985e-01 -6.94444403e-03\n",
      " -1.70266956e-01 -2.73336440e-01  5.61254881e-02  5.77448070e-01\n",
      " -1.46311522e-01 -9.73953083e-02  1.80509314e-01 -1.96469709e-01\n",
      " -1.92468278e-02  5.17333224e-02 -3.20740253e-01  1.07890874e-01\n",
      "  2.93000229e-02  1.23299338e-01 -1.61523908e-01  4.69499826e-01\n",
      " -4.03799951e-01 -3.63660336e-01 -3.49524543e-02  5.45119308e-02\n",
      " -4.74030018e-01 -2.78764069e-01  1.86169386e-01  1.43379271e-01\n",
      "  1.60762876e-01 -2.38244534e-01 -5.73553443e-02 -4.70603615e-01\n",
      "  3.70319068e-01 -2.28650477e-02 -2.18848109e-01 -1.07783535e-02\n",
      " -1.44366231e-02  7.41680264e-02 -2.54463881e-01 -2.31856741e-02\n",
      " -2.70969123e-02 -1.25868827e-01 -4.24021810e-01  2.17435285e-01\n",
      "  2.52356112e-01 -7.00614601e-02 -3.91105562e-02 -1.26929551e-01\n",
      " -1.31810931e-02 -3.23907971e-01 -8.30492191e-03  3.35852802e-01\n",
      "  2.26055551e-02  1.47247082e-02 -2.48041138e-01 -2.89080262e-01\n",
      "  3.62174928e-01 -1.74564183e-01 -4.14562732e-01 -9.48067755e-02\n",
      "  3.07581201e-02  3.24903220e-01  1.64051607e-01  4.15966898e-01\n",
      " -3.17674518e-01  6.26506060e-02 -2.59097815e-01  4.70553413e-02\n",
      " -3.59864207e-03  2.47511715e-01  3.34468409e-02  1.88169573e-02\n",
      " -7.07768872e-02 -2.02577487e-01  1.59394830e-01 -4.66500968e-02]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [-2.90779453e-02  2.83477932e-01  3.52815181e-01 -2.60045558e-01\n",
      " -5.02326190e-01  2.46797949e-01 -7.62958750e-02 -2.01397270e-01\n",
      " -2.98255980e-01 -6.14953116e-02  1.54098812e-02 -1.44852446e-02\n",
      "  1.42190093e-02  1.26130596e-01 -1.35581810e-02 -3.96335393e-01\n",
      "  9.82930958e-02 -6.86160326e-02 -8.36789608e-02  3.33080292e-01\n",
      "  3.91611725e-01  3.12101156e-01  1.43241465e-01  9.12983865e-02\n",
      "  8.18706974e-02  1.60446391e-01 -4.30588275e-02  1.39405861e-01\n",
      "  4.27100388e-03 -1.86625585e-01 -5.49098523e-03  2.03682661e-01\n",
      "  1.95953682e-01  1.17498234e-01  9.34686139e-02  1.72997743e-01\n",
      "  1.89894419e-02 -2.53051519e-01  4.37292121e-02  3.89510696e-03\n",
      "  1.91201404e-01 -1.89966634e-01 -2.33569950e-01  1.76665813e-01\n",
      "  8.70224759e-02  1.81255713e-01 -5.36434613e-02  2.63767660e-01\n",
      "  4.75038856e-01  9.07737575e-03  1.43963486e-01 -3.93455565e-01\n",
      " -1.89583600e-01 -7.54785538e-02  1.99275345e-01  8.32422301e-02\n",
      "  2.22731400e-02  1.22632831e-01  3.59310731e-02  1.17811352e-01\n",
      " -1.00458622e-01 -7.73086101e-02  3.98594178e-02 -8.32584575e-02\n",
      "  5.75249940e-02  9.06487554e-02 -3.88458312e-01 -2.01337799e-01\n",
      "  4.73867804e-01 -2.00673625e-01 -2.60429353e-01 -3.74956667e-01\n",
      " -2.07668155e-01  2.68868078e-02 -7.19044954e-02 -1.51376367e-01\n",
      " -2.34643355e-01  1.66848376e-01  4.55709361e-03 -1.36814460e-01\n",
      " -3.02666336e-01  3.22083533e-02  7.29379892e-01 -4.33229625e-01\n",
      "  3.09824497e-01  2.16375515e-01 -3.12541425e-01 -7.31617287e-02\n",
      "  2.31333449e-01 -3.35530452e-02 -6.42799959e-02 -5.30539930e-01\n",
      " -4.29987647e-02 -2.17848681e-02  1.52937546e-01 -1.91250846e-01\n",
      "  1.12478308e-01 -1.89906746e-01 -1.30214253e-02 -1.01526804e-01\n",
      " -3.00898822e-03 -1.88435435e-01 -3.40679348e-01 -2.06584353e-02\n",
      " -8.85863230e-02  1.06003791e-01  3.46042246e-01  2.39723865e-02\n",
      " -5.67367852e-01  3.77088368e-01 -7.30591491e-02  1.13494277e-01\n",
      " -4.22764033e-01  1.79425546e-03  1.34179136e-02  5.67384809e-02\n",
      " -2.18069524e-01  4.64567006e-01 -9.05830190e-02 -3.94330889e-01\n",
      "  1.29750922e-01 -5.13772629e-02  2.36586332e-01  1.80393860e-01\n",
      "  1.93646178e-01 -1.39434651e-01 -3.85103285e-01  5.13893291e-02\n",
      " -1.92931622e-01 -4.62281287e-01 -1.49900392e-01 -8.49844068e-02\n",
      "  6.49074540e-02 -8.58985186e-02  1.03832707e-01  2.60479242e-01\n",
      "  2.02175692e-01  2.18159929e-01 -9.86153260e-02 -1.19127654e-01\n",
      " -1.78829639e-03  6.93632290e-02  9.02206004e-02 -3.06189880e-02\n",
      "  1.99291900e-01 -1.15511842e-01 -8.33216906e-02  2.39649579e-01\n",
      "  5.07106911e-03 -2.36429214e-01  2.36712620e-01  2.21155286e-01\n",
      " -1.16328090e-01 -1.86654612e-01 -3.12045336e-01 -4.20358442e-02\n",
      "  2.41959989e-01 -2.40148142e-01 -3.58023942e-02 -5.76555371e-01\n",
      "  1.80642366e-01 -4.13247406e-01  7.75474031e-03  6.58144429e-03\n",
      "  4.59294021e-02  4.24102657e-02 -9.28813517e-02 -4.42857414e-01\n",
      " -1.22310936e-01 -1.78174507e-02 -1.46955818e-01 -1.01620965e-02\n",
      " -2.25467175e-01 -6.41475758e-03 -1.25618011e-01  1.57607242e-01\n",
      "  7.84680918e-02 -1.11417986e-01  1.73465624e-01 -4.86202031e-01\n",
      "  2.40329310e-01 -2.17831403e-01 -9.23165083e-02  5.23337610e-02\n",
      "  7.10119978e-02 -3.85860801e-01 -3.55701059e-01 -1.76803291e-01\n",
      "  2.32526362e-01 -9.16720554e-02 -3.24848816e-02 -1.58634290e-01\n",
      " -3.84321362e-01 -4.95673358e-01  9.49179530e-02 -2.18249470e-01\n",
      "  4.12616014e-01 -3.03883175e-03  2.41545543e-01 -7.56440461e-02\n",
      " -2.61226773e-01  7.23972768e-02 -2.35127285e-01 -1.40202403e-01\n",
      "  3.95919740e-01 -1.51113510e-01  1.73068747e-01 -5.34323007e-02\n",
      "  1.44857153e-01  7.90591389e-02 -7.02256337e-02  3.45312357e-02\n",
      " -2.42289260e-01 -5.11456847e-01 -2.68141538e-01  2.36567900e-01\n",
      " -1.75289229e-01 -1.18018895e-01  6.77458197e-02  2.90334433e-01\n",
      " -2.23471642e-01 -3.49670678e-01  4.15648490e-01  2.84918964e-01\n",
      " -2.66354531e-01  3.82708400e-01  2.05005124e-01  7.95221888e-03\n",
      " -1.63126513e-01 -3.37245136e-01 -1.70100793e-01  6.29066646e-01\n",
      " -2.25911569e-02  3.65365110e-03  9.74518061e-02 -3.39051515e-01\n",
      " -5.76068982e-02 -1.31842643e-02 -2.65694171e-01  1.38989359e-01\n",
      "  5.06923310e-02  1.75940305e-01 -1.41099274e-01  4.59631532e-01\n",
      " -1.93635523e-01 -2.47567311e-01 -1.08330242e-01  2.84331013e-03\n",
      " -5.27938306e-01 -1.02997161e-01  1.61694825e-01  1.18978508e-01\n",
      " -1.19365565e-03 -2.12164447e-01 -1.56205595e-01 -2.97847003e-01\n",
      "  3.43479306e-01 -6.66365996e-02 -3.27151179e-01 -1.18691117e-01\n",
      "  1.03434231e-02 -8.71281954e-04 -3.04742426e-01 -8.35414827e-02\n",
      " -1.65081146e-05 -7.43265823e-02 -2.07130864e-01  1.13159835e-01\n",
      "  2.56374151e-01 -1.18370563e-01  1.36961266e-01 -1.30730450e-01\n",
      " -1.29429093e-02 -3.66514593e-01  1.80693958e-02  1.23107262e-01\n",
      "  6.83492720e-02  1.72500312e-01 -1.09912448e-01 -2.57976413e-01\n",
      "  2.07470000e-01 -2.35325456e-01 -2.30357632e-01 -8.13008174e-02\n",
      " -7.63603970e-02  2.83707410e-01  7.26129562e-02  3.80142421e-01\n",
      " -2.84692943e-01  6.32274058e-03 -2.09705532e-01  1.14238657e-01\n",
      "  8.18949938e-02  3.08999628e-01  1.04053333e-01  4.12757248e-02\n",
      "  4.28670458e-02 -2.63445020e-01  1.20563760e-01 -3.75306787e-04]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 3.21923554e-01 -4.34284285e-02  3.99844766e-01 -3.64225477e-01\n",
      " -3.18923116e-01  1.79822192e-01 -1.51084751e-01 -3.07878643e-01\n",
      " -3.91375542e-01 -2.59864897e-01 -4.24229586e-03  1.35918155e-01\n",
      "  1.18228398e-01 -1.47660226e-01 -1.69879988e-01 -1.40910640e-01\n",
      " -1.68281421e-02 -1.01197027e-01 -1.11853443e-01  1.67454734e-01\n",
      "  2.04977449e-02  2.28319541e-01 -6.64809421e-02  7.01784864e-02\n",
      "  1.47967964e-01 -1.30819589e-01  5.01235306e-01 -6.15798607e-02\n",
      " -2.73614954e-02 -3.42224278e-02  1.72987245e-02  1.08035408e-01\n",
      "  2.12964773e-01  2.18807720e-02 -2.81388033e-03  1.53085321e-01\n",
      "  2.12385356e-01 -5.53621873e-02  2.59839177e-01 -4.69502583e-02\n",
      "  1.53409138e-01 -7.00487718e-02 -1.89126849e-01  2.38366544e-01\n",
      "  3.52459013e-01  4.17556345e-01 -1.99364983e-02  3.87698680e-01\n",
      "  3.79889786e-01  1.98428392e-01 -1.97058432e-02 -1.74864754e-01\n",
      " -2.52293080e-01 -3.64906609e-01  2.26699576e-01  2.09923297e-01\n",
      " -1.15875706e-01 -1.21543027e-01  1.30391017e-01  1.47561610e-01\n",
      " -3.56532335e-01 -9.01706591e-02  6.29442707e-02  2.68933684e-01\n",
      " -3.15200359e-01  8.83658603e-02  1.96331963e-02 -2.30135873e-01\n",
      "  7.57051855e-02 -5.92963472e-02  1.86675694e-02 -3.11652869e-01\n",
      " -3.58257651e-01  7.43449107e-02  5.15639372e-02 -3.05147320e-01\n",
      " -9.90559235e-02 -8.91280621e-02 -6.16910495e-02  3.18491757e-01\n",
      " -4.70466882e-01  2.27763038e-02  7.72649169e-01 -4.05763268e-01\n",
      "  2.50876367e-01  1.66205436e-01 -4.84697670e-01  3.94272414e-04\n",
      "  2.06838883e-02  4.43839952e-02  1.51085341e-02 -6.73910260e-01\n",
      "  1.22077875e-01 -3.17947268e-01  4.15241599e-01  1.63566638e-02\n",
      " -6.11872301e-02 -3.11064094e-01  9.97297168e-02  3.76258977e-02\n",
      "  8.77815410e-02 -7.26233646e-02 -4.11935836e-01  3.42182875e-01\n",
      " -7.38259032e-02  1.52740076e-01  1.41964436e-01  3.45695198e-01\n",
      " -3.62456203e-01  4.19417799e-01 -2.48663232e-01 -4.52398434e-02\n",
      " -1.29476473e-01  3.88924591e-02 -3.21288079e-01  2.50926942e-01\n",
      " -1.68363750e-01  4.53189701e-01 -1.49200812e-01 -1.45025209e-01\n",
      "  7.67984316e-02  9.10378695e-02  8.33761245e-02  3.22872639e-01\n",
      "  4.96113636e-02  1.92740545e-01 -4.61966187e-01  3.73336315e-01\n",
      " -4.04032677e-01 -3.59130621e-01 -1.26573965e-01  3.74937803e-01\n",
      "  2.48976126e-01 -3.09747040e-01 -2.56723966e-02 -1.28463656e-01\n",
      "  2.80902326e-01  4.69973832e-01  5.77350929e-02 -2.15999559e-01\n",
      "  1.22241221e-01  6.06638752e-02 -3.17458928e-01  1.62518293e-01\n",
      "  3.59571666e-01 -1.57199726e-01 -5.93923554e-02  2.99001098e-01\n",
      " -7.91617930e-02 -3.83960754e-01 -1.31387725e-01  1.78639993e-01\n",
      " -7.29341805e-02 -2.35073149e-01 -4.66872096e-01  5.93538815e-03\n",
      "  1.54006317e-01 -1.82304680e-01  3.15288186e-01 -2.35873945e-02\n",
      "  8.57774913e-02 -5.98386705e-01  1.37189269e-01  2.11404264e-01\n",
      "  1.48895353e-01  1.39951304e-01 -5.55564649e-02 -3.54336463e-02\n",
      "  2.11872071e-01  2.63769150e-01 -3.91773522e-01  2.16016099e-01\n",
      "  1.60486624e-02  1.99324265e-01 -3.33695233e-01  6.74169064e-02\n",
      "  1.06341884e-01  1.93507686e-01  1.47426292e-01 -5.49205244e-01\n",
      "  1.91738591e-01 -1.62239224e-01  9.05822664e-02  2.43789107e-01\n",
      " -1.28879949e-01 -2.63777971e-01 -3.44161838e-01  8.51139128e-02\n",
      "  1.64991319e-02 -1.92874104e-01 -2.32554182e-01 -1.99812070e-01\n",
      " -1.00267366e-01 -4.26320553e-01  1.14523992e-01 -2.92101860e-01\n",
      "  2.30328053e-01 -2.95319915e-01 -4.43311363e-01 -4.18336421e-01\n",
      " -4.28687572e-01 -1.20246030e-01 -2.56762356e-01 -5.89224100e-02\n",
      "  3.33227634e-01 -2.03737337e-03  2.40334749e-01  2.05493435e-01\n",
      "  4.57310155e-02  2.87881881e-01  2.20757157e-01  1.46472782e-01\n",
      "  7.32449144e-02 -7.74479330e-01 -3.83707508e-02  1.65719777e-01\n",
      " -2.28759214e-01 -1.97387576e-01  7.00198263e-02  1.87134817e-02\n",
      " -7.30693564e-02 -3.45260561e-01  2.22487271e-01  3.43800366e-01\n",
      " -8.66785049e-02  4.06563729e-01  3.68473321e-01  2.92286575e-01\n",
      " -1.25195682e-01 -3.51407260e-01  2.09915504e-01  5.35975337e-01\n",
      " -1.65721431e-01  4.32584211e-02  9.93185788e-02  9.73620191e-02\n",
      " -3.88062485e-02  2.24945322e-01 -3.33368659e-01 -1.28954977e-01\n",
      "  9.51101333e-02  4.35028136e-01 -1.96423456e-01  4.10780370e-01\n",
      " -1.45265698e-01 -2.42017657e-01 -1.90199877e-03 -3.35262679e-02\n",
      " -6.12497032e-01 -3.10651064e-01  1.78207934e-01  2.66017467e-01\n",
      "  1.75965622e-01 -3.21768112e-02 -6.01331033e-02 -2.96379834e-01\n",
      "  3.84003520e-01  3.00926775e-01 -2.45598882e-01 -9.30133238e-02\n",
      "  2.78520226e-01  2.21227273e-01 -1.48742288e-01  8.47769901e-02\n",
      "  1.41119778e-01  7.70469010e-02 -2.46102750e-01  1.35448292e-01\n",
      "  1.99891716e-01  1.83968022e-01  7.17909113e-02 -1.71604797e-01\n",
      "  1.26320953e-02 -7.31361657e-02  2.37268388e-01  1.71375602e-01\n",
      "  1.16674729e-01  1.02818742e-01 -2.29151323e-01 -7.45766684e-02\n",
      "  6.96940869e-02  2.36512706e-01 -1.89903483e-01  1.31482705e-01\n",
      "  6.76337183e-02  5.34388840e-01 -4.67746705e-02 -8.08431879e-02\n",
      " -3.68405581e-01  5.01846559e-02 -3.25709850e-01  1.51900277e-01\n",
      " -2.15892762e-01  1.94947988e-01 -1.40694259e-02 -9.12185386e-02\n",
      " -1.11433618e-01 -1.86543778e-01  4.29554611e-01 -9.28044245e-02]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 0.02347185  0.2361929   0.3217322  -0.2865988  -0.4204702   0.16793515\n",
      " -0.06503285 -0.18821765 -0.35074228 -0.0388342  -0.04114674 -0.02418019\n",
      "  0.00911521  0.12800787 -0.00828497 -0.27885547  0.0827696  -0.037332\n",
      " -0.04150847  0.29320568  0.35456032  0.3391074   0.10410209  0.09864461\n",
      "  0.07236109  0.1194252  -0.034631    0.16609845 -0.00508204 -0.19332038\n",
      "  0.04343731  0.20161596  0.24281476  0.14139387  0.07999165  0.12685531\n",
      "  0.04284868 -0.22629133  0.06142012 -0.06040164  0.11866331 -0.14797284\n",
      " -0.23328862  0.15265456  0.05504355  0.18422128 -0.03112902  0.27477267\n",
      "  0.3345695  -0.01541092  0.18567693 -0.37219235 -0.17227389 -0.07980123\n",
      "  0.194612    0.10214517  0.00301713  0.12819843  0.01535027  0.07217269\n",
      " -0.096118   -0.08246262  0.05869317 -0.11545657  0.06972835  0.08860402\n",
      " -0.41344547 -0.23670372  0.43807843 -0.16635698 -0.23262812 -0.31192687\n",
      " -0.20224665  0.02228368 -0.02179684 -0.16233109 -0.19641134  0.2747541\n",
      "  0.00407274 -0.15029947 -0.34691486 -0.00873562  0.63332754 -0.36708313\n",
      "  0.25119933  0.21656835 -0.24645469 -0.04043001  0.2240947  -0.08072128\n",
      " -0.09449117 -0.4702505  -0.08208263 -0.03139896  0.11519504 -0.19817126\n",
      "  0.04978627 -0.1785509  -0.10313328 -0.0659536   0.02444435 -0.13123733\n",
      " -0.33636266  0.02891018 -0.06368078  0.05678729  0.35241944  0.07712664\n",
      " -0.5245654   0.3139056  -0.08937988  0.08042506 -0.42075488 -0.00109642\n",
      "  0.00400057  0.05410888 -0.24695472  0.43224248 -0.08920157 -0.33094084\n",
      "  0.06362791 -0.03969448  0.18768752  0.22466199  0.20016176 -0.10707612\n",
      " -0.39158183  0.07842653 -0.18304273 -0.46836478 -0.119642   -0.10920201\n",
      "  0.07763689 -0.0697925   0.11449388  0.25812244  0.1986665   0.15964416\n",
      " -0.13382243 -0.09854056 -0.00596958  0.04885429  0.04673419 -0.05575755\n",
      "  0.20538749 -0.14132214 -0.06639917  0.236155   -0.04744757 -0.22564612\n",
      "  0.2330665   0.22140373 -0.05604783 -0.20201102 -0.2971241  -0.08071466\n",
      "  0.2804052  -0.199067    0.00395572 -0.5219027   0.13760732 -0.35541376\n",
      " -0.04431995  0.05839914  0.06191397  0.09793793 -0.05753183 -0.3595803\n",
      " -0.13715157 -0.04820183 -0.12296937 -0.01006627 -0.22617902  0.02006087\n",
      " -0.10525576  0.11742142 -0.0084455  -0.11040619  0.11284716 -0.38245672\n",
      "  0.25633729 -0.23225002 -0.06413405  0.06617249  0.084814   -0.34514043\n",
      " -0.3396778  -0.17715661  0.194042   -0.05612237 -0.037272   -0.14646578\n",
      " -0.37110943 -0.46087164  0.0755069  -0.25360125  0.3932692  -0.01221082\n",
      "  0.23222525 -0.10776047 -0.3069135   0.0687793  -0.20032646 -0.17632864\n",
      "  0.38649926 -0.16183266  0.12210196  0.01611724  0.19902542  0.11196806\n",
      " -0.14243804 -0.03392161 -0.19035907 -0.49227357 -0.25859097  0.1745352\n",
      " -0.15630679 -0.07076684 -0.0168351   0.25294814 -0.20502658 -0.30178502\n",
      "  0.30254528  0.2938641  -0.20327725  0.34350273  0.22202055  0.01907187\n",
      " -0.15481187 -0.34815082 -0.13440801  0.54728556 -0.03093564 -0.00379173\n",
      "  0.0662566  -0.28610766 -0.09518913 -0.04572272 -0.27061227  0.10845428\n",
      "  0.04459313  0.21445227 -0.18950911  0.42523086 -0.22195265 -0.24056593\n",
      " -0.10844167 -0.05466023 -0.5570039  -0.01027245  0.10073181  0.1541582\n",
      "  0.06576093 -0.19233972 -0.11153558 -0.27134562  0.35208678 -0.06291883\n",
      " -0.3327159  -0.08034527  0.041774    0.02288363 -0.265498   -0.0115723\n",
      " -0.0098399  -0.08453948 -0.24182183  0.15133567  0.23638996 -0.06715371\n",
      "  0.01853692 -0.13527864 -0.00593938 -0.32013378  0.08293592  0.15709476\n",
      "  0.04406644  0.18796974 -0.09694245 -0.20105313  0.21326989 -0.21135868\n",
      " -0.20528238 -0.04445554 -0.09057629  0.27111128  0.05494053  0.4051232\n",
      " -0.2723696   0.02614453 -0.20152412  0.13414682  0.10257585  0.33694944\n",
      "  0.11145366  0.01961389  0.05447352 -0.24309567  0.11807942 -0.06487086]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 2.71332543e-02  1.95533946e-01  2.51049519e-01 -3.27799648e-01\n",
      " -4.42605913e-01  1.73915029e-01 -1.39094263e-01 -1.76031932e-01\n",
      " -4.46361095e-01  5.87970775e-04 -3.01031638e-02 -1.34067878e-01\n",
      "  2.22345740e-02  1.52522445e-01  1.69512536e-02 -2.62487918e-01\n",
      "  6.97653517e-02 -1.18459478e-01 -9.74426717e-02  3.33545208e-01\n",
      "  3.78547341e-01  5.33868849e-01  7.41645694e-02  1.93308085e-01\n",
      "  2.23525297e-02  1.59847155e-01 -7.06282854e-02  2.90636450e-01\n",
      "  4.91730422e-02 -2.55818486e-01  3.75261642e-02  1.50713667e-01\n",
      "  2.68490076e-01  4.86174747e-02 -8.00230820e-03  1.60249069e-01\n",
      "  4.25382480e-02 -1.19201213e-01 -1.28518008e-02 -1.63093373e-01\n",
      "  8.44355524e-02 -2.41903722e-01 -2.70719767e-01  2.12758943e-01\n",
      " -2.89574433e-02  1.17694907e-01  1.09708654e-02  3.81148726e-01\n",
      "  2.33010337e-01 -1.30796999e-01  2.24257350e-01 -4.03049409e-01\n",
      " -9.45294723e-02 -1.36796787e-01  2.08670199e-01  7.11099505e-02\n",
      "  4.59149368e-02  1.92850485e-01 -1.07995354e-01  4.31519141e-03\n",
      " -3.09208389e-02 -1.54312372e-01 -6.99763298e-02 -2.02406198e-01\n",
      "  9.99038815e-02  1.45307466e-01 -4.74915206e-01 -3.09140384e-01\n",
      "  5.05995095e-01 -2.24492028e-01 -2.97155708e-01 -2.82300293e-01\n",
      " -1.14119008e-01  4.95432550e-03  1.45509206e-02 -1.65468305e-01\n",
      " -6.16191626e-02  4.03185159e-01 -1.65595308e-01 -2.71371394e-01\n",
      " -3.14834416e-01 -1.35997534e-01  6.01094306e-01 -3.07372749e-01\n",
      "  1.91059127e-01  2.75439978e-01 -2.95496166e-01 -5.67161329e-02\n",
      "  3.27347964e-01 -9.29893255e-02 -1.95548922e-01 -4.94550228e-01\n",
      " -1.44260034e-01  6.25961926e-03  1.14623301e-01 -2.30218798e-01\n",
      "  3.77713190e-03 -2.41523057e-01 -2.49489635e-01 -2.30904490e-01\n",
      "  4.19507958e-02 -1.65894896e-01 -3.20049345e-01 -9.81220528e-02\n",
      " -9.33603719e-02  4.37487848e-02  3.15749407e-01  8.68884549e-02\n",
      " -5.16089320e-01  3.11927795e-01 -9.28776562e-02  1.18884400e-01\n",
      " -3.45499814e-01  3.64825279e-02  1.99008405e-01  4.02209982e-02\n",
      " -2.09135160e-01  4.58197892e-01 -3.17065492e-02 -3.01944137e-01\n",
      " -1.31480331e-02  1.73182040e-02  1.80506855e-01  2.40225747e-01\n",
      "  3.07650208e-01 -1.91104665e-01 -3.56323808e-01  9.43415239e-02\n",
      " -1.85214713e-01 -4.86170232e-01 -2.96409279e-01 -9.00532752e-02\n",
      " -1.19910007e-02 -7.63227493e-02  1.06315404e-01  3.39994729e-01\n",
      "  2.02454418e-01  1.67140022e-01 -1.03800431e-01 -1.41765382e-02\n",
      " -7.49110505e-02  2.02052593e-01  9.53278989e-02 -9.93266180e-02\n",
      "  1.58124641e-01 -1.85263023e-01 -1.75971866e-01  2.81973958e-01\n",
      " -6.40816391e-02 -3.29781622e-01  2.29485393e-01  1.68315709e-01\n",
      " -6.95833415e-02 -2.91009903e-01 -2.55584359e-01 -4.05163951e-02\n",
      "  3.10402364e-01 -1.58723548e-01 -4.13561612e-02 -4.93355304e-01\n",
      "  1.54808730e-01 -3.60262394e-01 -1.37702271e-01  4.88594063e-02\n",
      "  9.48024094e-02  1.36350587e-01  2.26525310e-02 -3.44494611e-01\n",
      " -2.05959186e-01 -1.19097270e-01 -9.50498804e-02 -7.17551112e-02\n",
      " -2.14244828e-01 -8.06662627e-03 -6.47928193e-02  1.01551495e-01\n",
      " -7.55773187e-02 -2.10347623e-01  5.65930875e-03 -2.96618074e-01\n",
      "  3.15260768e-01 -1.43854097e-01 -6.91677406e-02  1.29953578e-01\n",
      "  9.93982106e-02 -3.79111528e-01 -4.28897351e-01 -1.70314953e-01\n",
      "  1.91434458e-01 -1.62903056e-01 -9.85285193e-02 -2.53911614e-01\n",
      " -3.99471670e-01 -4.09402847e-01  9.29171741e-02 -2.38435403e-01\n",
      "  3.93759727e-01 -1.32571841e-02  2.94172227e-01 -2.24711478e-01\n",
      " -3.02947760e-01  1.29742324e-01 -9.67146307e-02 -1.79108188e-01\n",
      "  3.68827611e-01 -4.63859141e-02  1.58971876e-01  4.54655811e-02\n",
      "  2.49918640e-01  5.19219115e-02 -2.30510831e-01 -5.51415384e-02\n",
      " -1.94608867e-01 -5.07859588e-01 -3.18064153e-01  2.23442867e-01\n",
      " -6.12557046e-02 -1.12176603e-02 -9.14277583e-02  3.15017968e-01\n",
      " -2.43537635e-01 -3.29295695e-01  3.19754958e-01  3.19476217e-01\n",
      " -2.32117921e-01  4.12039638e-01  1.83390111e-01 -3.04760709e-02\n",
      " -1.46641001e-01 -3.41885716e-01 -6.11100234e-02  5.04485250e-01\n",
      " -1.36014625e-01 -4.75852937e-02  1.26926765e-01 -2.48665765e-01\n",
      " -8.26071724e-02  4.32974175e-02 -2.75283456e-01  7.42981359e-02\n",
      "  4.75360341e-02  1.97147280e-01 -1.53362125e-01  4.53581095e-01\n",
      " -3.25136453e-01 -2.45413646e-01 -1.64903045e-01 -3.82290334e-02\n",
      " -5.14363348e-01 -8.09474364e-02  1.00739144e-01  2.03693837e-01\n",
      "  6.91983253e-02 -2.05362350e-01 -8.21256787e-02 -3.98347139e-01\n",
      "  4.17782664e-01 -2.87909550e-03 -3.57517362e-01  4.99644593e-05\n",
      " -3.26566100e-02 -2.90011503e-02 -2.74389386e-01  2.82779243e-02\n",
      " -2.78099179e-02 -1.01542272e-01 -3.92621547e-01  2.35861108e-01\n",
      "  2.53156573e-01 -5.84676601e-02 -1.47636369e-01 -1.74674556e-01\n",
      " -3.26438732e-02 -3.72465253e-01  5.27089313e-02  2.66012400e-01\n",
      "  7.32259639e-03  1.78957999e-01 -7.89625868e-02 -2.67603308e-01\n",
      "  2.78577089e-01 -2.14881361e-01 -2.87926584e-01 -9.85390507e-03\n",
      " -1.04398467e-01  2.46916309e-01  1.17457502e-01  4.46953326e-01\n",
      " -2.80792475e-01 -2.18805596e-02 -1.15546383e-01  1.58082470e-01\n",
      "  1.85064420e-01  3.09978545e-01  4.41282466e-02 -4.28421469e-03\n",
      "  4.13099565e-02 -2.30427593e-01  1.74812183e-01 -5.02770059e-02]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 0.00367043  0.29842913  0.3682745  -0.09544881 -0.43635973  0.26394582\n",
      "  0.04448451 -0.21608648 -0.19379072 -0.07988591  0.01516928  0.16537711\n",
      " -0.05916414  0.14173904 -0.13234249 -0.44461548  0.0851194  -0.03390843\n",
      " -0.0930732   0.22802573  0.29224718  0.06878745  0.08918816  0.0891046\n",
      "  0.03661269  0.1921653   0.06636477  0.00829221 -0.0066907  -0.22800857\n",
      "  0.10660273  0.23391515  0.15451424  0.23013149  0.03241015  0.11866554\n",
      "  0.03134196 -0.2509817   0.08521713  0.0736216   0.20807265 -0.10884818\n",
      " -0.19795708  0.10086542  0.17142624  0.15056507 -0.14224792  0.128615\n",
      "  0.52219254  0.09783027  0.05173571 -0.35133636 -0.2815678  -0.08117057\n",
      "  0.2353367   0.08286028 -0.00756898  0.03814952  0.15032132  0.20627719\n",
      " -0.20233469  0.00132246  0.10004892 -0.0017251  -0.02281108  0.0809434\n",
      " -0.3657897  -0.2427007   0.40377778 -0.17694141 -0.19393447 -0.36521107\n",
      " -0.2430183  -0.02471391 -0.10013565 -0.17084283 -0.37100726  0.0245195\n",
      "  0.14186627  0.07652592 -0.3554161   0.1575202   0.75533885 -0.41861764\n",
      "  0.31636828  0.12189187 -0.23850374 -0.07378163  0.1010517   0.05805133\n",
      "  0.04901748 -0.49333173 -0.04069586 -0.09442648  0.14097334 -0.15978391\n",
      "  0.14048764 -0.1621564   0.2116607   0.0415186   0.0709465  -0.13628\n",
      " -0.38251978  0.03691714 -0.1013374   0.11574724  0.3190848   0.11360373\n",
      " -0.63832176  0.36933967 -0.15242396  0.02320493 -0.48706144 -0.05421891\n",
      " -0.07627129  0.00729555 -0.23174542  0.40454763 -0.06807593 -0.4221041\n",
      "  0.15945178 -0.08900291  0.2581327   0.12631181  0.08578563 -0.03144009\n",
      " -0.40104386  0.02276326 -0.17852281 -0.45902002 -0.00168514 -0.08032876\n",
      "  0.15107    -0.0668584   0.11369233  0.19948609  0.25333294  0.1508142\n",
      " -0.11253119 -0.14979447  0.13503535 -0.05004876 -0.02180745 -0.01624221\n",
      "  0.2278756  -0.0668249  -0.00472822  0.21746527 -0.04427976 -0.21446794\n",
      "  0.15981992  0.16259867 -0.07100055 -0.11094613 -0.36565855 -0.10215135\n",
      "  0.14859736 -0.30369145  0.00987922 -0.5268774   0.12297436 -0.3418465\n",
      "  0.00500291  0.03879191  0.0380315   0.01371537 -0.15050544 -0.4499369\n",
      " -0.03736867  0.04028134 -0.13692284  0.02932433 -0.19597203  0.0871518\n",
      " -0.03769571  0.14251107  0.11848478 -0.05724283  0.2857635  -0.5016781\n",
      "  0.1438733  -0.27428457 -0.09166064 -0.01411854  0.04026219 -0.3956054\n",
      " -0.31483382 -0.17970729  0.20042033 -0.00654491  0.01257436 -0.1053795\n",
      " -0.3124125  -0.6007001   0.12574436 -0.1901688   0.37621975 -0.07264205\n",
      "  0.16305172  0.02673565 -0.25648195  0.05029542 -0.20094998 -0.15742448\n",
      "  0.36553022 -0.19704519  0.17529397 -0.00783492 -0.00882017  0.06407532\n",
      "  0.00347209  0.06140808 -0.2019523  -0.45341963 -0.23929581  0.2994072\n",
      " -0.2929254  -0.10764234  0.14500421  0.22327459 -0.19159466 -0.39871612\n",
      "  0.29071474  0.24266596 -0.19738477  0.3733103   0.27458408  0.05519926\n",
      " -0.1460816  -0.38798887 -0.31103012  0.5971463   0.00237137  0.00915193\n",
      "  0.03255274 -0.29722577 -0.1177304  -0.05379854 -0.23981999  0.13757116\n",
      "  0.07446735  0.23690529 -0.14063348  0.45201305 -0.06654352 -0.14953405\n",
      " -0.18012851 -0.0601691  -0.57872736 -0.06444822  0.05561132  0.09633157\n",
      "  0.01294451 -0.17672302 -0.19547643 -0.23810391  0.3385844  -0.09475093\n",
      " -0.33640543 -0.23962088  0.07988349 -0.04629654 -0.30460176 -0.12318265\n",
      "  0.06520196 -0.02270262 -0.09355737  0.0888626   0.23165041 -0.09573507\n",
      "  0.18678702 -0.13840367  0.01854817 -0.31547242  0.05000571  0.02911122\n",
      "  0.07637687  0.24768871 -0.01559224 -0.20131798  0.12474439 -0.21127555\n",
      " -0.00824989 -0.00825806 -0.1774719   0.3341259  -0.0075468   0.32150355\n",
      " -0.18619917  0.00851188 -0.20357876  0.13636076  0.08629494  0.35817817\n",
      "  0.12023475 -0.02813351  0.04561703 -0.30237737  0.17750219 -0.01972912]\n",
      "\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:08<00:00,  8.96s/it]\u001b[A\n",
      "Epoch:  50%|███████████████████                   | 1/2 [00:11<00:11, 11.19s/it]\n",
      "Evaluating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[Atype of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [-0.0437052   0.18126032  0.24163993 -0.12241691 -0.35775596  0.15817113\n",
      "  0.02952419 -0.14124137 -0.28441596 -0.09388468 -0.04216089  0.02370456\n",
      "  0.01163383  0.13412914  0.02772311 -0.2580163   0.11038472  0.01369261\n",
      " -0.00644023  0.1503242   0.24903038  0.16538444  0.08233606  0.04709032\n",
      "  0.02515708  0.1874316  -0.09367822  0.15142703  0.03126718 -0.19259936\n",
      "  0.05652053  0.19988032  0.17306264  0.16386326  0.05215827  0.06422045\n",
      "  0.02279321 -0.21385106  0.04982905  0.0515744   0.1322156  -0.09792399\n",
      " -0.12307718  0.08884692  0.03887888  0.10044432 -0.02388573  0.13337599\n",
      "  0.34269828  0.03725961  0.17252119 -0.28175393 -0.19971779 -0.03777985\n",
      "  0.17946182  0.08955227 -0.00345953  0.08179796  0.0769163   0.08576398\n",
      " -0.05390439  0.0299295   0.04410189 -0.11386707  0.02344075  0.09751345\n",
      " -0.32437992 -0.24357253  0.36037576 -0.20681906 -0.17325993 -0.18686777\n",
      " -0.13001317 -0.01350316 -0.06658874 -0.16059642 -0.23224926  0.10618199\n",
      "  0.06148258 -0.12516357 -0.25613683  0.09151451  0.49680543 -0.3847383\n",
      "  0.28791597  0.15732746 -0.20269221 -0.028993    0.16990998 -0.01262473\n",
      " -0.050539   -0.31845862  0.00394755  0.01131955  0.10931459 -0.16283794\n",
      "  0.09834945 -0.09498511 -0.01985789 -0.05835922 -0.04912766 -0.09963202\n",
      " -0.27493355 -0.02088352  0.00642092 -0.02886838  0.29695877  0.02963725\n",
      " -0.52001226  0.2174897  -0.00177251  0.02114736 -0.35841045  0.00401563\n",
      "  0.00605049  0.07671271 -0.2495912   0.31680474 -0.05716721 -0.30082557\n",
      "  0.08179557 -0.06654659  0.18974474  0.1839617   0.15803681 -0.03058812\n",
      " -0.33874083 -0.04929578 -0.09452997 -0.3558519  -0.06923202 -0.13877717\n",
      "  0.10795565 -0.02205531  0.07884119  0.24782756  0.14087921  0.03288255\n",
      " -0.13043435 -0.11722144  0.00776817 -0.02029484  0.06845222 -0.05211204\n",
      "  0.15243456 -0.01954093  0.00410901  0.09823283 -0.01195442 -0.21854834\n",
      "  0.13578823  0.15830852 -0.04826091 -0.07347626 -0.26212764 -0.05020856\n",
      "  0.19968386 -0.2055695  -0.02825753 -0.39806822  0.21011983 -0.28973264\n",
      " -0.07594419  0.02674435  0.03859829  0.0572245  -0.08764871 -0.33702934\n",
      " -0.10835814 -0.03468564 -0.05392177  0.01157449 -0.23695229  0.06627209\n",
      " -0.01945658  0.08455709  0.10249418 -0.14363529  0.19813013 -0.29808465\n",
      "  0.15149763 -0.20154245 -0.0114397   0.04395403  0.0238233  -0.29094082\n",
      " -0.16518812 -0.14360209  0.19606745  0.01524745  0.00376432 -0.12692352\n",
      " -0.3137633  -0.4068391   0.07136444 -0.16548648  0.28683102  0.00727743\n",
      "  0.22335336  0.04788792 -0.2494571   0.03256259 -0.18751971 -0.13231117\n",
      "  0.31268594 -0.14491218  0.12058013 -0.02914888  0.04871011  0.09060958\n",
      " -0.11971989 -0.00982043 -0.18312739 -0.28275472 -0.24134597  0.2200214\n",
      " -0.18203641 -0.00867527  0.09435589  0.15535384 -0.12893544 -0.23386613\n",
      "  0.22965498  0.18189357 -0.19062184  0.25854224  0.20012116 -0.05804597\n",
      " -0.15224044 -0.28242865 -0.28517467  0.44492692  0.07134108 -0.033865\n",
      "  0.00661708 -0.3185968  -0.09892001 -0.09810058 -0.2372674   0.02914422\n",
      " -0.00345888  0.16557278 -0.09419833  0.402553   -0.14165454 -0.15547696\n",
      " -0.15751007 -0.02129118 -0.47417054  0.06405482  0.0508273   0.07339331\n",
      " -0.04950434 -0.16378058 -0.06178479 -0.20984042  0.261975   -0.03713617\n",
      " -0.27490672 -0.09897012 -0.03654451 -0.02609473 -0.22229877 -0.10405676\n",
      "  0.01736228 -0.0299616  -0.16069081  0.11035952  0.17085102 -0.13461551\n",
      "  0.11820506 -0.11626856 -0.05775748 -0.30037412  0.04785211  0.02978676\n",
      "  0.02587016  0.19247855  0.03286473 -0.17332332  0.10991823 -0.19446316\n",
      " -0.10185219  0.03444085 -0.02309558  0.2472342   0.04568168  0.3531691\n",
      " -0.17589317 -0.00751846 -0.1013115   0.09861632  0.12267366  0.22707006\n",
      "  0.01318465  0.03043661  0.0937885  -0.25421718  0.07490831 -0.03132427]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 2.86439620e-02  1.32207721e-01  2.20864639e-01 -3.76842171e-01\n",
      " -3.72460365e-01  1.33762538e-01 -1.11782312e-01 -1.95963651e-01\n",
      " -4.29762244e-01 -6.13881275e-02 -5.01801334e-02 -1.14480272e-01\n",
      "  3.79702449e-02  8.57558399e-02  3.77053842e-02 -2.29364559e-01\n",
      "  1.25184640e-01 -7.26585910e-02 -1.30959572e-02  3.35741788e-01\n",
      "  3.63114923e-01  4.69504267e-01  1.21355727e-01  1.04280524e-01\n",
      "  5.26734069e-02  1.09627672e-01 -3.83718126e-02  3.03808033e-01\n",
      "  3.87387164e-02 -1.60844937e-01 -2.92512365e-02  1.44868538e-01\n",
      "  2.55107313e-01  3.90699245e-02  1.11373059e-01  1.31304651e-01\n",
      "  3.48925143e-02 -1.86735824e-01  8.62720516e-03 -1.33301690e-01\n",
      "  1.22481257e-01 -2.57251114e-01 -2.54261345e-01  1.56974152e-01\n",
      " -1.83713976e-02  1.82353079e-01  9.76375863e-03  3.54908764e-01\n",
      "  2.03625649e-01 -9.09456015e-02  2.05310687e-01 -3.54018569e-01\n",
      " -1.12836763e-01 -8.10620040e-02  1.91638544e-01  7.71405697e-02\n",
      " -4.51715365e-02  1.58023328e-01 -9.79616418e-02 -3.14380638e-02\n",
      "  2.01581810e-02 -1.35279447e-01  2.80300099e-02 -1.33792758e-01\n",
      "  1.20854571e-01  7.91460425e-02 -3.51765275e-01 -2.18994632e-01\n",
      "  4.35337901e-01 -2.05615327e-01 -1.90896541e-01 -2.45239437e-01\n",
      " -1.84460491e-01  1.94002818e-02  5.27941473e-02 -1.32812306e-01\n",
      " -3.42590958e-02  3.15368146e-01 -1.16558112e-01 -3.02547395e-01\n",
      " -3.20322096e-01 -1.33574009e-01  4.78194565e-01 -3.92426014e-01\n",
      "  2.08309174e-01  3.39031756e-01 -2.73654044e-01  1.38269560e-02\n",
      "  2.71621525e-01 -8.80199596e-02 -2.62772560e-01 -5.02216756e-01\n",
      " -6.69321045e-02 -2.14584498e-03  1.25775620e-01 -1.75788149e-01\n",
      "  6.17645308e-02 -1.50758341e-01 -2.73532957e-01 -2.94575632e-01\n",
      " -2.51323916e-02 -1.17814839e-01 -3.08271110e-01 -1.03163933e-02\n",
      " -1.36605417e-02  2.41468940e-03  2.85800636e-01  1.33052766e-02\n",
      " -3.70139152e-01  3.00331622e-01 -4.81194444e-02  1.68799311e-01\n",
      " -2.63993651e-01  8.60110670e-02  8.76122490e-02  9.98986587e-02\n",
      " -1.99359462e-01  4.09596562e-01 -4.42251526e-02 -2.38710210e-01\n",
      "  2.36602537e-02  1.36987376e-03  1.28877848e-01  2.62507498e-01\n",
      "  2.62149870e-01 -1.97484478e-01 -3.53082806e-01  1.04056679e-01\n",
      " -1.56652763e-01 -4.06144589e-01 -2.84591645e-01 -1.02080114e-01\n",
      " -2.86938064e-02 -6.09352514e-02  8.21916759e-02  2.56826192e-01\n",
      "  1.73720792e-01  1.51589707e-01 -6.30476996e-02 -8.17871094e-02\n",
      " -1.12366617e-01  1.30959779e-01  1.27046034e-02 -6.31692559e-02\n",
      "  1.07889473e-01 -1.65582329e-01 -1.47805989e-01  2.33705088e-01\n",
      " -3.74619886e-02 -2.20148146e-01  2.87236571e-01  1.99647680e-01\n",
      " -4.76813391e-02 -2.64530271e-01 -2.16634989e-01  1.15345744e-02\n",
      "  2.47447878e-01 -9.89549011e-02 -2.80706082e-02 -3.88189137e-01\n",
      "  1.42920151e-01 -3.59762162e-01 -8.10841992e-02  1.01764269e-01\n",
      "  8.31346139e-02  1.00577161e-01  3.45424786e-02 -2.48236224e-01\n",
      " -1.07181735e-01 -6.75614178e-02 -6.44491836e-02 -4.89823893e-02\n",
      " -2.33009234e-01 -6.33458868e-02 -8.82738382e-02  7.40382299e-02\n",
      " -8.03533345e-02 -1.44543067e-01  7.63970986e-02 -3.18947583e-01\n",
      "  2.89628178e-01 -1.50599971e-01 -2.41101235e-02  1.49912611e-01\n",
      "  9.38058347e-02 -3.02335531e-01 -3.37606698e-01 -1.21548280e-01\n",
      "  1.66871786e-01 -1.71397075e-01 -1.04894467e-01 -1.82981566e-01\n",
      " -3.42312127e-01 -3.09644639e-01  4.73868959e-02 -3.16971451e-01\n",
      "  3.68008941e-01  5.05144373e-02  2.76230961e-01 -1.51069731e-01\n",
      " -2.66790003e-01  7.03691542e-02 -1.93109021e-01 -1.34049714e-01\n",
      "  3.54164690e-01  1.17006013e-02  9.70426276e-02 -2.40991749e-02\n",
      "  3.10293704e-01  7.76693299e-02 -1.45737320e-01 -7.88767859e-02\n",
      " -1.57823205e-01 -4.48737174e-01 -2.53488719e-01  9.37040150e-02\n",
      " -9.87066776e-02  1.92610193e-02 -9.75644812e-02  2.38835573e-01\n",
      " -1.42071813e-01 -3.11811626e-01  3.09878767e-01  3.34662974e-01\n",
      " -1.84977517e-01  3.19440335e-01  1.88990444e-01 -1.62629150e-02\n",
      " -8.87964591e-02 -2.95785278e-01 -7.47474134e-02  4.70160365e-01\n",
      " -8.06762204e-02 -4.02238406e-02  1.20202437e-01 -2.80234247e-01\n",
      " -3.62417884e-02 -2.87628379e-02 -3.01855177e-01  2.75903158e-02\n",
      "  4.08171080e-02  1.82857916e-01 -1.27118170e-01  4.24553722e-01\n",
      " -3.49958628e-01 -2.25146487e-01 -1.05699830e-01 -9.09502525e-03\n",
      " -4.54458028e-01 -3.38752083e-02  1.09642759e-01  1.44095197e-01\n",
      "  8.22407380e-02 -2.04875603e-01  9.37458035e-03 -3.79727095e-01\n",
      "  3.41898441e-01 -5.21698892e-02 -2.37854838e-01  7.07890512e-03\n",
      " -5.57419248e-02  4.02038768e-02 -1.84759930e-01  3.77397228e-04\n",
      "  1.50577985e-02 -1.01822875e-01 -3.30220282e-01  1.64145604e-01\n",
      "  1.94615543e-01 -3.56110074e-02 -1.94010660e-02 -1.07352965e-01\n",
      " -4.02912870e-02 -2.96587348e-01  3.38090397e-02  2.20087126e-01\n",
      " -3.01006734e-02  9.19061303e-02 -1.22444682e-01 -1.93417415e-01\n",
      "  2.20261052e-01 -1.72908098e-01 -3.60445023e-01 -1.11876605e-02\n",
      " -2.11757384e-02  2.09443718e-01  6.40729815e-02  3.93470466e-01\n",
      " -2.61181384e-01  2.05735676e-02 -1.83914140e-01  7.40498826e-02\n",
      "  6.88317940e-02  2.24522084e-01  3.08741108e-02  6.21070787e-02\n",
      "  2.36352142e-02 -2.08520353e-01  8.94243643e-02 -1.41677350e-01]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [-0.00554481  0.16270301  0.11816417 -0.18932016 -0.24410428  0.11305538\n",
      "  0.0066158  -0.16408604 -0.23837398 -0.02240835 -0.02700213 -0.01051871\n",
      "  0.04561871  0.1166358   0.02032206 -0.21094693  0.08432996 -0.0590212\n",
      " -0.00699788  0.23581669  0.2674314   0.29137564  0.09985496  0.07582866\n",
      " -0.02374565  0.1435208  -0.04374712  0.21763125  0.02492935 -0.13816957\n",
      "  0.10335153  0.10767283  0.23892818  0.12237868  0.06824565  0.01012804\n",
      "  0.06418323 -0.12440952 -0.00912936 -0.00120263  0.10607437 -0.16084032\n",
      " -0.05184167  0.09947509 -0.03400184  0.11706319 -0.02416147  0.15600713\n",
      "  0.1530483  -0.03683852  0.24254642 -0.21873188 -0.17063525 -0.07094907\n",
      "  0.17934588  0.12476736  0.00639125  0.14686672 -0.00170905 -0.0019633\n",
      "  0.02613473 -0.05467362  0.07726699 -0.18130772  0.04390783  0.06982994\n",
      " -0.34010532 -0.2634753   0.34441012 -0.15368333 -0.11060961 -0.18540299\n",
      " -0.11247732  0.0244591  -0.04529782 -0.15428987 -0.12515733  0.27044666\n",
      " -0.036216   -0.20623104 -0.19398302  0.03744221  0.4119569  -0.23843226\n",
      "  0.28136086  0.19473876 -0.23135485 -0.0502193   0.11771774 -0.06001556\n",
      " -0.19094692 -0.34144658 -0.09608068 -0.00856279  0.06589492 -0.17184725\n",
      " -0.00828866 -0.14439689 -0.0970443  -0.05982655 -0.0347022  -0.13340068\n",
      " -0.22812065  0.01521316 -0.07398391 -0.08903968  0.2712715   0.02153601\n",
      " -0.42489952  0.21848522 -0.06114459  0.11092243 -0.2765601   0.008692\n",
      "  0.08641125  0.06345025 -0.23373118  0.26024854 -0.08016209 -0.2567866\n",
      " -0.02517621 -0.10242913  0.17314021  0.20410883  0.17603108 -0.06707264\n",
      " -0.32835758 -0.05524067 -0.08138232 -0.39666313 -0.1114457  -0.20327947\n",
      "  0.02806743  0.07956998  0.10750118  0.22186162  0.11806121 -0.04289149\n",
      " -0.10976982 -0.07185262 -0.09686554 -0.01797089  0.04170915 -0.02780639\n",
      "  0.19540192 -0.09368991 -0.00245716  0.08926154 -0.01254061 -0.14704736\n",
      "  0.16969846  0.15445866  0.04542046 -0.19614045 -0.16380969 -0.01727139\n",
      "  0.20008637 -0.05027199 -0.0458102  -0.41398197  0.1504032  -0.20077899\n",
      " -0.10394252  0.04232273  0.04651465  0.11001523 -0.0202247  -0.18367536\n",
      " -0.11554874 -0.04932821 -0.05924549 -0.03480347 -0.22097781  0.0275392\n",
      " -0.0470655   0.07809444  0.01795579 -0.05310566  0.10257627 -0.11016271\n",
      "  0.19628713 -0.14618893  0.00442179  0.0271051   0.12237366 -0.31464913\n",
      " -0.18238492 -0.10463963  0.13076626 -0.01162485 -0.08891828 -0.08823722\n",
      " -0.25212067 -0.28781378  0.05012014 -0.14564835  0.19937077  0.03018776\n",
      "  0.18134502 -0.04486308 -0.16744636  0.05166282 -0.08237102 -0.13289385\n",
      "  0.2944301  -0.14036404  0.05262888 -0.01671408  0.13734959  0.0200449\n",
      " -0.11321139 -0.06527427 -0.09182124 -0.24690102 -0.16264407  0.10338883\n",
      " -0.15849228  0.01734716 -0.04605064  0.18901649 -0.1256046  -0.2210029\n",
      "  0.14092013  0.21218392 -0.17290188  0.24320447  0.23463064 -0.04285651\n",
      " -0.10443517 -0.3301485  -0.19253291  0.36243     0.02323617  0.00312987\n",
      "  0.05495926 -0.29443356 -0.03534667 -0.07870009 -0.21005854  0.03640048\n",
      "  0.02201689  0.18626386 -0.10828619  0.3619784  -0.19616883 -0.09989437\n",
      " -0.18079111 -0.07971364 -0.41390875  0.13965133  0.03687089  0.12105251\n",
      "  0.00394691 -0.14209329 -0.0347399  -0.22583681  0.2815634  -0.06659527\n",
      " -0.2531663   0.02970012 -0.04955916  0.00217856 -0.1506077   0.02797176\n",
      "  0.02178026 -0.06700382 -0.18636437  0.10900405  0.16858292 -0.00849553\n",
      " -0.01518765 -0.11122853 -0.03617311 -0.2126671   0.08403106  0.12724347\n",
      " -0.04368412  0.23430611  0.03912516 -0.12007798  0.12424298 -0.22151902\n",
      " -0.2053858  -0.01487197 -0.08883716  0.1433365  -0.00101137  0.3374709\n",
      " -0.17042774 -0.02715663 -0.09932847  0.06907269  0.17294525  0.23683566\n",
      "  0.04769035  0.06383867  0.13023241 -0.17950152  0.05144457 -0.09084719]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 5.03322817e-02  3.97464149e-02  1.75214991e-01 -3.86063665e-01\n",
      " -4.47733283e-01  1.39976844e-01 -2.22978100e-01 -1.60881847e-01\n",
      " -5.71342647e-01  6.08707182e-02 -5.79827838e-02 -1.36971891e-01\n",
      "  3.35060842e-02  4.44304086e-02  1.53498679e-01 -1.42636392e-02\n",
      " -3.05961613e-02 -1.54047012e-01 -6.61423951e-02  3.80777448e-01\n",
      "  3.08626473e-01  7.01633453e-01 -2.19291095e-02  1.77089170e-01\n",
      "  1.91092715e-01  7.14210644e-02  5.18501289e-02  3.47649813e-01\n",
      "  9.74194631e-02 -2.54510880e-01 -6.64574206e-02  1.86659709e-01\n",
      "  2.92458832e-01 -1.24049485e-02  8.59379489e-03  1.47325948e-01\n",
      "  6.65884018e-02 -1.10742606e-01  5.00771776e-02 -3.56411844e-01\n",
      " -1.82924625e-02 -3.60045344e-01 -2.18960643e-01  1.64184719e-01\n",
      " -6.73875287e-02  1.05955794e-01  1.48111820e-01  5.36484599e-01\n",
      "  1.29923612e-01 -2.29961962e-01  1.44348383e-01 -5.11837602e-01\n",
      "  3.08383279e-03 -1.95421636e-01  8.66028741e-02 -3.56104672e-02\n",
      "  5.48838861e-02  2.18092978e-01 -2.53983319e-01 -6.51487038e-02\n",
      " -1.62268872e-03 -2.69219249e-01 -6.14277311e-02 -2.28415772e-01\n",
      "  2.50143707e-01  1.31095186e-01 -4.06433135e-01 -3.10395360e-01\n",
      "  5.28052270e-01 -1.89381808e-01 -3.28056157e-01 -1.55166149e-01\n",
      " -1.15072407e-01 -2.01930255e-02  1.77411646e-01 -1.73586279e-01\n",
      "  1.95102185e-01  4.83224392e-01 -2.24421665e-01 -2.83971190e-01\n",
      " -3.95135909e-01 -2.71542132e-01  4.92210090e-01 -1.95328385e-01\n",
      "  6.85721859e-02  2.41886511e-01 -2.26038069e-01 -1.96941886e-02\n",
      "  3.67050529e-01 -1.46218270e-01 -3.26265335e-01 -5.65789282e-01\n",
      " -1.50307501e-02  7.18953907e-02  8.63574073e-02 -1.79247782e-01\n",
      "  6.26447890e-03 -2.79704958e-01 -4.99845713e-01 -3.74246299e-01\n",
      "  8.16269666e-02 -2.73902882e-02 -3.06528628e-01 -7.26985112e-02\n",
      " -1.35709783e-02  9.16610751e-03  2.60129571e-01  6.97537661e-02\n",
      " -4.57620919e-01  2.89968103e-01 -1.85306240e-02  6.31631836e-02\n",
      " -1.98531196e-01  1.54383719e-01  3.02458763e-01  7.11700320e-02\n",
      " -1.59126014e-01  4.99386251e-01  1.02042146e-02 -1.97137862e-01\n",
      " -6.73060119e-02  1.11420192e-01  2.13821847e-02  2.75004685e-01\n",
      "  4.24289465e-01 -3.41904134e-01 -2.56709963e-01  2.87133038e-01\n",
      " -2.14795992e-01 -3.45534682e-01 -4.41980571e-01  6.43547475e-02\n",
      " -1.65238585e-02 -1.38052002e-01 -7.76298158e-03  3.78302187e-01\n",
      "  1.61345497e-01  2.67102212e-01 -6.62133023e-02  8.61069784e-02\n",
      " -1.38846233e-01  2.95060992e-01  1.53953284e-01 -1.55314580e-01\n",
      " -1.65170897e-02 -2.26327121e-01 -2.08109245e-01  4.07012224e-01\n",
      " -7.24626854e-02 -3.95002127e-01  3.07249546e-01  1.35104880e-01\n",
      " -1.24156825e-01 -3.06497186e-01 -3.28058869e-01 -8.00267458e-02\n",
      "  2.81437844e-01 -1.65714413e-01 -7.64165446e-02 -3.11620295e-01\n",
      "  1.58003658e-01 -4.00837481e-01 -1.19439162e-01  1.43191501e-01\n",
      "  1.45009711e-01  1.68844208e-01  9.89949182e-02 -3.24211568e-01\n",
      " -1.93031475e-01 -1.28061667e-01 -6.93763122e-02 -4.07620594e-02\n",
      " -1.09325379e-01 -5.91699369e-02 -7.17300028e-02  4.70745489e-02\n",
      " -1.01947553e-01 -2.71906823e-01 -1.60654008e-01 -2.86641866e-01\n",
      "  2.98192501e-01 -1.81977332e-01 -8.43457431e-02  2.09755778e-01\n",
      "  3.59768420e-02 -2.16624126e-01 -5.07233083e-01 -1.37775496e-01\n",
      "  1.48334339e-01 -3.00996840e-01 -1.93075359e-01 -3.09525460e-01\n",
      " -3.13419580e-01 -2.64189601e-01  1.04223594e-01 -2.12621510e-01\n",
      "  3.76699179e-01 -3.47157940e-02  3.74832839e-01 -2.70276159e-01\n",
      " -2.77819246e-01  1.71924636e-01 -1.09678663e-01 -1.80572540e-01\n",
      "  3.20512235e-01  1.84896588e-01  1.31667659e-01  1.17365226e-01\n",
      "  3.69853765e-01  2.14528218e-01 -2.10341543e-01 -7.94930309e-02\n",
      " -9.90606621e-02 -5.36628187e-01 -3.14297557e-01  6.71846196e-02\n",
      "  7.43091479e-02  2.75064092e-02 -1.99568644e-01  3.32804710e-01\n",
      " -1.95104033e-01 -2.87144691e-01  3.51654887e-01  3.14516276e-01\n",
      " -1.51875839e-01  4.03607756e-01  1.38546020e-01  3.61658707e-02\n",
      " -2.07581043e-01 -2.93851674e-01  7.01355040e-02  4.16455746e-01\n",
      " -2.36399040e-01 -1.18366562e-01  1.38189360e-01 -1.08191967e-01\n",
      " -2.41815485e-02  1.07896484e-01 -3.01814079e-01  3.10358815e-02\n",
      " -6.04469189e-03  1.01093866e-01 -4.77739833e-02  4.13502663e-01\n",
      " -3.88662130e-01 -4.45850700e-01 -1.58395693e-01  2.62212195e-02\n",
      " -4.64804530e-01 -1.04389124e-01  1.06480815e-01  1.84089035e-01\n",
      "  2.07548767e-01 -1.10448167e-01 -7.03176409e-02 -4.98768002e-01\n",
      "  3.47466022e-01  7.81104863e-02 -2.13368103e-01  8.21514279e-02\n",
      " -1.60964672e-02  1.24699995e-01 -2.90489435e-01  5.01294471e-02\n",
      " -1.52181871e-02 -1.01950683e-01 -5.47823906e-01  3.47285599e-01\n",
      "  2.45635614e-01  3.92123312e-02 -3.38271677e-01 -1.56516626e-01\n",
      " -8.77533332e-02 -2.27766484e-01 -5.69214346e-04  3.54710281e-01\n",
      "  8.23883060e-03  7.82069638e-02 -9.56665501e-02 -2.22897723e-01\n",
      "  3.89234960e-01 -1.08030431e-01 -2.75107443e-01  5.66439889e-03\n",
      " -7.02113435e-02  3.40084225e-01  2.16436282e-01  3.95232707e-01\n",
      " -2.97890216e-01  8.30795318e-02 -1.76784024e-01  3.00274696e-03\n",
      "  1.07730083e-01  2.17270374e-01 -6.44174293e-02 -2.51569394e-02\n",
      "  3.76593359e-02 -2.21347466e-01  2.20540524e-01  6.59177378e-02]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 0.0551611   0.15436967  0.11152051 -0.26203102 -0.21134415  0.07011095\n",
      " -0.03217602 -0.18990329 -0.2510125   0.00606235 -0.01413728 -0.03755759\n",
      "  0.00054246  0.01224527 -0.05000494 -0.17598869  0.03611919 -0.10831079\n",
      " -0.04853027  0.27713707  0.24794333  0.3522783   0.08824891  0.130281\n",
      " -0.01105722  0.09727228  0.01767351  0.21389353  0.07188854 -0.13870573\n",
      "  0.11895604  0.05854179  0.24371214  0.04163129  0.09745131  0.0038377\n",
      "  0.11914458 -0.06963407  0.0565459  -0.13646986  0.03953319 -0.29181135\n",
      " -0.0918492   0.04442445 -0.01956743  0.13557185 -0.06927072  0.23148148\n",
      "  0.039174   -0.06011683  0.20168017 -0.2021301  -0.10068299 -0.09997265\n",
      "  0.13100195  0.07457072 -0.00441231  0.14154953 -0.11001641 -0.00571758\n",
      "  0.05139741 -0.10893419  0.10948909 -0.18373589  0.11425963  0.00657801\n",
      " -0.33966267 -0.23336661  0.3253697  -0.09063186 -0.04990222 -0.11614347\n",
      " -0.1151875   0.07429557  0.02418428 -0.10425454 -0.02285998  0.4433687\n",
      " -0.10408135 -0.19968061 -0.22840796 -0.04392048  0.37286586 -0.09189133\n",
      "  0.20738944  0.15294169 -0.1842643  -0.04258133  0.08866858 -0.11924853\n",
      " -0.29209015 -0.37313813 -0.13618857 -0.03690903  0.01005147 -0.13088468\n",
      " -0.06737029 -0.14724004 -0.18505293 -0.05556395 -0.00949559 -0.09329488\n",
      " -0.2531864   0.10970406 -0.08755483 -0.08518036  0.21688218  0.08515244\n",
      " -0.3357627   0.21376549 -0.09415445  0.17845334 -0.17121823  0.03482872\n",
      "  0.128587    0.03263604 -0.23893443  0.3003019  -0.06219864 -0.24787502\n",
      " -0.06624338 -0.05411895  0.07674786  0.22625501  0.17458224 -0.20150866\n",
      " -0.2942625   0.04508144 -0.14148653 -0.3511738  -0.17406149 -0.15221906\n",
      " -0.09028701  0.08426004  0.11817682  0.2136169   0.12050965 -0.03311785\n",
      " -0.06042686 -0.04179022 -0.16541958 -0.01291491  0.01931853 -0.09263662\n",
      "  0.22592092 -0.14532535  0.01903032  0.18890059 -0.06292529 -0.11640713\n",
      "  0.22400807  0.14012624  0.11571559 -0.28949997 -0.16696844  0.01419347\n",
      "  0.18751073 -0.03428835  0.00423218 -0.36661613  0.12299334 -0.19808905\n",
      " -0.03150475  0.07791661  0.07230818  0.1691909  -0.00771285 -0.11956253\n",
      " -0.14049634 -0.021983   -0.09873204 -0.03072813 -0.16793278  0.02034567\n",
      " -0.06312243  0.08679938 -0.08461587  0.03628867 -0.01332903 -0.00045461\n",
      "  0.16128185 -0.08862259  0.05211634  0.05077975  0.15771614 -0.25326756\n",
      " -0.2268752  -0.0698185   0.05882252 -0.03180622 -0.17010152 -0.1371377\n",
      " -0.18061288 -0.22035529 -0.0069375  -0.10753407  0.16893667 -0.00067004\n",
      "  0.1915609  -0.10264086 -0.13940454  0.07001916 -0.04754173 -0.18482931\n",
      "  0.31750837 -0.07314801  0.01075945  0.08493323  0.23026739 -0.04581034\n",
      " -0.14232616 -0.10250149 -0.01033864 -0.2556141  -0.12127516  0.0156548\n",
      " -0.12535702  0.04487602 -0.11680499  0.19966659 -0.15421143 -0.19273607\n",
      "  0.08360708  0.22719158 -0.15911295  0.21107784  0.23970363 -0.03988938\n",
      " -0.13805595 -0.28805336 -0.030261    0.28260046 -0.05807213 -0.03599579\n",
      "  0.02412938 -0.22946091  0.02255348 -0.06122725 -0.19534129  0.04431972\n",
      "  0.04976992  0.160408   -0.08014765  0.3652847  -0.22306012 -0.07274929\n",
      " -0.17866544 -0.11283645 -0.4138888   0.08080659  0.04057552  0.11235683\n",
      "  0.1313146  -0.13037455 -0.00893275 -0.27474797  0.24160041 -0.07523949\n",
      " -0.15375021  0.04923205  0.01528909  0.08485462 -0.09344752  0.19404152\n",
      "  0.06652772 -0.08195377 -0.18370573  0.13112536  0.17291538  0.04582424\n",
      " -0.07552928 -0.05976304 -0.00849041 -0.06869335  0.11938768  0.13563748\n",
      " -0.10010089  0.25493258 -0.01607354 -0.10858011  0.19151726 -0.22707447\n",
      " -0.2242013  -0.1076886  -0.12816933  0.11828514  0.00212809  0.3436233\n",
      " -0.1786378  -0.03825357 -0.12760028  0.03302714  0.15628874  0.3383203\n",
      "  0.1493542   0.06994109  0.07824708 -0.11679594  0.06361394 -0.16313684]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [ 0.0237671   0.15397403  0.09088863 -0.16342889 -0.19122586  0.14733785\n",
      "  0.00719429 -0.17789921 -0.18265751 -0.00106256  0.00920894 -0.04142069\n",
      "  0.00094129  0.11057006 -0.08284782 -0.3035283   0.03246203 -0.09232224\n",
      " -0.0490128   0.23407312  0.24486782  0.29557726  0.09022793  0.12519172\n",
      " -0.0944047   0.1746862  -0.06691027  0.2173052   0.06528984 -0.12949508\n",
      "  0.16998975  0.041762    0.25948742  0.04420377  0.0097483   0.01513898\n",
      "  0.07503887 -0.01290198 -0.02307578 -0.09210494  0.11408954 -0.18344508\n",
      " -0.03331006  0.13322625 -0.0476916   0.11220668 -0.06143882  0.1168603\n",
      "  0.08411415 -0.04273463  0.28992775 -0.16110548 -0.16651449 -0.06365447\n",
      "  0.1312975   0.10747579 -0.03424713  0.14181319 -0.02469914  0.01310389\n",
      "  0.07160881 -0.06956665  0.05711421 -0.16840035  0.07190067  0.07317716\n",
      " -0.3475644  -0.30075675  0.32747418 -0.12534845 -0.08000716 -0.16416149\n",
      " -0.08890714  0.02615283 -0.05754015 -0.10563825 -0.09988266  0.37240154\n",
      " -0.07007118 -0.18604735 -0.18951125  0.02034863  0.39048234 -0.07787148\n",
      "  0.2589107   0.16111365 -0.21065643 -0.06649956  0.10822777 -0.04311127\n",
      " -0.17912914 -0.29772764 -0.1925633  -0.01928114  0.02201521 -0.12097635\n",
      " -0.04416092 -0.12514003 -0.0837964  -0.06508274 -0.03270009 -0.15815921\n",
      " -0.1965444   0.00788716 -0.1031884  -0.09193823  0.20360155  0.10303259\n",
      " -0.41608787  0.18285893 -0.09295008  0.16631931 -0.20732361 -0.02790208\n",
      "  0.14637744  0.04221412 -0.23415554  0.23045957 -0.04181997 -0.29101938\n",
      " -0.03796975 -0.0936243   0.13757618  0.21416196  0.13934909 -0.13905077\n",
      " -0.28013155 -0.05401985 -0.03188023 -0.38870755 -0.13503172 -0.1922384\n",
      " -0.06996997  0.12143572  0.13610081  0.19825244  0.12177993 -0.10478483\n",
      " -0.1015295  -0.04876288 -0.12072907 -0.01497241  0.02573222 -0.06869053\n",
      "  0.26509747 -0.09588378  0.0444182   0.11336451 -0.08451527 -0.17930241\n",
      "  0.14800867  0.1272545   0.11649396 -0.24957931 -0.08051097  0.00749745\n",
      "  0.14896579 -0.00917684 -0.03902965 -0.39382184  0.11324531 -0.14739612\n",
      " -0.12303047  0.02926486  0.03987403  0.09709913 -0.03331297 -0.12447587\n",
      " -0.15864283 -0.05141372 -0.06011678 -0.05729435 -0.19874553 -0.00431592\n",
      " -0.05015671  0.08820258 -0.01582246 -0.03303721  0.06600486  0.02349031\n",
      "  0.12850988 -0.04413114  0.02742834  0.00321498  0.18223013 -0.31180832\n",
      " -0.16237271 -0.07303111  0.08625747 -0.0050012  -0.12009209 -0.11143928\n",
      " -0.22209026 -0.2734555   0.02672977 -0.05762913  0.1398431  -0.03852341\n",
      "  0.1925962  -0.11890788 -0.12438858  0.05985598  0.01875919 -0.15527359\n",
      "  0.24674916 -0.11725033  0.03348049  0.02306984  0.12463691 -0.1060456\n",
      " -0.14695391 -0.0703614  -0.07664716 -0.17808437 -0.10351347  0.11424895\n",
      " -0.18052752  0.07812738 -0.07668815  0.18690316 -0.12563929 -0.23441969\n",
      "  0.05731265  0.16788521 -0.22575705  0.2505284   0.21341214 -0.04584896\n",
      " -0.04351823 -0.28555813 -0.12706837  0.28620353 -0.02450173 -0.02937343\n",
      "  0.10843664 -0.30639246 -0.01770416 -0.07317666 -0.15185475  0.01110119\n",
      "  0.06860563  0.16776955 -0.07007723  0.38211456 -0.18694834 -0.00084925\n",
      " -0.25393012 -0.12200981 -0.36557132  0.09533256  0.03536293  0.13981931\n",
      "  0.04009262 -0.14951755 -0.05707613 -0.2314672   0.2799374  -0.06203289\n",
      " -0.25899586  0.04501414 -0.03421952 -0.04878368 -0.11552119  0.15211995\n",
      "  0.03751462 -0.04623982 -0.19873644  0.11968644  0.16954401  0.02392025\n",
      " -0.04968829 -0.10475723  0.00043051 -0.18897201  0.08763764  0.0893101\n",
      " -0.06458398  0.28718138  0.05927501 -0.14450635  0.1293173  -0.25674722\n",
      " -0.202373   -0.01396269 -0.16968101  0.11120326 -0.05546251  0.3325532\n",
      " -0.12914494 -0.09539311 -0.02850733  0.09741274  0.25506273  0.3235386\n",
      "  0.11555681  0.04822047  0.10981786 -0.16681688  0.07469079 -0.12167764]\n",
      "type of n;  <class 'int'>\n",
      "Value of n:  1\n",
      "type of emb:  <class 'numpy.ndarray'>\n",
      "PRINTING emb:  [-0.04742591  0.27173376  0.30626348 -0.08454917 -0.40446144  0.22837608\n",
      "  0.06217035 -0.18586674 -0.1915247  -0.09334826 -0.02641004  0.11489245\n",
      " -0.00167005  0.16300234 -0.08170718 -0.41163084  0.07393508 -0.00907103\n",
      " -0.066328    0.18908487  0.29935735  0.07520343  0.10407545  0.04246371\n",
      "  0.01482818  0.19147962 -0.01286229  0.02315484  0.01425406 -0.22685064\n",
      "  0.01744302  0.23990871  0.14808987  0.18774755  0.04187528  0.10242688\n",
      "  0.03234136 -0.23398869  0.05126817  0.12739854  0.25512013 -0.06789166\n",
      " -0.1710808   0.07073198  0.10568348  0.12297857 -0.09672302  0.08249701\n",
      "  0.46895766  0.07868284  0.11903016 -0.2954091  -0.2581112  -0.0291992\n",
      "  0.22022675  0.12231232 -0.01536406  0.02406432  0.16212063  0.16825823\n",
      " -0.14510117  0.06489129  0.07976659  0.00760283 -0.00484936  0.07547763\n",
      " -0.3062732  -0.22232227  0.3698871  -0.20583993 -0.21113673 -0.29774666\n",
      " -0.20168163 -0.00907221 -0.12127891 -0.17010957 -0.33246708 -0.03123682\n",
      "  0.13155742  0.00962715 -0.31546822  0.13884807  0.64512044 -0.4698922\n",
      "  0.33774972  0.13201351 -0.22038046 -0.02842722  0.12642878  0.03721472\n",
      "  0.09678718 -0.4270036  -0.006977   -0.01249063  0.10314102 -0.13061959\n",
      "  0.1288755  -0.11211325  0.18780155  0.01957331 -0.00074    -0.14163558\n",
      " -0.29537863  0.03098749 -0.00190358  0.05349007  0.2986415   0.04459301\n",
      " -0.5734857   0.30037126 -0.05162334  0.03307503 -0.45681068 -0.04811064\n",
      " -0.138102    0.04005794 -0.20231266  0.33633316 -0.09727602 -0.38221115\n",
      "  0.1843781  -0.05241916  0.2662538   0.13231842  0.09243678  0.00524466\n",
      " -0.38143307 -0.05794589 -0.11413753 -0.39460388 -0.01670512 -0.10297732\n",
      "  0.15131232 -0.06931885  0.04393776  0.21160166  0.16834307  0.05972411\n",
      " -0.09170473 -0.1928237   0.08548059 -0.07032555 -0.00144589 -0.00250571\n",
      "  0.18933643 -0.01930748  0.03854781  0.13656327 -0.02517071 -0.20613624\n",
      "  0.15143314  0.14127913 -0.05860851 -0.06534684 -0.28309682 -0.07204457\n",
      "  0.16509013 -0.23300686 -0.01879866 -0.4697322   0.15950686 -0.33405674\n",
      " -0.04340848 -0.01470859  0.0335313   0.00533894 -0.17427589 -0.4109314\n",
      " -0.07082281  0.00747374 -0.08537832  0.03622957 -0.19494516  0.07399632\n",
      " -0.04236685  0.11538745  0.14711946 -0.11219109  0.28077528 -0.45622662\n",
      "  0.15217091 -0.24234755 -0.07478079 -0.01335609  0.02890042 -0.29388854\n",
      " -0.21412097 -0.17064683  0.22929747  0.02752631  0.03621503 -0.07064023\n",
      " -0.266147   -0.53419966  0.11730833 -0.17935245  0.31134775 -0.070087\n",
      "  0.1546713   0.0698266  -0.22109006  0.01616178 -0.20188664 -0.15138246\n",
      "  0.3495345  -0.19519068  0.11640796 -0.07936823 -0.01844283  0.07368842\n",
      " -0.0415787   0.03707837 -0.18636557 -0.32165948 -0.23077828  0.28739586\n",
      " -0.3022457  -0.07914598  0.17788912  0.193808   -0.11657035 -0.34258443\n",
      "  0.28876248  0.22373047 -0.21241061  0.315986    0.22275263 -0.02232948\n",
      " -0.1434078  -0.3350812  -0.34561408  0.55636764  0.05509635 -0.01626279\n",
      "  0.01859835 -0.34049678 -0.12891248 -0.11244585 -0.22412953  0.11413933\n",
      "  0.04808833  0.17144634 -0.09195381  0.42793354 -0.05848071 -0.11156062\n",
      " -0.18658683 -0.02365145 -0.49148077 -0.01172688  0.06867656  0.09948702\n",
      " -0.03012284 -0.18061267 -0.13032365 -0.22072594  0.3042315  -0.06493676\n",
      " -0.30750793 -0.22138205  0.0042286  -0.04129407 -0.24017625 -0.14363752\n",
      "  0.04908009  0.023507   -0.09383243  0.08513407  0.18308946 -0.12098655\n",
      "  0.22010542 -0.10214893 -0.02760707 -0.33679736  0.03455307 -0.01337025\n",
      "  0.04493318  0.18532304  0.04139074 -0.21747623  0.08103625 -0.20411062\n",
      " -0.09601878 -0.01720667 -0.07771889  0.26647022  0.01497846  0.3300295\n",
      " -0.15956241 -0.00784488 -0.16420813  0.10226763  0.10926916  0.27419245\n",
      "  0.07444547  0.04324985  0.08672414 -0.30017903  0.12535575 -0.02618013]\n",
      "\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:08<00:00,  8.94s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 2/2 [00:22<00:00, 11.19s/it]\n"
     ]
    }
   ],
   "source": [
    "!python bert_generator.py \\\n",
    "--task_name sst-2\\\n",
    "--do_eval\\\n",
    "--do_lower_case\\\n",
    "--data_dir ./data/sst-2/add_1/\\\n",
    "--bert_model bert-base-uncased\\\n",
    "--max_seq_length 64\\\n",
    "--train_batch_size 8\\\n",
    "--learning_rate 2e-5\\\n",
    "--output_dir ./tmp/sst2-gnrt/\\\n",
    "--num_eval_epochs 2\\\n",
    "--no_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After recovering the test instances, we can run a model to check the recovering effectiveness. The model in our settings is a sentiment classification model based on bert contextualized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python bert_classifier.py \n",
    "--task_name sst-2 \n",
    "--do_eval  \n",
    "--do_lower_case   \n",
    "--data_dir data/SST-2/add_1/  \n",
    "--bert_model bert-base-uncased   \n",
    "--max_seq_length 64   \n",
    "--train_batch_size 8  \n",
    "--learning_rate 2e-5   \n",
    "--output_dir ./tmp/sst2-gnrt/ \n",
    "--num_eval_epochs 2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GT_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
