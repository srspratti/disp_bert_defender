{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"bert_defender_modified.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFjvtMt8wsST","executionInfo":{"status":"ok","timestamp":1618166601261,"user_tz":240,"elapsed":483,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"115392f0-7fa1-4af4-feb8-4744f5c19350"},"source":["!pwd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/UdeM/GT_ML/bert-defender-master/Bert_Defender_ts/disp_bert_defender\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBNAxf8JxchI","executionInfo":{"status":"ok","timestamp":1618246550581,"user_tz":240,"elapsed":33454,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"3e1d6fc1-0e05-4ffa-b772-0e9b50768954"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_zF5_GS64jk","executionInfo":{"status":"ok","timestamp":1618247873449,"user_tz":240,"elapsed":1082,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"1800919a-dc8c-4a9f-db4f-28328171346d"},"source":["cd /content/drive/MyDrive/UdeM/GT_ML/bert-defender-master/Bert_Defender_ts/disp_bert_defender/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/UdeM/GT_ML/bert-defender-master/Bert_Defender_ts/disp_bert_defender\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haOwVP_qyA1D","executionInfo":{"status":"ok","timestamp":1618247875251,"user_tz":240,"elapsed":393,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"147f1882-a35f-4c17-e505-ded8621e9138"},"source":["ls"],"execution_count":12,"outputs":[{"output_type":"stream","text":["bert_classifier.py            \u001b[0m\u001b[01;34mdata\u001b[0m/                    generator.model\n","bert_config.json              Discriminator.model      \u001b[01;34mmodels\u001b[0m/\n","bert_defender_modified.ipynb  \u001b[01;34memb\u001b[0m/                     optimization.py\n","bert_discriminator.py         enumerate_attacks.py     \u001b[01;34m__pycache__\u001b[0m/\n","bert_eval_epoches.py          file_utils.py            README.md\n","bert_eval.py                  \u001b[01;34mGAN2vec\u001b[0m/                 \u001b[01;34mRobGAN\u001b[0m/\n","bert_generator.py             GAN2vec_gen_dis.py       sample\n","bert_model.py                 GAN2vec_RobGAN_train.py  \u001b[01;34mtmp\u001b[0m/\n","bert_random_attacks.py        \u001b[01;34mGan2vec_RobGAN_utils\u001b[0m/    tokenization.py\n","bert_utils.py                 GAN2vec_train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhj4N3zezAIk","outputId":"2ab9ace5-3c89-4bbe-d7f0-08fb3f9d866d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Mar 25 13:46:12 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  GeForce RTX 3090    Off  | 00000000:46:00.0 Off |                  N/A |\n","| 30%   37C    P8     9W / 350W |      6MiB / 24268MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  GeForce RTX 3090    Off  | 00000000:C2:00.0 Off |                  N/A |\n","|  0%   32C    P8    12W / 350W |      6MiB / 24268MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urDRDo6764jm","executionInfo":{"status":"ok","timestamp":1618247744315,"user_tz":240,"elapsed":237724,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"5f4227d5-91e3-452d-e7f4-175b42c713c6"},"source":["#!pip install torch\n","!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.1+cu111\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n","\u001b[K     |█████████████▌                  | 834.1MB 1.3MB/s eta 0:14:59tcmalloc: large alloc 1147494400 bytes == 0x55be78a06000 @  0x7f0d78999615 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007fe50 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be40180431 0x55be400e1049 0x55be4004bc84 0x55be4000c8e9 0x55be40080ade 0x55be4000d69a 0x55be4007ba45 0x55be4007ae0d 0x55be4000d77a 0x55be4007ba45 0x55be4000d69a 0x55be4007ba45\n","\u001b[K     |█████████████████               | 1055.7MB 1.2MB/s eta 0:13:14tcmalloc: large alloc 1434370048 bytes == 0x55bebd05c000 @  0x7f0d78999615 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007fe50 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be40180431 0x55be400e1049 0x55be4004bc84 0x55be4000c8e9 0x55be40080ade 0x55be4000d69a 0x55be4007ba45 0x55be4007ae0d 0x55be4000d77a 0x55be4007ba45 0x55be4000d69a 0x55be4007ba45\n","\u001b[K     |█████████████████████▋          | 1336.2MB 1.2MB/s eta 0:08:57tcmalloc: large alloc 1792966656 bytes == 0x55be41e8e000 @  0x7f0d78999615 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007fe50 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be40180431 0x55be400e1049 0x55be4004bc84 0x55be4000c8e9 0x55be40080ade 0x55be4000d69a 0x55be4007ba45 0x55be4007ae0d 0x55be4000d77a 0x55be4007ba45 0x55be4000d69a 0x55be4007ba45\n","\u001b[K     |███████████████████████████▎    | 1691.1MB 1.2MB/s eta 0:03:58tcmalloc: large alloc 2241208320 bytes == 0x55beacc76000 @  0x7f0d78999615 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007fe50 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be4007bee2 0x55be400fe7c6 0x55be40180431 0x55be400e1049 0x55be4004bc84 0x55be4000c8e9 0x55be40080ade 0x55be4000d69a 0x55be4007ba45 0x55be4007ae0d 0x55be4000d77a 0x55be4007ba45 0x55be4000d69a 0x55be4007ba45\n","\u001b[K     |████████████████████████████████| 1982.2MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0x55bf325d8000 @  0x7f0d789981e7 0x55be4003f017 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4000d69a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be4007ab0e\n","tcmalloc: large alloc 2477727744 bytes == 0x55bfa8832000 @  0x7f0d78999615 0x55be4000906c 0x55be400e8eba 0x55be4000be8d 0x55be400fd99d 0x55be4007ffe9 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007bc9e 0x55be4000d69a 0x55be4007bc9e 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be4007ab0e 0x55be4000d77a 0x55be4007c86a 0x55be4007ab0e 0x55be4000de11\n","\u001b[K     |████████████████████████████████| 1982.2MB 6.6kB/s \n","\u001b[?25hCollecting torchvision==0.9.1+cu111\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6MB)\n","\u001b[K     |████████████████████████████████| 17.6MB 309kB/s \n","\u001b[?25hCollecting torchaudio==0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-uziBBezoQ0","outputId":"a1a262f6-56c9-4236-a4d3-5b925a9d0669"},"source":["import torch\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9LVvTU180CDC","outputId":"49a9d74c-66e9-4aef-d8e9-0452f69e0ce1"},"source":["import tensorflow as tf\n","tf.test.gpu_device_name()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7L8NN5P16fx","executionInfo":{"status":"ok","timestamp":1617909791642,"user_tz":240,"elapsed":460,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"782dcea2-42f1-4090-efd2-5947c14fc7e1"},"source":["!which python"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/bin/python\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWou01Hj-c1Z","executionInfo":{"status":"ok","timestamp":1618247750353,"user_tz":240,"elapsed":230837,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"70a00fb1-ee83-4495-8d6c-87d1d1dedd07"},"source":["!pip install boto3"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/79/64c0815cbe8c6abd7fe5525ec37a2689d3cf10e387629ba4a6e44daff6d0/boto3-1.17.49-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 7.4MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n","\u001b[K     |████████████████████████████████| 81kB 6.0MB/s \n","\u001b[?25hCollecting botocore<1.21.0,>=1.20.49\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/6e28ce58206039ad2592992b75ee79a8f9dbc902a9704373ddacc4f96300/botocore-1.20.49-py2.py3-none-any.whl (7.4MB)\n","\u001b[K     |████████████████████████████████| 7.4MB 10.3MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.49->boto3) (2.8.1)\n","Collecting urllib3<1.27,>=1.25.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n","\u001b[K     |████████████████████████████████| 153kB 52.6MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.49->boto3) (1.15.0)\n","\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3\n","  Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.17.49 botocore-1.20.49 jmespath-0.10.0 s3transfer-0.3.6 urllib3-1.26.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MHA27HN-tKP","executionInfo":{"status":"ok","timestamp":1618247775497,"user_tz":240,"elapsed":255111,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"caa22769-dbb0-4f53-cc9f-75a97c1e10a6"},"source":["!pip install hnswlib==0.5.1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting hnswlib==0.5.1\n","  Downloading https://files.pythonhosted.org/packages/03/8c/3e0e608278b740f2a78ba76ba406dbecc8b7d3ce8cfb858580f13ea04930/hnswlib-0.5.1.tar.gz\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hnswlib==0.5.1) (1.19.5)\n","Building wheels for collected packages: hnswlib\n","  Building wheel for hnswlib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hnswlib: filename=hnswlib-0.5.1-cp37-cp37m-linux_x86_64.whl size=1296773 sha256=dec585e735c08dc7e3b24a4da52febae116ddc0b92cb62efc5124bbe87f2b458\n","  Stored in directory: /root/.cache/pip/wheels/46/7b/98/44c3a8a284506a54993f0b321e4a32a0c9e69215bbb72feff5\n","Successfully built hnswlib\n","Installing collected packages: hnswlib\n","Successfully installed hnswlib-0.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qV064Cit_kxn","executionInfo":{"status":"ok","timestamp":1618247780846,"user_tz":240,"elapsed":259603,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"e399fb08-cfda-4cb9-bc52-d451e4bae11e"},"source":["!pip install nltk\n","!pip install pytest\n","import nltk\n","nltk.download('punkt')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (54.2.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.7.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (20.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1s_07cjv64jo","executionInfo":{"status":"ok","timestamp":1618247783059,"user_tz":240,"elapsed":260924,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"5ada5304-b46e-4bb9-db71-75a8d517b24e"},"source":["!pip install sklearn"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJ2hZwQq64jo","executionInfo":{"status":"ok","timestamp":1618247792482,"user_tz":240,"elapsed":269631,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"8a2fd805-3b10-4649-8ead-6488891dbce0"},"source":["!pip install pandas\n","!pip install gensim\n","!pip install python-Levenshtein\n","from gensim.models import Word2Vec"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Collecting python-Levenshtein\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (54.2.0)\n","Building wheels for collected packages: python-Levenshtein\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149818 sha256=0581fb8a274411d3a416d523f65511f3244ec8122dbbf5804bc2f16296910afe\n","  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n","Successfully built python-Levenshtein\n","Installing collected packages: python-Levenshtein\n","Successfully installed python-Levenshtein-0.12.2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"id":"AjwXewgN64jo","outputId":"6e16d987-45bb-4e41-c9d6-b4d680a872b4"},"source":["!nvidia-smi -q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","==============NVSMI LOG==============\n","\n","Timestamp                                 : Mon Mar 22 19:25:35 2021\n","Driver Version                            : 460.39\n","CUDA Version                              : 11.2\n","\n","Attached GPUs                             : 2\n","GPU 00000000:46:00.0\n","    Product Name                          : GeForce RTX 3090\n","    Product Brand                         : GeForce\n","    Display Mode                          : Disabled\n","    Display Active                        : Disabled\n","    Persistence Mode                      : Enabled\n","    MIG Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    Accounting Mode                       : Disabled\n","    Accounting Mode Buffer Size           : 4000\n","    Driver Model\n","        Current                           : N/A\n","        Pending                           : N/A\n","    Serial Number                         : 1324020038945\n","    GPU UUID                              : GPU-8ad6666a-677c-9f0e-3b58-90220b309ddc\n","    Minor Number                          : 3\n","    VBIOS Version                         : 94.02.27.00.0A\n","    MultiGPU Board                        : No\n","    Board ID                              : 0x4600\n","    GPU Part Number                       : 900-1G136-2510-000\n","    Inforom Version\n","        Image Version                     : G001.0000.03.03\n","        OEM Object                        : 2.0\n","        ECC Object                        : N/A\n","        Power Management Object           : N/A\n","    GPU Operation Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    GPU Virtualization Mode\n","        Virtualization Mode               : None\n","        Host VGPU Mode                    : N/A\n","    IBMNPU\n","        Relaxed Ordering Mode             : N/A\n","    PCI\n","        Bus                               : 0x46\n","        Device                            : 0x00\n","        Domain                            : 0x0000\n","        Device Id                         : 0x220410DE\n","        Bus Id                            : 00000000:46:00.0\n","        Sub System Id                     : 0x147D10DE\n","        GPU Link Info\n","            PCIe Generation\n","                Max                       : 3\n","                Current                   : 1\n","            Link Width\n","                Max                       : 16x\n","                Current                   : 16x\n","        Bridge Chip\n","            Type                          : N/A\n","            Firmware                      : N/A\n","        Replays Since Reset               : 0\n","        Replay Number Rollovers           : 0\n","        Tx Throughput                     : 0 KB/s\n","        Rx Throughput                     : 0 KB/s\n","    Fan Speed                             : 0 %\n","    Performance State                     : P8\n","    Clocks Throttle Reasons\n","        Idle                              : Active\n","        Applications Clocks Setting       : Not Active\n","        SW Power Cap                      : Not Active\n","        HW Slowdown                       : Not Active\n","            HW Thermal Slowdown           : Not Active\n","            HW Power Brake Slowdown       : Not Active\n","        Sync Boost                        : Not Active\n","        SW Thermal Slowdown               : Not Active\n","        Display Clock Setting             : Not Active\n","    FB Memory Usage\n","        Total                             : 24268 MiB\n","        Used                              : 414 MiB\n","        Free                              : 23854 MiB\n","    BAR1 Memory Usage\n","        Total                             : 256 MiB\n","        Used                              : 11 MiB\n","        Free                              : 245 MiB\n","    Compute Mode                          : Default\n","    Utilization\n","        Gpu                               : 0 %\n","        Memory                            : 0 %\n","        Encoder                           : 0 %\n","        Decoder                           : 0 %\n","    Encoder Stats\n","        Active Sessions                   : 0\n","        Average FPS                       : 0\n","        Average Latency                   : 0\n","    FBC Stats\n","        Active Sessions                   : 0\n","        Average FPS                       : 0\n","        Average Latency                   : 0\n","    Ecc Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    ECC Errors\n","        Volatile\n","            SRAM Correctable              : N/A\n","            SRAM Uncorrectable            : N/A\n","            DRAM Correctable              : N/A\n","            DRAM Uncorrectable            : N/A\n","        Aggregate\n","            SRAM Correctable              : N/A\n","            SRAM Uncorrectable            : N/A\n","            DRAM Correctable              : N/A\n","            DRAM Uncorrectable            : N/A\n","    Retired Pages\n","        Single Bit ECC                    : N/A\n","        Double Bit ECC                    : N/A\n","        Pending Page Blacklist            : N/A\n","    Remapped Rows                         : N/A\n","    Temperature\n","        GPU Current Temp                  : 22 C\n","        GPU Shutdown Temp                 : 98 C\n","        GPU Slowdown Temp                 : 95 C\n","        GPU Max Operating Temp            : 93 C\n","        GPU Target Temperature            : 75 C\n","        Memory Current Temp               : N/A\n","        Memory Max Operating Temp         : N/A\n","    Power Readings\n","        Power Management                  : Supported\n","        Power Draw                        : 16.14 W\n","        Power Limit                       : 375.00 W\n","        Default Power Limit               : 350.00 W\n","        Enforced Power Limit              : 375.00 W\n","        Min Power Limit                   : 100.00 W\n","        Max Power Limit                   : 400.00 W\n","    Clocks\n","        Graphics                          : 0 MHz\n","        SM                                : 0 MHz\n","        Memory                            : 405 MHz\n","        Video                             : 555 MHz\n","    Applications Clocks\n","        Graphics                          : N/A\n","        Memory                            : N/A\n","    Default Applications Clocks\n","        Graphics                          : N/A\n","        Memory                            : N/A\n","    Max Clocks\n","        Graphics                          : 2100 MHz\n","        SM                                : 2100 MHz\n","        Memory                            : 9751 MHz\n","        Video                             : 1950 MHz\n","    Max Customer Boost Clocks\n","        Graphics                          : N/A\n","    Clock Policy\n","        Auto Boost                        : N/A\n","        Auto Boost Default                : N/A\n","    Processes                             : None\n","\n","GPU 00000000:81:00.0\n","    Product Name                          : GeForce RTX 3090\n","    Product Brand                         : GeForce\n","    Display Mode                          : Disabled\n","    Display Active                        : Disabled\n","    Persistence Mode                      : Enabled\n","    MIG Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    Accounting Mode                       : Disabled\n","    Accounting Mode Buffer Size           : 4000\n","    Driver Model\n","        Current                           : N/A\n","        Pending                           : N/A\n","    Serial Number                         : 1323920020675\n","    GPU UUID                              : GPU-d45bf8ec-5a88-7fa3-7baa-c0196ebd87c8\n","    Minor Number                          : 2\n","    VBIOS Version                         : 94.02.27.00.0A\n","    MultiGPU Board                        : No\n","    Board ID                              : 0x8100\n","    GPU Part Number                       : 900-1G136-2510-000\n","    Inforom Version\n","        Image Version                     : G001.0000.03.03\n","        OEM Object                        : 2.0\n","        ECC Object                        : N/A\n","        Power Management Object           : N/A\n","    GPU Operation Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    GPU Virtualization Mode\n","        Virtualization Mode               : None\n","        Host VGPU Mode                    : N/A\n","    IBMNPU\n","        Relaxed Ordering Mode             : N/A\n","    PCI\n","        Bus                               : 0x81\n","        Device                            : 0x00\n","        Domain                            : 0x0000\n","        Device Id                         : 0x220410DE\n","        Bus Id                            : 00000000:81:00.0\n","        Sub System Id                     : 0x147D10DE\n","        GPU Link Info\n","            PCIe Generation\n","                Max                       : 3\n","                Current                   : 1\n","            Link Width\n","                Max                       : 16x\n","                Current                   : 16x\n","        Bridge Chip\n","            Type                          : N/A\n","            Firmware                      : N/A\n","        Replays Since Reset               : 0\n","        Replay Number Rollovers           : 0\n","        Tx Throughput                     : 0 KB/s\n","        Rx Throughput                     : 0 KB/s\n","    Fan Speed                             : 0 %\n","    Performance State                     : P8\n","    Clocks Throttle Reasons\n","        Idle                              : Active\n","        Applications Clocks Setting       : Not Active\n","        SW Power Cap                      : Not Active\n","        HW Slowdown                       : Not Active\n","            HW Thermal Slowdown           : Not Active\n","            HW Power Brake Slowdown       : Not Active\n","        Sync Boost                        : Not Active\n","        SW Thermal Slowdown               : Not Active\n","        Display Clock Setting             : Not Active\n","    FB Memory Usage\n","        Total                             : 24268 MiB\n","        Used                              : 414 MiB\n","        Free                              : 23854 MiB\n","    BAR1 Memory Usage\n","        Total                             : 256 MiB\n","        Used                              : 7 MiB\n","        Free                              : 249 MiB\n","    Compute Mode                          : Default\n","    Utilization\n","        Gpu                               : 0 %\n","        Memory                            : 0 %\n","        Encoder                           : 0 %\n","        Decoder                           : 0 %\n","    Encoder Stats\n","        Active Sessions                   : 0\n","        Average FPS                       : 0\n","        Average Latency                   : 0\n","    FBC Stats\n","        Active Sessions                   : 0\n","        Average FPS                       : 0\n","        Average Latency                   : 0\n","    Ecc Mode\n","        Current                           : N/A\n","        Pending                           : N/A\n","    ECC Errors\n","        Volatile\n","            SRAM Correctable              : N/A\n","            SRAM Uncorrectable            : N/A\n","            DRAM Correctable              : N/A\n","            DRAM Uncorrectable            : N/A\n","        Aggregate\n","            SRAM Correctable              : N/A\n","            SRAM Uncorrectable            : N/A\n","            DRAM Correctable              : N/A\n","            DRAM Uncorrectable            : N/A\n","    Retired Pages\n","        Single Bit ECC                    : N/A\n","        Double Bit ECC                    : N/A\n","        Pending Page Blacklist            : N/A\n","    Remapped Rows                         : N/A\n","    Temperature\n","        GPU Current Temp                  : 24 C\n","        GPU Shutdown Temp                 : 98 C\n","        GPU Slowdown Temp                 : 95 C\n","        GPU Max Operating Temp            : 93 C\n","        GPU Target Temperature            : 75 C\n","        Memory Current Temp               : N/A\n","        Memory Max Operating Temp         : N/A\n","    Power Readings\n","        Power Management                  : Supported\n","        Power Draw                        : 13.63 W\n","        Power Limit                       : 375.00 W\n","        Default Power Limit               : 350.00 W\n","        Enforced Power Limit              : 375.00 W\n","        Min Power Limit                   : 100.00 W\n","        Max Power Limit                   : 400.00 W\n","    Clocks\n","        Graphics                          : 0 MHz\n","        SM                                : 0 MHz\n","        Memory                            : 405 MHz\n","        Video                             : 555 MHz\n","    Applications Clocks\n","        Graphics                          : N/A\n","        Memory                            : N/A\n","    Default Applications Clocks\n","        Graphics                          : N/A\n","        Memory                            : N/A\n","    Max Clocks\n","        Graphics                          : 2100 MHz\n","        SM                                : 2100 MHz\n","        Memory                            : 9751 MHz\n","        Video                             : 1950 MHz\n","    Max Customer Boost Clocks\n","        Graphics                          : N/A\n","    Clock Policy\n","        Auto Boost                        : N/A\n","        Auto Boost Default                : N/A\n","    Processes                             : None\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tVMGJJMF64jo"},"source":["Training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FIO5U77A1QcJ","jupyter":{"outputs_hidden":true},"tags":[],"outputId":"3653c367-8f1e-49ac-cbb9-bcbdd2ef4d30"},"source":["# Discriminator train \n","!python bert_discriminator.py \\\n","--task_name sst-2\\\n","--do_train\\\n","--do_lower_case\\\n","--data_dir data/sst-2/\\\n","--bert_model bert-base-uncased\\\n","--max_seq_length 128\\\n","--train_batch_size 8\\\n","--learning_rate 2e-5\\\n","--num_train_epochs 25\\\n","--output_dir ./tmp/disc/\n","#--no_cuda"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","03/28/2021 18:10:33 - INFO - bert_utils -   device: cpu , distributed training: False, 16-bits training: False\n","03/28/2021 18:10:34 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","03/28/2021 18:10:34 - INFO - bert_utils -   *** Example ***\n","03/28/2021 18:10:34 - INFO - bert_utils -   tokens: that loves its characters and communicates something rather beautiful about human nature\n","03/28/2021 18:10:34 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 18:10:34 - INFO - bert_utils -   *** Example ***\n","03/28/2021 18:10:34 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the same throughout\n","03/28/2021 18:10:34 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 18:10:34 - INFO - bert_utils -   ***** Running training *****\n","03/28/2021 18:10:34 - INFO - bert_utils -     Num examples = 5\n","03/28/2021 18:10:34 - INFO - bert_utils -     Num token vocab = 59\n","03/28/2021 18:10:34 - INFO - bert_utils -     Batch size = 8\n","03/28/2021 18:10:34 - INFO - bert_utils -     Num steps = 0\n","03/28/2021 18:10:34 - INFO - bert_utils -   Loading word embeddings ... \n","03/28/2021 18:10:45 - INFO - bert_utils -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/28/2021 18:10:45 - INFO - bert_utils -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp8sz0b55q\n","03/28/2021 18:10:48 - INFO - bert_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/28/2021 18:10:51 - INFO - bert_utils -   Weights of BertForDiscriminator not initialized from pretrained model: ['discriminator.weight', 'discriminator.bias']\n","03/28/2021 18:10:51 - INFO - bert_utils -   Weights from pretrained model not used in BertForDiscriminator: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0437, -0.0144, -0.1032, -0.0202, -0.0252, 0.019, 0.0131, -0.0011, -0.0168, -0.0128, -0.0008, 0.0121, -0.0426, 0.0192, 0.0158, -0.1123, 0.0189, 0.0492, 0.1644, -0.0394, -0.0248, 0.0215, -0.0129, -0.0496, 0.005, 0.0421, -0.0329, -0.0741, -0.1048, -0.0552, 0.0227, -0.0345, -0.0368, -0.0959, -0.0059, 0.0261, -0.0136, 0.0411, -0.0187, 0.0081, -0.0292, -0.0747, -0.0101, -0.0148, 0.0238, 0.0097, -0.0126, 0.0149, -0.0238, -0.0384, 0.0375, 0.071, -0.5428, -0.0469, 0.0233, 0.0216, 0.0095, 0.0793, -0.043, 0.0157, 0.0183, -0.0651, -0.0323, -0.0266, 0.0534, -0.0686, 0.0119, -0.0291, -0.0892, -0.0047, 0.0317, 0.0145, 0.024, -0.0603, -0.0286, 0.0417, -0.0266, -0.0207, -0.0179, 0.0923, -0.0155, -0.0167, 0.0018, -0.167, -0.0138, -0.0397, 0.004, 0.069, 0.2466, 0.0105, -0.029, -0.0633, 0.0889, -0.0542, -0.0125, 0.0102, -0.0743, -0.0065, -0.0179, 0.0513, -0.0914, -0.0053, -0.0592, 0.0338, -0.1552, 0.002, -0.0752, 0.0243, -0.0747, -0.0947, -0.071, -0.1083, -0.0362, 0.0888, -0.007, -0.0735, 0.0336, -0.0083, 0.0342, -0.3091, -0.0721, -0.0342, -0.0242, -0.2222, -0.0489, 0.137, -0.0865, 0.0516, 0.03, -0.0035, 0.0154, -0.0025, -0.0195, -0.0189, -0.0473, -0.1336, -0.001, -0.0033, -0.0832, 0.0505, -0.0475, -0.0468, 0.0717, 0.1783, 0.0927, 0.0197, 0.0592, -0.0596, 0.0006, 0.0368, 0.0973, 0.0478, -0.021, -0.0317, -0.0101, 0.0674, 0.0452, 0.0379, -0.0198, -0.0358, -0.0723, -0.0105, -0.0479, 0.0109, -0.1331, -0.0287, 0.0535, -0.0281, -0.0316, -0.0065, -0.0265, 0.0235, 0.0284, 0.0521, 0.0836, -0.1519, 0.1407, -0.2958, 0.0723, -0.0243, 0.0768, -0.0225, 0.0749, 0.0447, 0.0244, 0.1271, -0.0649, 0.0196, 0.4245, 0.0026, 0.0269, 0.0098, -0.0427, 0.0266, 0.0082, -0.0294, -0.0156, -0.0197, 0.2449, -0.0039, 0.0266, -0.2229, -0.0209, -0.0469, 0.0384, 0.0065, 0.0036, 0.0006, -0.0314, 0.0044, 0.0778, 0.0198, -0.0378, -0.0136, 0.0212, -0.0012, 0.0102, 0.0594, 0.0661, -0.0734, 0.0559, 0.0097, 0.0514, 0.019, 0.025, -0.0928, 0.045, 0.005, -0.0685, -0.114, 0.1314, 0.0066, 0.2853, -0.0352, 0.0347, -0.365, -0.0628, 0.1728, -0.2572, 0.0756, -0.0777, 0.027, 0.0623, -0.0096, 0.0195, -0.0214, 0.0187, -0.0461, -0.0247, 0.3452, 0.0195, -0.0091, -0.0981, 0.0356, -0.0193, 0.0092, -0.0804, -0.0073, 0.0464, 0.0069, 0.0712, -0.0265, 0.0014, -0.0015, -0.3931, -0.0186, -0.0188, -0.009, -0.0198, -0.031, -0.051, -0.0472, 0.0292, 0.0231, -0.035, -0.005, 0.0204, 0.0888, -0.0003, 0.0259, 0.036, -0.2131, 0.0651, -0.0641, -0.0229, 0.0442, -0.0742, 0.0325, -0.0619, 0.0497, 0.0603, 0.0174, -0.0593, 0.0226, -0.0546, 0.013, 0.0049, 0.0448, -0.0326, -0.013]\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.052, -0.0294, 0.0662, -0.0196, 0.0562, -0.0476, -0.0404, -0.1003, -0.0313, 0.0058, -0.0193, 0.008, -0.0415, 0.1196, 0.0555, -0.0765, 0.0005, 0.0354, -0.1444, -0.0218, 0.077, -0.1535, 0.0877, 0.162, 0.0346, 0.0851, 0.001, 0.0243, 0.0587, -0.04, -0.0109, -0.029, 0.0163, 0.0215, 0.0047, -0.1097, 0.0595, -0.1733, -0.0144, -0.0021, 0.1223, -0.0542, 0.0603, -0.0935, 0.0581, -0.1618, -0.0367, 0.0942, 0.0466, -0.034, -0.104, -0.0293, -0.5887, 0.0164, -0.0315, -0.0253, -0.039, 0.0221, -0.1189, 0.1224, 0.0414, 0.0568, 0.0248, -0.004, -0.1998, -0.1871, 0.0959, -0.1397, -0.136, -0.0227, -0.1149, 0.005, 0.0852, -0.0841, -0.1255, 0.0172, 0.0228, -0.0628, -0.0599, 0.1372, 0.0186, -0.0327, 0.0482, -0.2448, 0.0334, 0.0841, -0.0758, 0.0206, -0.4397, -0.025, -0.054, -0.0346, -0.0057, 0.0891, -0.0616, -0.0016, -0.0335, -0.0542, -0.0714, -0.0707, -0.2133, 0.0412, -0.0672, 0.0981, -0.3939, -0.085, -0.1221, -0.0232, 0.1097, -0.1054, -0.0446, -0.2737, 0.0793, -0.0647, 0.153, -0.1979, 0.0088, -0.0935, 0.0276, -0.3312, 0.0648, -0.0808, 0.054, -0.0313, -0.1577, 0.2038, 0.0053, -0.1417, 0.1395, 0.0545, 0.2468, -0.1229, -0.1038, -0.0565, -0.027, 0.2695, 0.104, 0.2153, -0.1012, -0.0243, -0.028, -0.0283, -0.1783, 0.3281, -0.0311, -0.1017, 0.1204, 0.0994, 0.0132, 0.1076, 0.1126, -0.1037, -0.0332, -0.0535, 0.0519, 0.0318, -0.0137, 0.0347, 0.0317, -0.0868, -0.0686, 0.0359, -0.0939, 0.2033, 0.0513, 0.0057, 0.0493, -0.0229, -0.0925, -0.0906, -0.1301, 0.0273, 0.1754, -0.1269, 0.189, -0.2194, 0.2082, 0.1058, -0.1408, -0.0965, 0.0585, 0.0207, 0.0668, -0.0127, -0.0278, 0.074, 0.0081, -0.0431, 0.8819, -0.0017, 0.0222, -0.0824, -0.1692, 0.0367, -0.0723, -0.014, -0.0677, 0.0994, 0.1914, -0.1304, -0.0475, 0.035, 0.0831, -0.0874, 0.0876, -0.0784, 0.006, -0.0642, 0.0893, 0.0028, 0.1467, -0.1508, 0.0415, 0.0912, -0.0696, -0.0012, 0.0188, 0.1286, -0.0516, 0.096, -0.1434, 0.0708, -0.0041, 0.0725, -0.0137, -0.0933, 0.1138, 0.0651, -0.0779, -0.238, 0.1244, 0.1053, 0.2864, 0.1014, 0.1074, 0.0836, 0.1157, 0.106, -0.2746, -0.0583, 0.0355, -0.0039, 0.0105, 0.116, 0.0939, 0.1633, -0.0605, 0.0444, 0.1468, 0.3065, -0.0717, 0.1178, -0.0014, 0.0606, -0.2838, -0.054, -0.0281, 0.0148, -0.0554, 0.0352, 0.0071, -0.047, -0.0562, -0.0249, -0.1897, 0.0359, 0.0688, -0.0628, -0.0416, 0.0313, -0.0206, 0.1185, -0.0501, 0.0159, -0.0199, -0.0524, 0.1503, -0.0864, -0.1211, -0.02, 0.0191, -0.1345, -0.0777, 0.1295, 0.0247, 0.018, 0.0964, 0.1789, -0.062, -0.0767, -0.0568, 0.0522, -0.0159, -0.0909, -0.1266, 0.0103, -0.1333, 0.153, 0.0375, 0.2267]\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","/root/bert_defender_master_vastai_new_instance/optimization.py:132: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.36s/it]\u001b[A\n","03/28/2021 18:10:55 - INFO - bert_utils -     flaw_f1 = 0.05309734513274336\n","03/28/2021 18:10:55 - INFO - bert_utils -     flaw_precision = 0.027573529411764705\n","03/28/2021 18:10:55 - INFO - bert_utils -     flaw_recall = 0.7142857142857143\n","03/28/2021 18:10:55 - INFO - bert_utils -     loss = 0.8163223266601562\n","Epoch:   4%|█▍                                   | 1/25 [00:05<02:00,  5.04s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.30s/it]\u001b[A\n","03/28/2021 18:11:00 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:00 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:00 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:00 - INFO - bert_utils -     loss = 0.44367465376853943\n","Epoch:   8%|██▉                                  | 2/25 [00:10<01:54,  4.99s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0369, -0.0747, -0.0469, 0.0361, -0.0919, -0.0009, -0.0225, -0.0086, 0.0385, 0.0186, 0.0531, -0.0053, 0.0438, 0.0318, 0.0474, -0.0007, 0.041, 0.0475, -0.1702, -0.0859, 0.0242, 0.0221, 0.0487, -0.0435, -0.064, -0.3495, 0.0443, -0.0841, -0.0702, -0.1877, 0.0284, 0.064, -0.1133, -0.0202, 0.002, 0.0423, -0.0616, -0.1389, -0.0825, 0.0436, 0.0003, -0.0068, -0.1889, 0.0691, 0.2175, -0.057, -0.0354, -0.0194, -0.0878, 0.0954, -0.036, 0.193, -0.6756, -0.0179, -0.1067, -0.0159, -0.0191, -0.0452, 0.1099, -0.0407, -0.0379, 0.0292, -0.0908, -0.0199, -0.0426, 0.0648, -0.018, 0.0435, -0.01, -0.0118, 0.0286, 0.0318, -0.0296, 0.0815, 0.0263, 0.0283, -0.0327, -0.1023, -0.0261, -0.0393, 0.0171, -0.0652, 0.0306, -0.2258, 0.0422, -0.0264, -0.0298, 0.0358, 0.0322, -0.0455, -0.0502, 0.0434, 0.0162, -0.0809, -0.12, -0.0164, -0.0162, -0.0172, -0.0128, -0.0066, -0.1001, -0.011, 0.0629, 0.0164, -0.1474, 0.0805, 0.2876, 0.0508, 0.1542, 0.0635, 0.0978, 0.1995, -0.0425, 0.009, 0.0627, 0.0527, -0.0341, 0.1002, 0.0001, -0.3458, -0.0683, 0.0053, 0.0336, 0.0839, 0.165, 0.1572, 0.0059, 0.1544, -0.1236, 0.0389, 0.0085, -0.0136, -0.1793, 0.0274, -0.0333, -0.2917, 0.0114, -0.1436, 0.1732, -0.0428, -0.0056, 0.0282, 0.0279, 0.2743, -0.0107, -0.0348, 0.0237, 0.136, -0.0467, 0.0389, 0.0732, -0.0709, 0.0007, -0.0509, 0.0266, 0.0308, -0.0009, -0.0623, -0.0245, 0.0403, -0.0138, -0.0155, 0.0327, -0.0914, -0.2653, 0.0259, 0.0943, 0.1989, 0.0456, -0.0408, 0.1517, 0.0092, -0.0148, 0.1394, 0.1615, -0.1643, 0.1515, -0.0187, 0.0064, -0.0194, 0.0148, 0.0217, 0.0553, 0.0124, 0.1067, -0.3425, -0.0749, -0.1354, 0.0049, 0.0316, 0.0313, -0.015, 0.0393, -0.0033, 0.0031, 0.0138, -0.0337, -0.0591, 0.1925, -0.0098, -0.0113, 0.0, -0.0543, 0.0013, 0.0086, 0.0152, -0.0175, -0.0826, -0.0609, 0.0307, 0.0502, -0.0115, -0.0115, -0.0416, 0.0216, -0.0092, -0.2547, 0.0468, -0.0902, 0.0619, 0.0911, 0.0074, 0.0039, 0.088, -0.0067, -0.1804, 0.0322, -0.0157, -0.0377, 0.1205, -0.1194, -0.0555, 0.2795, -0.0318, -0.0034, 0.1598, -0.0423, -0.0498, -0.2052, -0.0395, -0.0274, -0.0264, -0.024, -0.0693, 0.0042, 0.1378, 0.0812, 0.052, -0.0359, 0.2623, 0.0796, 0.2137, 0.0467, 0.0616, -0.0922, 0.0232, 0.0758, 0.033, 0.0098, 0.06, -0.0736, 0.0035, 0.0342, 0.0351, -0.049, -0.0351, 0.2016, -0.0154, 0.0575, 0.0059, 0.0248, 0.0083, -0.064, -0.0317, -0.0728, -0.004, -0.1209, 0.0036, -0.0315, -0.0304, -0.1957, 0.0333, 0.0718, -0.1432, -0.0835, -0.0423, 0.0635, 0.0478, -0.0796, 0.0728, 0.0468, -0.0089, 0.0349, 0.0238, 0.0034, -0.0854, 0.0075, 0.2231, 0.0352, 0.0672]\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.018, 0.0137, 0.0563, 0.0203, -0.0224, -0.0579, 0.0017, -0.0083, -0.0074, 0.0621, 0.0457, -0.0904, 0.0844, 0.013, -0.1144, 0.0202, -0.0783, -0.0346, -0.0351, 0.0287, -0.0507, -0.0797, 0.0259, 0.0543, 0.0421, -0.1147, -0.0449, -0.0618, -0.0264, 0.0732, 0.0393, -0.0798, -0.0383, -0.0212, 0.0242, -0.0619, 0.0028, -0.0545, -0.1246, 0.0882, -0.0696, 0.0284, -0.1216, 0.0243, 0.0088, 0.0228, -0.0419, 0.0312, 0.0095, -0.0116, -0.0563, -0.0754, -0.6494, -0.0508, 0.0136, -0.0307, -0.0149, -0.052, 0.0104, 0.0411, 0.0321, -0.0849, 0.0666, -0.0499, -0.0406, -0.0305, 0.0453, 0.0657, -0.0713, 0.0026, -0.0071, -0.0073, -0.012, -0.0021, 0.0152, 0.0728, -0.0177, 0.0067, 0.0472, 0.2579, 0.0086, 0.0526, -0.0531, -0.2221, -0.005, -0.0434, 0.0486, 0.0129, -0.0179, 0.0281, -0.0121, 0.0198, 0.0175, -0.026, -0.0251, -0.0165, 0.0305, 0.0568, 0.0422, -0.0578, -0.1283, -0.0809, -0.0824, 0.0095, 0.0213, 0.0042, 0.1039, 0.2486, 0.0316, 0.0603, 0.0895, 0.0652, 0.0716, 0.0304, -0.2216, -0.0121, 0.0162, -0.0474, -0.0523, -0.3365, -0.0059, -0.0389, -0.0493, 0.054, 0.0473, 0.1572, -0.0403, 0.2288, -0.1701, 0.0341, 0.1491, -0.042, 0.0274, -0.0687, 0.0232, -0.2326, -0.0087, -0.0353, 0.0201, -0.0392, -0.0263, -0.0005, 0.0106, 0.3222, 0.1188, -0.0025, 0.1381, -0.0301, -0.1146, 0.0012, -0.0458, 0.0793, 0.0299, -0.0276, 0.0263, -0.0017, -0.0616, -0.0363, 0.0757, -0.0887, -0.0505, 0.0739, 0.0222, 0.0485, -0.1574, 0.0128, 0.115, -0.0137, 0.0246, 0.0834, 0.062, 0.0275, -0.0376, -0.0543, 0.242, 0.0166, 0.1765, -0.1171, 0.0097, 0.0407, 0.0245, 0.0482, 0.0543, 0.0164, 0.0306, 0.0662, -0.0758, -0.0842, -0.1406, 0.0708, -0.0021, -0.032, 0.0435, 0.0469, -0.0045, 0.0869, 0.0144, 0.0109, 0.1798, -0.0279, 0.0251, -0.0165, -0.0496, -0.0203, 0.0504, 0.0375, 0.055, -0.0736, -0.1372, 0.0084, 0.0426, -0.001, -0.0611, -0.003, 0.0637, -0.0185, 0.0292, 0.022, -0.0052, 0.0605, 0.1659, -0.0197, -0.073, 0.0347, 0.0514, -0.0462, -0.033, 0.1258, -0.0531, -0.1589, -0.0904, -0.0152, 0.3261, -0.1132, -0.0136, 0.0702, -0.02, 0.0705, -0.1598, 0.0617, -0.0651, 0.0309, 0.0323, -0.0752, -0.0333, 0.0489, -0.0132, 0.0433, 0.0011, 0.3065, 0.0577, 0.1041, 0.0583, -0.0108, -0.1463, -0.0195, -0.0661, 0.0006, -0.0628, 0.0766, -0.1766, -0.1066, -0.0112, -0.0637, -0.3468, -0.0268, -0.0201, -0.0342, 0.05, 0.047, -0.0543, 0.0142, -0.102, -0.0296, -0.0426, 0.0159, 0.0272, -0.065, -0.0514, -0.0284, 0.0657, -0.0163, 0.0494, -0.0867, 0.022, 0.0482, 0.0015, 0.0358, 0.0483, 0.0545, -0.0117, -0.0571, 0.014, -0.047, 0.0189, -0.0918, -0.0813, 0.2471, 0.0845, -0.0592]\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.46s/it]\u001b[A\n","03/28/2021 18:11:05 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:05 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:05 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:05 - INFO - bert_utils -     loss = 0.2200097143650055\n","Epoch:  12%|████▍                                | 3/25 [00:15<01:50,  5.04s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\u001b[A\n","03/28/2021 18:11:10 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:10 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:10 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:10 - INFO - bert_utils -     loss = 0.15955662727355957\n","Epoch:  16%|█████▉                               | 4/25 [00:20<01:45,  5.04s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.01, -0.1164, -0.1155, 0.0128, 0.0066, 0.1118, 0.0019, 0.1206, 0.0421, -0.101, -0.004, -0.0397, -0.0289, 0.0068, -0.0612, 0.0406, 0.0486, 0.0586, 0.1727, -0.0766, 0.0337, -0.0184, -0.0366, 0.2488, 0.0496, -0.0027, -0.0022, -0.0953, 0.0506, -0.1002, 0.0485, 0.0158, -0.0052, 0.0754, 0.1312, 0.0125, -0.0013, 0.1345, 0.1304, -0.0553, 0.0419, -0.0352, -0.0879, 0.011, 0.1502, 0.077, 0.0504, -0.0595, 0.0213, 0.0447, -0.0306, 0.1326, -0.6322, -0.0197, 0.0337, -0.0027, -0.1529, -0.1602, -0.0631, -0.0668, -0.0243, 0.0625, -0.1633, -0.0073, -0.0559, -0.1796, -0.0004, -0.0384, 0.0424, -0.0468, 0.0202, -0.0719, -0.0158, 0.0609, -0.0076, -0.0106, 0.2165, 0.0046, 0.0026, 0.1756, 0.018, 0.0555, 0.0287, -0.2628, 0.0122, 0.1043, 0.109, -0.0115, -0.2083, -0.0649, 0.0975, 0.007, -0.001, -0.124, 0.03, 0.0333, -0.0295, 0.0596, 0.0244, -0.0265, -0.1616, -0.0581, 0.0439, 0.0733, -0.0714, -0.0393, 0.2064, 0.1114, -0.0533, 0.119, -0.0603, 0.0178, -0.0118, 0.0112, 0.1174, -0.0652, 0.0505, -0.1118, 0.0196, -0.3306, 0.0246, -0.0156, -0.0363, 0.0558, -0.0872, 0.1793, -0.0551, 0.2096, -0.3445, 0.0282, -0.0852, 0.0196, -0.1081, -0.0563, 0.138, -0.1404, 0.0364, -0.0088, 0.1013, 0.0078, 0.0305, -0.0422, -0.0317, 0.3136, -0.01, 0.0061, 0.1233, -0.1092, 0.0066, -0.0221, 0.1111, 0.0233, 0.0763, 0.0268, -0.0634, 0.0247, -0.0976, 0.112, 0.0448, 0.0234, 0.2722, -0.0352, -0.0033, -0.039, -0.1779, -0.0273, 0.1093, 0.0029, -0.0524, 0.0917, -0.0883, 0.0905, 0.0899, 0.0649, -0.0216, 0.0238, 0.232, -0.1989, -0.0122, 0.0077, -0.0079, -0.1072, -0.0594, -0.0178, -0.0088, 0.1284, 0.0422, -0.0185, 0.0461, 0.0493, 0.1004, 0.0176, -0.0153, -0.0245, -0.0135, 0.0783, 0.0164, -0.0252, 0.295, -0.0319, 0.1103, 0.01, -0.0029, -0.1097, -0.0587, -0.0045, -0.0928, -0.306, 0.0651, -0.0328, 0.0264, -0.1218, -0.0211, 0.0559, -0.0267, 0.0574, -0.2908, 0.06, 0.1002, -0.251, 0.0734, -0.2104, 0.0144, 0.0174, 0.0529, -0.0086, 0.0128, 0.0311, -0.1217, 0.0557, 0.0154, 0.0036, 0.2933, 0.1089, 0.0268, 0.248, 0.0073, 0.0447, -0.2185, 0.071, -0.0075, -0.0175, -0.0303, -0.0635, 0.0388, 0.1332, 0.1139, -0.0396, 0.0176, 0.3505, -0.0208, -0.0023, 0.0646, 0.0537, -0.0967, 0.0124, -0.0847, 0.0001, 0.0942, 0.0022, 0.0561, 0.0588, -0.0494, 0.0042, -0.2125, -0.047, 0.0718, 0.0505, -0.045, 0.0896, 0.0691, -0.0113, -0.0217, -0.0209, -0.0119, 0.0106, 0.0667, 0.0265, 0.0548, -0.0067, 0.0107, -0.0307, 0.0228, 0.0027, 0.0046, 0.0296, -0.1323, 0.1093, -0.4649, 0.0183, 0.087, -0.0078, -0.1624, 0.0289, 0.0533, -0.0682, -0.0692, 0.104, 0.0964, 0.0392]\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0004, 0.0032, -0.0204, 0.0479, -0.045, -0.1165, 0.0142, 0.0068, -0.0334, -0.0504, 0.0224, -0.0029, -0.0258, 0.0265, 0.0059, -0.0459, 0.0753, 0.0422, 0.0269, -0.0283, -0.1013, 0.0992, -0.0114, 0.0583, -0.1547, -0.1972, -0.0282, -0.1391, -0.0288, -0.0283, 0.0273, 0.0189, 0.0275, -0.054, 0.0458, 0.0306, -0.0158, 0.2338, 0.0206, -0.0081, -0.018, -0.0059, 0.1045, 0.0409, 0.0352, -0.0038, 0.0403, -0.0129, -0.0074, 0.0003, -0.0484, 0.0412, -0.5999, 0.0224, -0.0153, 0.0296, 0.0011, 0.064, -0.1061, 0.0009, -0.0038, -0.0197, 0.0198, -0.0056, -0.0287, 0.0157, -0.0262, -0.0003, -0.0033, -0.0007, -0.0421, 0.0367, -0.024, -0.0519, -0.0098, 0.0297, 0.0251, -0.011, -0.0059, -0.0042, 0.0191, 0.0912, 0.0142, -0.0469, 0.0047, -0.0461, -0.0007, -0.0242, -0.1023, 0.0221, -0.0055, -0.0246, 0.0235, 0.1175, 0.0527, -0.0013, 0.0069, 0.0075, 0.0653, 0.0739, -0.0852, -0.017, -0.0102, -0.0225, -0.3273, -0.004, -0.0259, 0.0374, -0.1285, -0.026, 0.0512, 0.0295, -0.0648, 0.008, 0.01, -0.0888, 0.0268, 0.0209, 0.0172, -0.2961, 0.0117, -0.1024, -0.0671, -0.1541, 0.0014, 0.0895, -0.009, -0.0117, 0.0023, 0.0197, 0.0513, 0.0514, -0.0087, -0.0016, -0.0187, -0.1328, -0.0309, 0.0093, -0.016, -0.0328, 0.0123, -0.0135, 0.0707, -0.4418, -0.0293, 0.0321, 0.0725, -0.015, -0.0241, -0.0308, 0.1423, 0.0205, -0.0443, -0.0164, -0.004, 0.041, 0.0311, 0.0291, -0.0144, 0.0029, 0.1101, 0.0305, 0.0559, -0.1322, -0.2437, -0.0496, 0.1666, -0.0371, -0.0255, -0.0138, -0.2298, -0.006, 0.0206, 0.0459, -0.1113, -0.0365, -0.0248, -0.3067, 0.0166, 0.0334, 0.0021, -0.0163, 0.0237, -0.025, 0.0108, -0.1783, 0.0301, -0.0656, 0.1937, 0.0227, 0.0142, -0.0309, -0.0313, 0.0592, 0.0157, -0.0146, 0.0691, -0.0355, 0.2422, 0.0033, 0.0094, 0.0925, -0.028, -0.0084, 0.1211, 0.0053, -0.0082, 0.0111, -0.0628, -0.0273, 0.0068, 0.0178, -0.0397, 0.0079, 0.013, -0.0139, -0.1617, -0.035, -0.059, -0.0596, 0.0098, 0.0481, 0.0207, -0.0105, 0.0466, 0.2175, 0.0148, 0.0207, -0.0174, -0.1542, 0.0322, -0.0149, 0.6264, 0.0136, -0.0067, 0.243, -0.0644, -0.1055, -0.189, -0.0042, -0.0424, -0.0319, 0.0419, 0.0078, -0.0486, -0.0519, -0.0194, 0.032, 0.0181, 0.0615, -0.0305, -0.0008, -0.0281, 0.0642, 0.0569, 0.0512, -0.0689, -0.01, 0.0339, -0.001, -0.0024, 0.0837, 0.0032, -0.0312, -0.1129, 0.0081, -0.032, 0.0065, 0.0968, -0.0263, -0.0471, -0.0256, -0.0003, 0.0188, -0.0397, 0.0475, -0.0811, -0.043, -0.0117, 0.0414, -0.0028, 0.0524, 0.0216, 0.082, 0.0114, -0.0173, -0.0362, -0.0067, -0.0118, 0.0435, 0.0637, 0.0022, -0.0096, -0.036, -0.1679, 0.0304, 0.029, 0.207, 0.0689, -0.0467]\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.36s/it]\u001b[A\n","03/28/2021 18:11:15 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:15 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:15 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:15 - INFO - bert_utils -     loss = 0.10847373306751251\n","Epoch:  20%|███████▍                             | 5/25 [00:25<01:41,  5.07s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.35s/it]\u001b[A\n","03/28/2021 18:11:20 - INFO - bert_utils -     flaw_f1 = 0.05714285714285715\n","03/28/2021 18:11:20 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:11:20 - INFO - bert_utils -     flaw_recall = 0.029411764705882353\n","03/28/2021 18:11:20 - INFO - bert_utils -     loss = 0.11955753713846207\n","Epoch:  24%|████████▉                            | 6/25 [00:30<01:34,  5.00s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.35s/it]\u001b[A\n","03/28/2021 18:11:25 - INFO - bert_utils -     flaw_f1 = 0.46616541353383456\n","03/28/2021 18:11:25 - INFO - bert_utils -     flaw_precision = 0.30392156862745096\n","03/28/2021 18:11:25 - INFO - bert_utils -     flaw_recall = 1.0\n","03/28/2021 18:11:25 - INFO - bert_utils -     loss = 0.23758944869041443\n","Epoch:  28%|██████████▎                          | 7/25 [00:35<01:29,  4.97s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\u001b[A\n","03/28/2021 18:11:30 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:30 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:30 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:30 - INFO - bert_utils -     loss = 0.07304968684911728\n","Epoch:  32%|███████████▊                         | 8/25 [00:40<01:24,  4.99s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.1208, 0.0144, -0.0927, -0.0181, 0.1125, 0.0495, -0.0467, -0.0193, 0.0746, -0.0119, 0.0817, 0.0189, 0.0596, 0.0681, -0.0196, 0.1208, -0.0401, -0.0571, -0.1717, 0.1402, -0.0256, 0.0317, 0.0454, 0.1437, -0.0175, 0.0, 0.0317, 0.201, 0.0216, -0.0175, 0.0565, -0.0301, 0.0244, -0.045, 0.0697, -0.031, 0.0226, -0.1532, -0.0043, -0.0578, 0.0305, -0.0136, 0.0062, -0.0937, -0.0747, 0.0217, -0.0568, 0.0469, 0.0378, -0.0502, -0.1851, 0.1019, -0.5416, -0.0081, 0.0269, 0.0144, 0.0698, -0.1973, -0.1727, 0.0096, 0.0363, 0.0037, -0.0497, 0.0526, -0.0885, -0.0309, 0.0668, -0.0395, 0.0433, -0.0078, -0.0366, 0.0286, -0.0301, 0.0023, 0.0284, -0.0965, -0.0228, -0.0075, -0.0968, 0.2231, 0.0464, -0.0177, -0.0262, -0.2496, -0.0343, 0.0082, -0.087, -0.0224, -0.5577, 0.0062, 0.0702, 0.0396, -0.0915, 0.0162, -0.0607, 0.0235, 0.0355, -0.1029, -0.0998, -0.0685, -0.271, 0.0459, 0.0225, 0.0368, 0.0759, -0.0153, -0.3556, 0.1072, 0.1017, -0.048, 0.0321, 0.0134, 0.0368, 0.0471, -0.2595, 0.0088, 0.0406, 0.0368, -0.0217, -0.2414, -0.0309, -0.0005, -0.0251, 0.0478, 0.1212, 0.2134, 0.0031, 0.2574, 0.0242, 0.0058, 0.0395, 0.027, -0.1224, -0.0122, 0.0541, -0.256, -0.0568, 0.0002, 0.0708, -0.0554, -0.0341, -0.0603, -0.0489, 0.4024, -0.0091, -0.0236, 0.1828, -0.0233, 0.0515, -0.108, -0.0503, -0.0573, 0.1002, -0.0205, 0.0517, 0.0068, -0.068, 0.0079, 0.0352, 0.0006, 0.0082, 0.0527, -0.0963, -0.0826, 0.1342, 0.0436, 0.1095, 0.0198, -0.0052, 0.0599, -0.1023, 0.0427, 0.08, -0.0834, 0.1454, 0.1132, 0.2139, -0.6224, -0.0544, -0.0541, 0.0465, 0.036, 0.0616, 0.0823, 0.0118, -0.3318, 0.0023, -0.0795, 0.6474, 0.0441, -0.0659, -0.0212, -0.0356, 0.0759, 0.0593, 0.0551, 0.0249, 0.0295, 0.279, -0.0214, -0.0544, -0.0026, 0.0026, -0.0114, 0.0507, 0.023, -0.0792, -0.0548, -0.0457, 0.0767, -0.1153, 0.0243, 0.034, 0.0527, -0.0762, 0.0999, -0.045, -0.0452, 0.0266, -0.0179, -0.0063, -0.0617, 0.0071, 0.0549, 0.1069, -0.0226, -0.0059, -0.0535, -0.0097, -0.0183, 0.2572, -0.0157, 0.1969, 0.0353, -0.1263, -0.4035, 0.0456, -0.1514, -0.2378, 0.124, 0.0007, 0.0017, 0.0054, 0.0166, -0.0424, 0.033, 0.0231, -0.0127, 0.0388, 0.3186, 0.0278, -0.0014, -0.0062, 0.0322, 0.3341, 0.003, -0.1596, 0.0029, -0.0605, -0.0301, -0.1263, -0.0143, -0.0255, 0.0705, -0.8316, -0.0347, 0.0973, 0.0364, -0.019, -0.018, 0.0257, 0.0578, -0.0185, 0.0114, -0.0141, 0.0957, 0.1123, -0.0257, 0.0211, -0.0602, 0.1877, -0.0612, 0.0284, -0.0978, -0.0529, -0.0266, -0.0446, -0.0117, -0.1348, -0.0104, 0.0987, -0.0837, 0.1619, 0.05, -0.0111, 0.0056, -0.1176, 0.2829, 0.0629, 0.0723]\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0312, -0.0133, -0.0627, -0.0361, 0.0148, 0.0319, 0.0276, 0.0409, -0.0129, -0.0438, 0.0246, -0.087, -0.0367, -0.0649, -0.124, -0.0144, 0.0648, 0.0107, 0.1197, -0.0742, -0.0414, 0.0702, -0.094, -0.0256, -0.0283, -0.06, -0.1258, -0.0883, -0.0243, -0.1137, 0.0832, -0.0581, 0.1514, 0.0574, 0.0183, 0.0585, 0.0279, 0.0626, -0.0219, 0.0182, -0.0218, 0.031, -0.06, -0.0169, -0.0235, 0.0086, 0.0327, -0.0764, 0.0083, -0.002, -0.0496, -0.117, -0.6124, -0.009, 0.0031, -0.0017, -0.0477, -0.0916, -0.0452, -0.087, 0.0233, 0.0115, -0.0832, -0.0693, -0.0229, 0.1129, 0.0037, 0.0396, -0.0102, -0.0518, 0.052, -0.0792, -0.0331, -0.0026, 0.012, 0.0248, 0.0572, 0.0273, 0.109, 0.1978, -0.0371, -0.073, 0.0189, -0.2491, -0.007, 0.035, -0.0297, -0.043, -0.141, 0.0299, 0.1118, 0.0242, 0.0185, 0.0143, 0.0662, 0.0149, -0.0218, -0.0294, 0.0472, -0.0472, -0.133, -0.0284, -0.0129, 0.0475, -0.0595, -0.0138, 0.087, 0.0678, 0.0049, 0.0161, 0.0955, -0.1739, 0.0335, 0.1083, -0.0178, 0.0103, 0.0859, -0.0761, 0.0037, -0.3074, -0.0399, -0.0065, -0.0188, 0.0208, -0.1801, 0.1801, -0.0101, 0.1407, -0.1353, -0.0314, -0.029, 0.0156, -0.0953, -0.0124, 0.0264, -0.167, 0.0313, -0.1026, 0.0345, 0.0018, 0.0197, -0.0375, 0.0248, 0.3195, -0.0365, 0.0152, 0.1094, -0.026, -0.0107, -0.0482, 0.0186, -0.0329, 0.0716, 0.0384, 0.0098, 0.0439, -0.0697, 0.0827, 0.0656, 0.0292, 0.042, -0.0535, 0.0484, 0.0079, -0.1818, -0.0666, 0.1992, -0.0962, -0.0236, 0.0963, -0.0563, 0.009, 0.0392, 0.0302, 0.0433, -0.0182, 0.1542, -0.1861, 0.0033, 0.0275, -0.0038, -0.0268, -0.0119, 0.0041, 0.0624, 0.179, -0.0033, -0.1647, 0.0734, 0.0999, 0.0103, -0.0102, 0.0046, 0.0222, 0.0389, 0.0567, -0.0187, 0.0296, 0.3547, 0.0149, 0.0383, 0.0586, -0.0006, -0.0506, -0.0019, 0.0274, -0.0577, -0.0971, -0.0288, -0.0018, 0.1553, -0.0006, -0.0519, 0.0523, 0.0322, 0.0971, -0.2139, -0.0007, -0.0589, -0.0342, 0.1299, -0.0024, 0.0472, -0.0086, 0.0416, -0.1646, -0.0201, -0.0121, 0.0873, -0.0313, -0.0525, -0.0498, 0.3044, -0.0349, -0.1103, 0.1482, -0.0257, -0.1129, -0.2016, 0.0639, 0.0723, -0.0193, -0.0216, -0.0315, -0.0082, -0.0027, -0.0274, -0.0552, -0.0741, 0.2908, 0.0462, 0.0082, 0.0626, 0.0313, 0.0459, -0.0516, -0.1157, 0.0854, 0.0207, 0.0754, 0.0232, -0.0468, 0.0433, 0.0039, -0.1978, -0.0108, -0.0164, 0.0037, -0.0748, -0.0055, 0.1568, 0.0083, 0.0083, 0.0333, 0.0282, 0.0223, 0.005, -0.012, 0.0558, 0.0323, 0.0078, 0.013, 0.0351, -0.1201, -0.0071, -0.0138, -0.0476, 0.0112, -0.1684, 0.0376, -0.0111, -0.0568, -0.0641, 0.0521, 0.0665, -0.0576, -0.0074, 0.2201, 0.0738, -0.0633]\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0009, -0.0881, 0.1765, -0.0176, -0.1541, 0.0409, 0.0307, -0.0144, -0.0246, 0.105, 0.027, 0.0544, -0.0062, -0.0447, 0.0202, -0.058, 0.164, -0.0453, -0.024, -0.0168, -0.1362, -0.0312, -0.0716, 0.1757, -0.1112, 0.0055, -0.0377, -0.1418, -0.1168, 0.1707, 0.017, -0.068, -0.1887, 0.0019, 0.0381, 0.0233, 0.0031, -0.0202, -0.0539, -0.036, -0.1265, 0.0073, 0.1233, -0.0215, -0.1209, -0.0334, 0.1118, 0.1577, 0.0902, 0.0078, -0.0016, 0.0598, -0.661, -0.0722, -0.1674, 0.1576, 0.1408, 0.0291, -0.0688, 0.0597, -0.0933, -0.0038, -0.045, 0.0151, -0.1437, -0.0867, 0.0815, -0.1201, -0.0307, 0.0743, -0.1026, -0.0983, 0.0382, -0.0869, 0.0487, -0.0953, 0.1129, -0.1147, 0.1359, 0.1193, 0.0537, 0.0182, 0.0141, -0.2045, 0.0028, 0.0297, -0.0625, -0.0117, 0.2854, -0.0367, 0.0642, 0.0183, 0.0189, -0.0093, -0.0304, 0.0167, 0.1154, -0.0762, 0.1511, 0.0994, -0.1831, 0.0124, -0.0153, -0.0587, -0.0917, -0.0008, -0.0941, -0.0359, 0.0014, -0.0287, -0.0127, 0.0417, -0.0886, -0.0065, 0.1682, -0.0144, -0.0772, -0.0103, -0.0385, -0.2877, -0.0109, 0.0506, -0.0174, 0.1893, -0.0117, 0.2806, 0.0132, 0.0948, 0.0031, 0.0773, -0.0818, -0.0596, -0.0435, -0.1742, -0.1914, 0.0599, 0.0749, 0.058, 0.0998, -0.0271, -0.0564, -0.1195, 0.0616, 0.1958, -0.0821, 0.0322, -0.0055, 0.0017, 0.065, 0.051, 0.0739, 0.0013, -0.0018, -0.1519, 0.01, 0.0759, -0.1445, 0.0589, -0.0538, -0.1143, 0.0199, -0.0943, 0.0982, -0.1638, -0.0821, -0.0061, 0.1112, -0.0863, 0.013, -0.2725, -0.0053, -0.1254, 0.0097, 0.03, 0.0313, 0.1254, 0.2384, -0.4316, 0.0763, 0.0028, -0.0084, 0.0688, -0.0141, 0.0091, -0.055, -0.0366, -0.1464, -0.1668, -0.1206, -0.2776, -0.072, 0.1359, -0.0936, -0.0924, -0.0496, 0.0898, 0.0531, -0.059, 0.2331, -0.0631, -0.045, -0.0764, 0.0475, -0.0309, 0.1361, -0.0188, 0.0453, -0.0273, 0.0273, -0.0647, -0.0586, -0.0932, -0.2062, 0.1993, -0.0598, -0.0007, 0.007, 0.0067, -0.1401, -0.1123, -0.1363, -0.0536, 0.087, 0.0379, 0.1337, -0.1219, 0.1295, 0.0334, -0.0151, -0.1794, -0.1829, -0.0777, 0.3214, -0.0099, -0.0804, -0.0891, -0.1079, -0.0647, -0.2523, -0.2306, 0.0633, 0.0056, -0.1124, 0.004, -0.0387, -0.0482, 0.154, 0.1341, 0.0859, 0.3972, -0.0013, -0.1454, -0.1012, -0.1137, -0.0307, -0.0036, 0.1134, -0.0087, 0.1112, -0.0305, -0.0272, -0.0568, -0.0435, 0.0541, -0.3689, -0.0552, -0.1175, 0.0657, -0.1042, 0.043, 0.1862, -0.0658, 0.0141, 0.0465, -0.0112, 0.0364, 0.0719, -0.0872, -0.0338, 0.1012, -0.1876, 0.0278, -0.1171, -0.1303, 0.1601, 0.1416, -0.1077, 0.0635, 0.0294, 0.0996, -0.0902, -0.0323, 0.045, 0.1077, -0.0811, 0.1731, -0.0808, 0.1723, 0.009, 0.1053]\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.46s/it]\u001b[A\n","03/28/2021 18:11:35 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:35 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:35 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:35 - INFO - bert_utils -     loss = 0.07746419310569763\n","Epoch:  36%|█████████████▎                       | 9/25 [00:45<01:20,  5.01s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.01, -0.1164, -0.1155, 0.0128, 0.0066, 0.1118, 0.0019, 0.1206, 0.0421, -0.101, -0.004, -0.0397, -0.0289, 0.0068, -0.0612, 0.0406, 0.0486, 0.0586, 0.1727, -0.0766, 0.0337, -0.0184, -0.0366, 0.2488, 0.0496, -0.0027, -0.0022, -0.0953, 0.0506, -0.1002, 0.0485, 0.0158, -0.0052, 0.0754, 0.1312, 0.0125, -0.0013, 0.1345, 0.1304, -0.0553, 0.0419, -0.0352, -0.0879, 0.011, 0.1502, 0.077, 0.0504, -0.0595, 0.0213, 0.0447, -0.0306, 0.1326, -0.6322, -0.0197, 0.0337, -0.0027, -0.1529, -0.1602, -0.0631, -0.0668, -0.0243, 0.0625, -0.1633, -0.0073, -0.0559, -0.1796, -0.0004, -0.0384, 0.0424, -0.0468, 0.0202, -0.0719, -0.0158, 0.0609, -0.0076, -0.0106, 0.2165, 0.0046, 0.0026, 0.1756, 0.018, 0.0555, 0.0287, -0.2628, 0.0122, 0.1043, 0.109, -0.0115, -0.2083, -0.0649, 0.0975, 0.007, -0.001, -0.124, 0.03, 0.0333, -0.0295, 0.0596, 0.0244, -0.0265, -0.1616, -0.0581, 0.0439, 0.0733, -0.0714, -0.0393, 0.2064, 0.1114, -0.0533, 0.119, -0.0603, 0.0178, -0.0118, 0.0112, 0.1174, -0.0652, 0.0505, -0.1118, 0.0196, -0.3306, 0.0246, -0.0156, -0.0363, 0.0558, -0.0872, 0.1793, -0.0551, 0.2096, -0.3445, 0.0282, -0.0852, 0.0196, -0.1081, -0.0563, 0.138, -0.1404, 0.0364, -0.0088, 0.1013, 0.0078, 0.0305, -0.0422, -0.0317, 0.3136, -0.01, 0.0061, 0.1233, -0.1092, 0.0066, -0.0221, 0.1111, 0.0233, 0.0763, 0.0268, -0.0634, 0.0247, -0.0976, 0.112, 0.0448, 0.0234, 0.2722, -0.0352, -0.0033, -0.039, -0.1779, -0.0273, 0.1093, 0.0029, -0.0524, 0.0917, -0.0883, 0.0905, 0.0899, 0.0649, -0.0216, 0.0238, 0.232, -0.1989, -0.0122, 0.0077, -0.0079, -0.1072, -0.0594, -0.0178, -0.0088, 0.1284, 0.0422, -0.0185, 0.0461, 0.0493, 0.1004, 0.0176, -0.0153, -0.0245, -0.0135, 0.0783, 0.0164, -0.0252, 0.295, -0.0319, 0.1103, 0.01, -0.0029, -0.1097, -0.0587, -0.0045, -0.0928, -0.306, 0.0651, -0.0328, 0.0264, -0.1218, -0.0211, 0.0559, -0.0267, 0.0574, -0.2908, 0.06, 0.1002, -0.251, 0.0734, -0.2104, 0.0144, 0.0174, 0.0529, -0.0086, 0.0128, 0.0311, -0.1217, 0.0557, 0.0154, 0.0036, 0.2933, 0.1089, 0.0268, 0.248, 0.0073, 0.0447, -0.2185, 0.071, -0.0075, -0.0175, -0.0303, -0.0635, 0.0388, 0.1332, 0.1139, -0.0396, 0.0176, 0.3505, -0.0208, -0.0023, 0.0646, 0.0537, -0.0967, 0.0124, -0.0847, 0.0001, 0.0942, 0.0022, 0.0561, 0.0588, -0.0494, 0.0042, -0.2125, -0.047, 0.0718, 0.0505, -0.045, 0.0896, 0.0691, -0.0113, -0.0217, -0.0209, -0.0119, 0.0106, 0.0667, 0.0265, 0.0548, -0.0067, 0.0107, -0.0307, 0.0228, 0.0027, 0.0046, 0.0296, -0.1323, 0.1093, -0.4649, 0.0183, 0.087, -0.0078, -0.1624, 0.0289, 0.0533, -0.0682, -0.0692, 0.104, 0.0964, 0.0392]\n","tok_id :  31\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0312, -0.0133, -0.0627, -0.0361, 0.0148, 0.0319, 0.0276, 0.0409, -0.0129, -0.0438, 0.0246, -0.087, -0.0367, -0.0649, -0.124, -0.0144, 0.0648, 0.0107, 0.1197, -0.0742, -0.0414, 0.0702, -0.094, -0.0256, -0.0283, -0.06, -0.1258, -0.0883, -0.0243, -0.1137, 0.0832, -0.0581, 0.1514, 0.0574, 0.0183, 0.0585, 0.0279, 0.0626, -0.0219, 0.0182, -0.0218, 0.031, -0.06, -0.0169, -0.0235, 0.0086, 0.0327, -0.0764, 0.0083, -0.002, -0.0496, -0.117, -0.6124, -0.009, 0.0031, -0.0017, -0.0477, -0.0916, -0.0452, -0.087, 0.0233, 0.0115, -0.0832, -0.0693, -0.0229, 0.1129, 0.0037, 0.0396, -0.0102, -0.0518, 0.052, -0.0792, -0.0331, -0.0026, 0.012, 0.0248, 0.0572, 0.0273, 0.109, 0.1978, -0.0371, -0.073, 0.0189, -0.2491, -0.007, 0.035, -0.0297, -0.043, -0.141, 0.0299, 0.1118, 0.0242, 0.0185, 0.0143, 0.0662, 0.0149, -0.0218, -0.0294, 0.0472, -0.0472, -0.133, -0.0284, -0.0129, 0.0475, -0.0595, -0.0138, 0.087, 0.0678, 0.0049, 0.0161, 0.0955, -0.1739, 0.0335, 0.1083, -0.0178, 0.0103, 0.0859, -0.0761, 0.0037, -0.3074, -0.0399, -0.0065, -0.0188, 0.0208, -0.1801, 0.1801, -0.0101, 0.1407, -0.1353, -0.0314, -0.029, 0.0156, -0.0953, -0.0124, 0.0264, -0.167, 0.0313, -0.1026, 0.0345, 0.0018, 0.0197, -0.0375, 0.0248, 0.3195, -0.0365, 0.0152, 0.1094, -0.026, -0.0107, -0.0482, 0.0186, -0.0329, 0.0716, 0.0384, 0.0098, 0.0439, -0.0697, 0.0827, 0.0656, 0.0292, 0.042, -0.0535, 0.0484, 0.0079, -0.1818, -0.0666, 0.1992, -0.0962, -0.0236, 0.0963, -0.0563, 0.009, 0.0392, 0.0302, 0.0433, -0.0182, 0.1542, -0.1861, 0.0033, 0.0275, -0.0038, -0.0268, -0.0119, 0.0041, 0.0624, 0.179, -0.0033, -0.1647, 0.0734, 0.0999, 0.0103, -0.0102, 0.0046, 0.0222, 0.0389, 0.0567, -0.0187, 0.0296, 0.3547, 0.0149, 0.0383, 0.0586, -0.0006, -0.0506, -0.0019, 0.0274, -0.0577, -0.0971, -0.0288, -0.0018, 0.1553, -0.0006, -0.0519, 0.0523, 0.0322, 0.0971, -0.2139, -0.0007, -0.0589, -0.0342, 0.1299, -0.0024, 0.0472, -0.0086, 0.0416, -0.1646, -0.0201, -0.0121, 0.0873, -0.0313, -0.0525, -0.0498, 0.3044, -0.0349, -0.1103, 0.1482, -0.0257, -0.1129, -0.2016, 0.0639, 0.0723, -0.0193, -0.0216, -0.0315, -0.0082, -0.0027, -0.0274, -0.0552, -0.0741, 0.2908, 0.0462, 0.0082, 0.0626, 0.0313, 0.0459, -0.0516, -0.1157, 0.0854, 0.0207, 0.0754, 0.0232, -0.0468, 0.0433, 0.0039, -0.1978, -0.0108, -0.0164, 0.0037, -0.0748, -0.0055, 0.1568, 0.0083, 0.0083, 0.0333, 0.0282, 0.0223, 0.005, -0.012, 0.0558, 0.0323, 0.0078, 0.013, 0.0351, -0.1201, -0.0071, -0.0138, -0.0476, 0.0112, -0.1684, 0.0376, -0.0111, -0.0568, -0.0641, 0.0521, 0.0665, -0.0576, -0.0074, 0.2201, 0.0738, -0.0633]\n","tok_id :  32\n","tok_id :  16\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0495, 0.0411, 0.0041, 0.0309, -0.0044, -0.1151, 0.006, 0.017, 0.0045, -0.0288, 0.017, 0.0007, 0.0533, 0.0094, -0.0609, -0.0267, 0.0497, 0.0474, 0.0054, 0.0511, -0.0715, 0.0876, 0.055, -0.001, -0.0746, 0.0008, 0.0258, -0.1404, 0.0022, 0.0469, 0.0114, 0.0083, -0.0127, -0.0453, 0.0011, -0.008, -0.013, -0.1271, 0.002, 0.0389, -0.0395, -0.0295, -0.0308, 0.0348, -0.1388, -0.0647, 0.0302, 0.0184, 0.0499, 0.0168, -0.0176, 0.089, -0.5547, 0.0144, 0.03, 0.0127, 0.0345, 0.1792, 0.0629, -0.0242, -0.0491, -0.0397, -0.0014, -0.0571, 0.0906, -0.0009, 0.0266, -0.0018, 0.0308, -0.0057, 0.0569, 0.0273, -0.0338, 0.1003, 0.0299, 0.0115, 0.0717, 0.0319, -0.0726, 0.1526, -0.0026, -0.1321, -0.0287, -0.2439, 0.0073, -0.0062, 0.0101, -0.0128, -0.0106, 0.0202, -0.0165, -0.0867, 0.0493, -0.0916, 0.0507, 0.1032, 0.0108, 0.0881, 0.0655, -0.0127, -0.0895, -0.0348, 0.0439, 0.0069, -0.3768, -0.0176, 0.1296, 0.0027, 0.2343, -0.0009, 0.0337, 0.0613, -0.0369, 0.0564, -0.0901, -0.0046, 0.036, 0.0341, -0.0171, -0.1717, -0.0041, -0.0553, -0.0661, 0.0957, -0.0804, 0.0868, -0.0181, -0.0602, -0.1523, -0.0104, -0.0034, -0.0547, 0.0094, -0.0223, -0.0184, -0.3151, -0.0358, 0.0354, 0.0393, 0.0526, 0.001, -0.0163, 0.0497, 0.2518, -0.0173, -0.0036, 0.018, -0.1081, 0.0368, -0.0141, -0.0436, 0.0291, -0.0366, -0.0523, 0.0464, 0.0018, -0.0183, 0.0766, 0.0156, 0.0276, 0.0522, -0.0221, 0.0408, -0.0703, -0.2291, -0.003, 0.0343, -0.0961, -0.0092, 0.0222, 0.0166, -0.0344, 0.0463, 0.0186, 0.0283, -0.0522, 0.0369, -0.4955, 0.0276, -0.0247, 0.0257, 0.0632, -0.0232, -0.0063, -0.0076, -0.3897, -0.0108, -0.0612, 0.1962, -0.006, -0.0353, -0.0994, -0.0124, -0.0031, 0.0427, 0.0134, -0.0043, -0.0102, 0.21, 0.0163, -0.0155, 0.1707, -0.0339, -0.0125, 0.02, 0.0148, -0.0342, -0.0254, 0.0001, 0.0412, -0.0258, -0.0169, 0.0113, 0.0031, 0.0075, -0.0059, 0.1255, 0.0225, 0.0005, 0.0462, 0.0096, -0.0664, 0.0138, 0.0511, 0.0372, 0.0073, -0.0187, 0.0186, -0.0021, -0.012, 0.0007, -0.0026, 0.1725, 0.079, 0.0265, -0.0066, -0.0765, 0.2336, -0.1445, 0.0072, -0.026, -0.0147, 0.0521, 0.0011, -0.0244, 0.0583, -0.0207, 0.032, 0.0294, 0.483, -0.0444, -0.087, -0.0754, 0.0276, -0.0606, -0.0227, -0.0057, -0.0298, 0.054, -0.0607, -0.0746, 0.0666, -0.024, -0.0399, -0.2321, 0.0054, 0.0233, 0.046, 0.1874, -0.053, -0.0285, -0.0521, -0.0146, 0.0264, -0.0093, 0.025, -0.0552, 0.0152, 0.0242, -0.0577, 0.006, 0.0511, 0.023, -0.0345, 0.0134, -0.0042, -0.1267, -0.1572, -0.0783, 0.0706, 0.0004, -0.0142, -0.0976, -0.0489, -0.0625, -0.0327, 0.007, 0.2371, -0.0298, -0.0284]\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0169, 0.0542, 0.0311, -0.0461, 0.0195, 0.0657, -0.049, 0.0718, 0.0308, -0.0474, 0.0373, -0.0207, -0.0222, -0.0486, -0.1308, 0.051, 0.0104, -0.1136, -0.1407, 0.0007, -0.0533, -0.0364, 0.0279, -0.1236, -0.0104, -0.0262, 0.0281, -0.0941, 0.0612, 0.0196, 0.0554, -0.0727, 0.0461, -0.0601, -0.0269, 0.0226, -0.0258, -0.0506, 0.0607, -0.0627, -0.0754, -0.0065, -0.1208, -0.0697, -0.0514, 0.055, -0.0268, 0.022, 0.0651, -0.063, 0.0938, -0.1551, -0.6362, 0.0753, 0.0465, -0.0253, 0.0198, 0.0698, 0.0703, 0.0622, 0.015, -0.0942, -0.065, -0.1049, 0.0751, -0.1141, 0.0022, 0.0171, -0.0676, -0.03, 0.0063, -0.0088, -0.0381, -0.0068, 0.0143, -0.0023, 0.0603, 0.023, -0.04, 0.1527, 0.0037, -0.0434, 0.0, -0.2494, -0.1049, -0.0362, 0.0832, 0.0071, 0.0319, 0.0803, 0.0325, -0.0535, -0.0142, -0.0326, -0.0205, -0.0343, -0.0712, -0.0522, 0.082, 0.0663, -0.1346, -0.0427, -0.0286, 0.0231, -0.201, -0.0192, 0.1474, -0.1347, -0.0674, 0.0379, 0.0173, 0.1309, -0.0477, 0.0143, -0.1489, 0.0666, -0.0036, -0.0256, -0.0121, -0.3078, -0.0362, 0.042, -0.0266, 0.1928, -0.0723, 0.1245, 0.0488, 0.117, 0.0043, -0.0332, 0.0002, -0.0555, 0.0043, -0.048, -0.0064, -0.1869, -0.042, -0.0004, 0.1103, -0.1727, -0.0189, 0.0143, 0.0159, 0.3573, 0.0272, 0.039, 0.0773, 0.0238, -0.0406, 0.009, 0.1301, -0.013, -0.0266, -0.0657, -0.002, 0.0254, -0.043, 0.0677, 0.0368, -0.0019, -0.0161, 0.0341, 0.0689, -0.0734, -0.285, 0.0103, 0.1173, 0.1027, -0.0588, -0.1237, 0.0464, 0.02, 0.0045, 0.0427, 0.2493, -0.0509, 0.1712, -0.0888, 0.1136, 0.0143, 0.0118, 0.0735, -0.0039, 0.0056, -0.0206, 0.0521, 0.0371, -0.0837, 0.2374, -0.0178, -0.0484, 0.0031, -0.0008, -0.0601, -0.0609, -0.0021, -0.0568, -0.0317, 0.2517, -0.0359, 0.0292, -0.0984, -0.0113, -0.0954, 0.0397, -0.1018, 0.0002, -0.0668, -0.0187, 0.0313, -0.0811, 0.0204, -0.0357, -0.035, -0.0121, -0.0259, -0.1144, 0.0894, -0.0788, -0.0201, 0.0817, -0.0239, -0.0632, -0.0535, 0.0203, 0.0705, 0.0715, -0.0494, -0.1694, -0.1397, -0.0321, 0.0185, 0.327, -0.093, -0.0162, 0.0699, -0.0131, 0.2046, -0.1557, 0.1149, 0.001, 0.0126, -0.0164, -0.0303, 0.0191, 0.0303, -0.0135, 0.0564, 0.0331, 0.2624, 0.0173, 0.007, 0.0385, -0.1233, -0.2565, 0.0582, -0.0977, -0.0546, 0.0624, 0.019, 0.1011, 0.0149, 0.0108, -0.0075, -0.1214, 0.0641, -0.0207, 0.0869, 0.0696, 0.085, -0.0308, -0.0189, 0.0067, -0.0064, 0.0047, -0.0595, 0.1695, -0.0391, 0.0638, -0.0239, 0.0583, 0.1853, -0.0096, -0.0887, 0.0432, -0.0111, 0.0581, 0.1766, 0.0378, 0.0417, 0.0937, -0.0021, -0.0388, -0.0136, -0.0179, -0.0308, -0.0366, 0.1723, 0.0272, -0.1241]\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.37s/it]\u001b[A\n","03/28/2021 18:11:40 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:40 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:40 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:40 - INFO - bert_utils -     loss = 0.17405396699905396\n","Epoch:  40%|██████████████▍                     | 10/25 [00:50<01:14,  5.00s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0685, 0.0436, 0.0876, 0.0343, -0.1407, -0.0022, 0.0907, 0.1207, -0.0634, 0.0474, -0.0613, -0.2585, 0.1362, 0.0302, 0.0678, -0.1044, 0.0679, -0.0501, -0.046, 0.0721, -0.0615, 0.0052, 0.0265, 0.0553, 0.0739, 0.1436, 0.0617, -0.2076, 0.0626, -0.0215, 0.0303, -0.0909, 0.0796, -0.0468, 0.0471, -0.0377, 0.1013, -0.2236, -0.0454, 0.0141, 0.0549, -0.0494, 0.0921, -0.0206, -0.0119, 0.0382, 0.0383, 0.068, 0.1218, 0.1234, -0.2106, 0.033, -0.8115, 0.0913, -0.0889, 0.1473, -0.2201, -0.0381, 0.1106, -0.0855, 0.0832, 0.054, -0.1591, -0.1177, 0.0305, -0.0727, 0.1166, -0.1095, 0.0617, 0.0556, -0.1038, -0.1466, -0.0321, 0.0165, 0.0803, 0.1344, 0.1113, 0.0537, 0.1112, 0.0829, -0.0197, 0.1339, -0.0772, -0.214, -0.0254, -0.1179, -0.1341, -0.104, 0.0344, 0.1171, -0.0044, -0.0009, 0.142, 0.0659, 0.0995, -0.0581, 0.1026, 0.1173, 0.0989, -0.1691, -0.141, 0.2469, -0.0646, -0.0783, 0.0477, -0.098, 0.217, -0.0115, 0.0215, 0.0928, 0.0171, -0.0789, 0.0312, 0.0426, -0.0341, 0.0256, 0.0014, 0.0331, -0.0163, -0.2818, -0.0578, 0.0467, -0.1878, 0.1264, -0.3744, 0.1664, 0.0603, 0.1012, -0.09, 0.0681, -0.1515, 0.0238, -0.1266, 0.0194, 0.1907, -0.2536, -0.1319, 0.0278, 0.0734, 0.0086, 0.0381, -0.0133, -0.0044, 0.2154, -0.0498, 0.0575, 0.0033, -0.0922, -0.0268, 0.1245, -0.0261, -0.1517, -0.0902, -0.1291, -0.0064, 0.0302, 0.1057, 0.1446, 0.0221, -0.0246, 0.1665, -0.0093, 0.0622, 0.0727, 0.1815, 0.1833, 0.1672, 0.0655, -0.1608, -0.0446, 0.0452, -0.0805, 0.2006, -0.0761, 0.1609, 0.005, 0.2726, 0.0181, -0.07, 0.0954, -0.0658, -0.0871, -0.2224, -0.0431, -0.0652, 0.1016, -0.0707, -0.1596, 0.0328, 0.0733, -0.1572, 0.0093, -0.2113, -0.106, -0.0783, 0.0909, 0.0886, 0.0824, 0.1612, -0.1643, 0.0743, 0.052, 0.0249, -0.1409, -0.0267, 0.0257, 0.0348, -0.111, -0.0206, 0.1474, 0.0508, -0.2169, -0.0269, -0.05, -0.0022, -0.0559, -0.0335, -0.0472, -0.1692, -0.0924, 0.2929, 0.1039, -0.1878, -0.0744, -0.0394, -0.1387, 0.2079, 0.038, 0.0911, -0.3023, 0.1811, -0.1264, 0.2608, -0.0484, 0.1155, 0.0116, -0.0354, -0.3764, -0.2078, 0.0478, -0.0017, -0.0003, 0.0043, 0.1262, -0.0336, -0.1264, 0.0736, -0.066, -0.1539, 0.3558, -0.1012, 0.1345, 0.1251, -0.0782, 0.026, 0.0163, -0.0148, 0.0403, 0.0377, 0.07, 0.0038, -0.1903, 0.2018, 0.16, 0.0563, 0.0419, 0.2017, 0.1132, -0.1395, 0.0497, -0.0405, 0.0217, 0.0512, -0.0171, -0.0249, 0.1802, 0.0679, -0.0776, -0.0647, 0.0203, -0.196, -0.2066, -0.1197, -0.1547, 0.3984, 0.0785, -0.0163, 0.199, -0.0673, -0.0226, -0.0658, 0.082, -0.0271, -0.1362, -0.0309, -0.0124, -0.0784, 0.0714, 0.1751, -0.0285]\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.38s/it]\u001b[A\n","03/28/2021 18:11:45 - INFO - bert_utils -     flaw_f1 = 0.0\n","03/28/2021 18:11:45 - INFO - bert_utils -     flaw_precision = 0.0\n","03/28/2021 18:11:45 - INFO - bert_utils -     flaw_recall = 0.0\n","03/28/2021 18:11:45 - INFO - bert_utils -     loss = 0.06341788917779922\n","Epoch:  44%|███████████████▊                    | 11/25 [00:55<01:09,  4.99s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0312, -0.0133, -0.0627, -0.0361, 0.0148, 0.0319, 0.0276, 0.0409, -0.0129, -0.0438, 0.0246, -0.087, -0.0367, -0.0649, -0.124, -0.0144, 0.0648, 0.0107, 0.1197, -0.0742, -0.0414, 0.0702, -0.094, -0.0256, -0.0283, -0.06, -0.1258, -0.0883, -0.0243, -0.1137, 0.0832, -0.0581, 0.1514, 0.0574, 0.0183, 0.0585, 0.0279, 0.0626, -0.0219, 0.0182, -0.0218, 0.031, -0.06, -0.0169, -0.0235, 0.0086, 0.0327, -0.0764, 0.0083, -0.002, -0.0496, -0.117, -0.6124, -0.009, 0.0031, -0.0017, -0.0477, -0.0916, -0.0452, -0.087, 0.0233, 0.0115, -0.0832, -0.0693, -0.0229, 0.1129, 0.0037, 0.0396, -0.0102, -0.0518, 0.052, -0.0792, -0.0331, -0.0026, 0.012, 0.0248, 0.0572, 0.0273, 0.109, 0.1978, -0.0371, -0.073, 0.0189, -0.2491, -0.007, 0.035, -0.0297, -0.043, -0.141, 0.0299, 0.1118, 0.0242, 0.0185, 0.0143, 0.0662, 0.0149, -0.0218, -0.0294, 0.0472, -0.0472, -0.133, -0.0284, -0.0129, 0.0475, -0.0595, -0.0138, 0.087, 0.0678, 0.0049, 0.0161, 0.0955, -0.1739, 0.0335, 0.1083, -0.0178, 0.0103, 0.0859, -0.0761, 0.0037, -0.3074, -0.0399, -0.0065, -0.0188, 0.0208, -0.1801, 0.1801, -0.0101, 0.1407, -0.1353, -0.0314, -0.029, 0.0156, -0.0953, -0.0124, 0.0264, -0.167, 0.0313, -0.1026, 0.0345, 0.0018, 0.0197, -0.0375, 0.0248, 0.3195, -0.0365, 0.0152, 0.1094, -0.026, -0.0107, -0.0482, 0.0186, -0.0329, 0.0716, 0.0384, 0.0098, 0.0439, -0.0697, 0.0827, 0.0656, 0.0292, 0.042, -0.0535, 0.0484, 0.0079, -0.1818, -0.0666, 0.1992, -0.0962, -0.0236, 0.0963, -0.0563, 0.009, 0.0392, 0.0302, 0.0433, -0.0182, 0.1542, -0.1861, 0.0033, 0.0275, -0.0038, -0.0268, -0.0119, 0.0041, 0.0624, 0.179, -0.0033, -0.1647, 0.0734, 0.0999, 0.0103, -0.0102, 0.0046, 0.0222, 0.0389, 0.0567, -0.0187, 0.0296, 0.3547, 0.0149, 0.0383, 0.0586, -0.0006, -0.0506, -0.0019, 0.0274, -0.0577, -0.0971, -0.0288, -0.0018, 0.1553, -0.0006, -0.0519, 0.0523, 0.0322, 0.0971, -0.2139, -0.0007, -0.0589, -0.0342, 0.1299, -0.0024, 0.0472, -0.0086, 0.0416, -0.1646, -0.0201, -0.0121, 0.0873, -0.0313, -0.0525, -0.0498, 0.3044, -0.0349, -0.1103, 0.1482, -0.0257, -0.1129, -0.2016, 0.0639, 0.0723, -0.0193, -0.0216, -0.0315, -0.0082, -0.0027, -0.0274, -0.0552, -0.0741, 0.2908, 0.0462, 0.0082, 0.0626, 0.0313, 0.0459, -0.0516, -0.1157, 0.0854, 0.0207, 0.0754, 0.0232, -0.0468, 0.0433, 0.0039, -0.1978, -0.0108, -0.0164, 0.0037, -0.0748, -0.0055, 0.1568, 0.0083, 0.0083, 0.0333, 0.0282, 0.0223, 0.005, -0.012, 0.0558, 0.0323, 0.0078, 0.013, 0.0351, -0.1201, -0.0071, -0.0138, -0.0476, 0.0112, -0.1684, 0.0376, -0.0111, -0.0568, -0.0641, 0.0521, 0.0665, -0.0576, -0.0074, 0.2201, 0.0738, -0.0633]\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.1208, 0.0144, -0.0927, -0.0181, 0.1125, 0.0495, -0.0467, -0.0193, 0.0746, -0.0119, 0.0817, 0.0189, 0.0596, 0.0681, -0.0196, 0.1208, -0.0401, -0.0571, -0.1717, 0.1402, -0.0256, 0.0317, 0.0454, 0.1437, -0.0175, 0.0, 0.0317, 0.201, 0.0216, -0.0175, 0.0565, -0.0301, 0.0244, -0.045, 0.0697, -0.031, 0.0226, -0.1532, -0.0043, -0.0578, 0.0305, -0.0136, 0.0062, -0.0937, -0.0747, 0.0217, -0.0568, 0.0469, 0.0378, -0.0502, -0.1851, 0.1019, -0.5416, -0.0081, 0.0269, 0.0144, 0.0698, -0.1973, -0.1727, 0.0096, 0.0363, 0.0037, -0.0497, 0.0526, -0.0885, -0.0309, 0.0668, -0.0395, 0.0433, -0.0078, -0.0366, 0.0286, -0.0301, 0.0023, 0.0284, -0.0965, -0.0228, -0.0075, -0.0968, 0.2231, 0.0464, -0.0177, -0.0262, -0.2496, -0.0343, 0.0082, -0.087, -0.0224, -0.5577, 0.0062, 0.0702, 0.0396, -0.0915, 0.0162, -0.0607, 0.0235, 0.0355, -0.1029, -0.0998, -0.0685, -0.271, 0.0459, 0.0225, 0.0368, 0.0759, -0.0153, -0.3556, 0.1072, 0.1017, -0.048, 0.0321, 0.0134, 0.0368, 0.0471, -0.2595, 0.0088, 0.0406, 0.0368, -0.0217, -0.2414, -0.0309, -0.0005, -0.0251, 0.0478, 0.1212, 0.2134, 0.0031, 0.2574, 0.0242, 0.0058, 0.0395, 0.027, -0.1224, -0.0122, 0.0541, -0.256, -0.0568, 0.0002, 0.0708, -0.0554, -0.0341, -0.0603, -0.0489, 0.4024, -0.0091, -0.0236, 0.1828, -0.0233, 0.0515, -0.108, -0.0503, -0.0573, 0.1002, -0.0205, 0.0517, 0.0068, -0.068, 0.0079, 0.0352, 0.0006, 0.0082, 0.0527, -0.0963, -0.0826, 0.1342, 0.0436, 0.1095, 0.0198, -0.0052, 0.0599, -0.1023, 0.0427, 0.08, -0.0834, 0.1454, 0.1132, 0.2139, -0.6224, -0.0544, -0.0541, 0.0465, 0.036, 0.0616, 0.0823, 0.0118, -0.3318, 0.0023, -0.0795, 0.6474, 0.0441, -0.0659, -0.0212, -0.0356, 0.0759, 0.0593, 0.0551, 0.0249, 0.0295, 0.279, -0.0214, -0.0544, -0.0026, 0.0026, -0.0114, 0.0507, 0.023, -0.0792, -0.0548, -0.0457, 0.0767, -0.1153, 0.0243, 0.034, 0.0527, -0.0762, 0.0999, -0.045, -0.0452, 0.0266, -0.0179, -0.0063, -0.0617, 0.0071, 0.0549, 0.1069, -0.0226, -0.0059, -0.0535, -0.0097, -0.0183, 0.2572, -0.0157, 0.1969, 0.0353, -0.1263, -0.4035, 0.0456, -0.1514, -0.2378, 0.124, 0.0007, 0.0017, 0.0054, 0.0166, -0.0424, 0.033, 0.0231, -0.0127, 0.0388, 0.3186, 0.0278, -0.0014, -0.0062, 0.0322, 0.3341, 0.003, -0.1596, 0.0029, -0.0605, -0.0301, -0.1263, -0.0143, -0.0255, 0.0705, -0.8316, -0.0347, 0.0973, 0.0364, -0.019, -0.018, 0.0257, 0.0578, -0.0185, 0.0114, -0.0141, 0.0957, 0.1123, -0.0257, 0.0211, -0.0602, 0.1877, -0.0612, 0.0284, -0.0978, -0.0529, -0.0266, -0.0446, -0.0117, -0.1348, -0.0104, 0.0987, -0.0837, 0.1619, 0.05, -0.0111, 0.0056, -0.1176, 0.2829, 0.0629, 0.0723]\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0314, 0.0149, -0.0205, 0.0557, 0.0205, -0.0405, 0.0044, -0.0118, -0.0424, -0.049, 0.0123, -0.0023, 0.0083, 0.0203, -0.0055, 0.0004, 0.0031, 0.0834, -0.0637, 0.0349, -0.0457, 0.0898, 0.0432, -0.0026, -0.0806, 0.0526, 0.0038, -0.0263, 0.0262, 0.0294, -0.0055, 0.0247, 0.0263, -0.018, -0.0112, 0.0273, -0.0116, 0.0214, -0.0195, 0.0116, -0.0549, -0.0488, 0.0112, 0.0063, -0.0783, -0.002, 0.0292, -0.0212, 0.0208, 0.0024, -0.0147, 0.0165, -0.5414, 0.03, 0.0143, -0.0017, -0.0407, 0.1298, -0.0201, 0.0314, -0.0184, -0.0424, 0.0014, -0.0571, 0.0164, 0.0138, 0.0341, 0.0563, 0.0247, 0.0347, 0.0689, 0.0543, -0.0183, -0.0371, 0.0431, 0.012, 0.0688, -0.0098, -0.0226, 0.1135, -0.0141, -0.0054, -0.0119, -0.0566, -0.0205, 0.0233, 0.0102, -0.0315, -0.0161, -0.0019, 0.0033, -0.0457, -0.0164, 0.0118, 0.0428, 0.0267, -0.0266, -0.0141, 0.0576, 0.0513, -0.071, -0.0518, 0.0271, 0.0022, -0.1131, 0.0104, -0.1406, 0.0032, 0.017, 0.0329, 0.002, -0.0997, -0.0957, 0.0234, 0.0161, -0.001, 0.0106, -0.0022, 0.0124, -0.2695, -0.0634, -0.0576, -0.0199, 0.0576, -0.1378, 0.0771, 0.0437, 0.0569, 0.0588, 0.0295, -0.0351, 0.0381, -0.0133, 0.0161, -0.0503, -0.2031, -0.0856, 0.0608, -0.0175, -0.022, -0.0389, 0.0169, 0.0498, 0.3705, -0.0604, -0.0628, -0.0244, -0.0707, -0.0015, -0.0172, 0.0158, 0.0173, -0.0505, -0.0219, 0.0269, 0.0487, -0.0391, 0.0063, -0.0457, -0.0057, 0.027, -0.0346, 0.0418, -0.0495, -0.3807, -0.0217, 0.0251, -0.0074, -0.0195, -0.0083, -0.1613, -0.0667, -0.0322, 0.0747, 0.0054, -0.0337, 0.0282, -0.2981, 0.052, -0.0155, 0.0078, -0.0601, 0.0236, 0.0141, 0.0156, 0.0074, 0.0129, -0.0426, 0.2257, 0.0365, -0.0322, -0.0239, 0.0033, -0.0038, 0.0397, -0.0383, -0.0171, -0.0329, -0.0741, 0.0276, 0.0548, 0.0042, -0.0282, 0.0351, 0.0503, 0.0134, -0.0078, 0.0157, 0.0043, 0.0211, -0.0642, 0.0078, -0.0224, 0.046, -0.0332, -0.019, -0.1574, -0.0022, 0.03, 0.0958, -0.0508, -0.0002, 0.0108, 0.0007, -0.0441, -0.031, -0.0396, 0.0195, -0.0184, 0.1056, 0.0989, -0.0166, 0.319, -0.0119, -0.0023, -0.018, 0.0394, -0.0697, -0.1624, 0.0192, 0.0027, 0.0104, 0.0139, 0.0033, -0.0001, -0.0441, 0.0124, -0.0177, 0.0182, 0.4824, 0.02, 0.0509, -0.0566, 0.0301, 0.0316, 0.0032, -0.0746, 0.0167, 0.0208, -0.025, -0.0214, 0.0037, -0.0355, 0.0386, -0.1528, -0.0324, 0.0454, 0.0605, 0.0038, 0.0018, 0.008, 0.0331, -0.005, -0.0109, -0.0723, 0.0122, 0.0056, 0.0147, 0.031, -0.0107, 0.0094, -0.0049, 0.0782, 0.0244, 0.0615, -0.0404, -0.0025, -0.0371, 0.0194, 0.0174, -0.0003, 0.0575, -0.0078, -0.0101, -0.0901, -0.0214, 0.0267, 0.098, 0.0893, 0.0148]\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0522, -0.0577, -0.08, -0.0138, -0.0632, 0.0383, -0.0158, 0.1009, 0.1504, 0.0328, 0.0768, -0.0675, -0.0245, 0.0406, -0.0328, -0.0579, -0.017, 0.011, -0.126, 0.1232, -0.0153, 0.0165, 0.105, -0.0045, 0.025, 0.053, -0.005, -0.1389, -0.0299, -0.0116, 0.0492, -0.0364, -0.1012, -0.0191, 0.0373, -0.081, 0.0657, 0.0358, 0.0732, -0.0816, 0.0009, -0.0249, -0.077, 0.1623, -0.1175, -0.0071, -0.038, 0.0305, 0.062, 0.0618, -0.0716, -0.1001, -0.5564, 0.0087, 0.0555, 0.0177, 0.0222, -0.0323, 0.1196, -0.0463, -0.0475, 0.053, -0.0289, -0.0057, 0.047, 0.0076, 0.0207, -0.0438, -0.0163, 0.011, 0.0221, -0.0711, -0.0315, -0.0356, 0.0074, 0.0709, 0.1003, 0.0979, 0.0106, 0.0881, 0.0003, 0.1519, 0.0095, -0.2199, -0.0371, 0.0366, -0.0174, -0.0459, -0.1171, 0.0885, -0.0168, -0.0144, -0.0345, 0.0175, 0.0443, -0.008, -0.0116, 0.0218, -0.0292, -0.0804, -0.1982, 0.017, 0.0099, 0.0297, 0.0073, -0.0415, 0.0753, -0.0301, -0.1676, 0.0376, 0.033, -0.3744, -0.049, -0.0712, 0.0666, -0.0117, 0.0585, -0.0962, -0.0816, -0.2582, -0.0403, -0.1146, -0.0818, 0.1487, -0.1585, 0.2021, -0.0174, 0.1464, -0.1206, 0.0154, -0.0007, -0.0399, 0.0218, -0.1013, 0.0487, -0.2436, 0.0481, -0.0288, 0.0094, 0.0108, -0.0466, -0.0301, 0.0111, 0.2631, 0.1462, 0.0456, 0.1315, -0.1163, -0.1031, -0.014, -0.0452, 0.01, -0.0869, -0.0759, 0.0424, 0.0302, -0.0585, -0.0625, -0.0263, -0.0134, 0.164, 0.1405, -0.0468, -0.0048, -0.3448, 0.0088, 0.0358, 0.0816, -0.0784, -0.0014, -0.0572, -0.0078, 0.0553, 0.0434, 0.3011, -0.0993, 0.2263, -0.1553, 0.0837, 0.0191, 0.029, -0.0192, -0.0538, -0.005, -0.0435, 0.0477, -0.0344, -0.1844, 0.2465, 0.02, -0.014, 0.0137, 0.0825, -0.0335, -0.061, 0.1011, -0.0039, -0.0336, 0.1575, 0.0063, 0.0207, 0.0494, 0.018, -0.0354, 0.0293, -0.1018, 0.0932, -0.1445, 0.1547, 0.0049, 0.0552, -0.0438, -0.0178, -0.0603, -0.0367, 0.0044, -0.0518, 0.0132, 0.0255, -0.0641, 0.1019, -0.1094, -0.1376, 0.0305, 0.0507, -0.1747, -0.0077, 0.0993, -0.0008, 0.0333, -0.0107, 0.0062, 0.2749, -0.2, 0.0457, 0.0986, 0.0683, -0.064, -0.2219, -0.0103, 0.0155, 0.0711, 0.0893, -0.0376, -0.0259, 0.0159, 0.0514, 0.033, 0.054, 0.3481, 0.0075, 0.0394, 0.0322, -0.0571, -0.107, -0.0318, 0.0374, -0.0441, 0.0561, 0.0034, -0.1182, -0.1773, 0.0183, -0.0646, -0.2369, 0.0798, -0.1296, -0.0233, -0.0756, 0.029, 0.0196, -0.0783, 0.0319, -0.035, -0.0019, 0.1052, -0.1602, -0.0377, -0.0288, -0.0471, 0.0298, 0.1499, 0.0558, 0.0641, 0.0663, 0.0251, -0.046, -0.1189, -0.1004, 0.1189, 0.0945, 0.0211, -0.0613, -0.0054, -0.1099, -0.076, -0.0626, 0.2131, 0.1156, -0.0522]\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\u001b[A\n","03/28/2021 18:11:50 - INFO - bert_utils -     flaw_f1 = 0.35294117647058826\n","03/28/2021 18:11:50 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:11:50 - INFO - bert_utils -     flaw_recall = 0.21428571428571427\n","03/28/2021 18:11:50 - INFO - bert_utils -     loss = 0.04684960097074509\n","Epoch:  48%|█████████████████▎                  | 12/25 [00:59<01:04,  4.97s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0522, -0.0577, -0.08, -0.0138, -0.0632, 0.0383, -0.0158, 0.1009, 0.1504, 0.0328, 0.0768, -0.0675, -0.0245, 0.0406, -0.0328, -0.0579, -0.017, 0.011, -0.126, 0.1232, -0.0153, 0.0165, 0.105, -0.0045, 0.025, 0.053, -0.005, -0.1389, -0.0299, -0.0116, 0.0492, -0.0364, -0.1012, -0.0191, 0.0373, -0.081, 0.0657, 0.0358, 0.0732, -0.0816, 0.0009, -0.0249, -0.077, 0.1623, -0.1175, -0.0071, -0.038, 0.0305, 0.062, 0.0618, -0.0716, -0.1001, -0.5564, 0.0087, 0.0555, 0.0177, 0.0222, -0.0323, 0.1196, -0.0463, -0.0475, 0.053, -0.0289, -0.0057, 0.047, 0.0076, 0.0207, -0.0438, -0.0163, 0.011, 0.0221, -0.0711, -0.0315, -0.0356, 0.0074, 0.0709, 0.1003, 0.0979, 0.0106, 0.0881, 0.0003, 0.1519, 0.0095, -0.2199, -0.0371, 0.0366, -0.0174, -0.0459, -0.1171, 0.0885, -0.0168, -0.0144, -0.0345, 0.0175, 0.0443, -0.008, -0.0116, 0.0218, -0.0292, -0.0804, -0.1982, 0.017, 0.0099, 0.0297, 0.0073, -0.0415, 0.0753, -0.0301, -0.1676, 0.0376, 0.033, -0.3744, -0.049, -0.0712, 0.0666, -0.0117, 0.0585, -0.0962, -0.0816, -0.2582, -0.0403, -0.1146, -0.0818, 0.1487, -0.1585, 0.2021, -0.0174, 0.1464, -0.1206, 0.0154, -0.0007, -0.0399, 0.0218, -0.1013, 0.0487, -0.2436, 0.0481, -0.0288, 0.0094, 0.0108, -0.0466, -0.0301, 0.0111, 0.2631, 0.1462, 0.0456, 0.1315, -0.1163, -0.1031, -0.014, -0.0452, 0.01, -0.0869, -0.0759, 0.0424, 0.0302, -0.0585, -0.0625, -0.0263, -0.0134, 0.164, 0.1405, -0.0468, -0.0048, -0.3448, 0.0088, 0.0358, 0.0816, -0.0784, -0.0014, -0.0572, -0.0078, 0.0553, 0.0434, 0.3011, -0.0993, 0.2263, -0.1553, 0.0837, 0.0191, 0.029, -0.0192, -0.0538, -0.005, -0.0435, 0.0477, -0.0344, -0.1844, 0.2465, 0.02, -0.014, 0.0137, 0.0825, -0.0335, -0.061, 0.1011, -0.0039, -0.0336, 0.1575, 0.0063, 0.0207, 0.0494, 0.018, -0.0354, 0.0293, -0.1018, 0.0932, -0.1445, 0.1547, 0.0049, 0.0552, -0.0438, -0.0178, -0.0603, -0.0367, 0.0044, -0.0518, 0.0132, 0.0255, -0.0641, 0.1019, -0.1094, -0.1376, 0.0305, 0.0507, -0.1747, -0.0077, 0.0993, -0.0008, 0.0333, -0.0107, 0.0062, 0.2749, -0.2, 0.0457, 0.0986, 0.0683, -0.064, -0.2219, -0.0103, 0.0155, 0.0711, 0.0893, -0.0376, -0.0259, 0.0159, 0.0514, 0.033, 0.054, 0.3481, 0.0075, 0.0394, 0.0322, -0.0571, -0.107, -0.0318, 0.0374, -0.0441, 0.0561, 0.0034, -0.1182, -0.1773, 0.0183, -0.0646, -0.2369, 0.0798, -0.1296, -0.0233, -0.0756, 0.029, 0.0196, -0.0783, 0.0319, -0.035, -0.0019, 0.1052, -0.1602, -0.0377, -0.0288, -0.0471, 0.0298, 0.1499, 0.0558, 0.0641, 0.0663, 0.0251, -0.046, -0.1189, -0.1004, 0.1189, 0.0945, 0.0211, -0.0613, -0.0054, -0.1099, -0.076, -0.0626, 0.2131, 0.1156, -0.0522]\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259]\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0657, 0.046, 0.0251, -0.0332, 0.0141, -0.221, -0.0121, -0.0384, 0.0709, -0.0542, 0.06, 0.0671, 0.0604, 0.1536, -0.0443, 0.0414, 0.0221, 0.0859, -0.0448, -0.0593, -0.0176, -0.036, 0.0366, -0.296, 0.0502, -0.2343, 0.047, -0.0906, 0.0382, -0.2112, 0.0204, -0.0113, 0.2169, -0.0023, 0.0375, 0.0031, 0.0218, 0.0073, -0.0524, 0.0786, 0.0447, -0.0189, -0.1872, 0.0252, -0.0965, -0.0778, -0.1135, -0.0199, 0.0901, 0.0191, -0.0134, -0.001, -0.6779, -0.0296, -0.0169, 0.0048, 0.0055, 0.0217, 0.0459, 0.1198, -0.1142, -0.0131, 0.1316, -0.034, 0.1073, -0.0355, -0.0246, 0.0521, -0.055, 0.0158, 0.03, 0.0168, 0.0335, 0.1986, 0.072, -0.0165, 0.0566, -0.1098, -0.0796, -0.0808, -0.0356, 0.0266, 0.1186, -0.2048, -0.0201, -0.1187, -0.0025, -0.0004, -0.0488, 0.0154, -0.0453, -0.0443, -0.0276, -0.1272, -0.0878, 0.007, 0.0324, -0.0797, -0.0007, -0.0803, -0.1543, -0.01, 0.0158, 0.0556, -0.1504, -0.0414, 0.216, -0.0864, 0.149, -0.0059, 0.0869, 0.2644, -0.023, 0.0089, -0.0612, 0.0043, 0.0118, 0.1146, -0.017, -0.3391, 0.117, 0.0525, 0.0654, 0.188, 0.0733, 0.1355, -0.0028, -0.0488, -0.1325, -0.0084, 0.0245, -0.0142, -0.0509, 0.0929, 0.0061, -0.2864, 0.1034, -0.0612, -0.0258, -0.0505, -0.0298, 0.0362, 0.0213, 0.2663, -0.0181, -0.059, 0.0939, -0.0895, 0.0385, -0.0212, 0.1066, 0.1416, 0.0303, 0.0044, -0.085, -0.0252, -0.0332, 0.0527, -0.032, -0.0311, -0.0576, 0.0001, 0.0041, 0.0065, -0.1985, 0.0077, 0.1188, 0.008, 0.0286, -0.0599, 0.1298, 0.0204, 0.0183, 0.1724, 0.0711, 0.0252, 0.1688, -0.0282, 0.0433, 0.0193, -0.0003, 0.0717, 0.0823, 0.0143, 0.0575, -0.2815, -0.0972, -0.1434, 0.0879, -0.0272, -0.041, 0.0076, 0.0508, 0.0021, 0.0043, 0.0017, -0.0012, -0.0067, 0.1853, 0.0582, 0.0609, -0.0207, 0.0806, 0.0253, 0.0604, 0.0744, -0.1165, -0.0971, -0.0967, 0.0329, 0.0521, 0.0893, 0.0125, -0.0923, -0.0302, 0.014, -0.1939, -0.0287, 0.0101, 0.1531, 0.0165, -0.1121, 0.0018, 0.1513, 0.0316, -0.094, -0.0563, -0.0422, 0.0181, 0.1168, -0.1048, 0.0031, 0.2662, 0.0742, 0.0081, 0.1066, -0.0511, 0.0307, -0.1781, 0.0602, -0.0484, 0.0319, 0.0762, 0.0056, -0.0201, -0.0665, -0.0476, 0.0214, -0.0166, 0.3347, 0.0167, 0.0944, -0.0148, -0.022, -0.2036, 0.0307, -0.1116, -0.0157, 0.0638, 0.017, -0.1067, 0.0272, -0.0475, -0.0247, -0.0591, -0.0323, 0.1388, 0.0192, -0.0207, 0.0467, 0.0291, -0.0942, -0.0741, -0.0043, -0.0332, 0.051, -0.0713, 0.0526, 0.004, -0.0196, -0.1, -0.1013, -0.0433, -0.07, -0.0107, 0.0362, -0.0375, 0.1955, -0.1428, 0.0313, -0.0333, 0.0039, 0.0837, 0.0162, -0.1025, 0.0986, -0.0369, 0.2393, 0.0885, -0.0732]\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.30s/it]\u001b[A\n","03/28/2021 18:11:55 - INFO - bert_utils -     flaw_f1 = 0.6153846153846154\n","03/28/2021 18:11:55 - INFO - bert_utils -     flaw_precision = 0.8\n","03/28/2021 18:11:55 - INFO - bert_utils -     flaw_recall = 0.5\n","03/28/2021 18:11:55 - INFO - bert_utils -     loss = 0.04648766666650772\n","Epoch:  52%|██████████████████▋                 | 13/25 [01:04<00:59,  4.92s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.36s/it]\u001b[A\n","03/28/2021 18:12:00 - INFO - bert_utils -     flaw_f1 = 0.8999999999999999\n","03/28/2021 18:12:00 - INFO - bert_utils -     flaw_precision = 0.84375\n","03/28/2021 18:12:00 - INFO - bert_utils -     flaw_recall = 0.9642857142857143\n","03/28/2021 18:12:00 - INFO - bert_utils -     loss = 0.057157643139362335\n","Epoch:  56%|████████████████████▏               | 14/25 [01:09<00:54,  4.92s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0986, -0.2203, 0.1277, -0.0038, 0.1349, 0.1766, -0.0717, 0.0962, 0.0286, 0.1331, -0.088, 0.0097, 0.2899, -0.0971, 0.1511, -0.1695, -0.0228, 0.0897, 0.0779, 0.0124, -0.2479, 0.1565, -0.059, -0.1955, 0.1632, -0.0734, -0.1309, 0.1795, -0.0663, -0.2319, 0.0335, 0.0467, 0.0133, 0.1546, 0.3338, 0.3256, -0.0399, 0.1533, 0.0351, 0.0008, 0.0269, 0.1504, -0.1719, 0.003, -0.0687, -0.0554, 0.2062, -0.2141, -0.1168, -0.265, 0.1165, -0.1076, -0.7506, -0.0155, -0.0613, -0.1515, 0.0801, 0.021, 0.142, 0.0209, 0.0878, -0.3437, -0.0167, 0.1133, -0.0731, -0.1679, 0.1667, -0.0529, 0.1493, 0.1807, 0.0853, -0.0441, 0.097, 0.1113, -0.2308, -0.0581, -0.034, 0.1075, -0.0655, 0.2295, 0.1005, 0.0773, 0.0227, -0.1905, 0.0464, 0.0436, 0.1483, 0.0768, 0.2949, 0.1511, 0.1139, -0.0214, -0.0618, 0.031, -0.1978, -0.0764, 0.0518, -0.0625, 0.1776, -0.1261, -0.1297, 0.0028, -0.2233, 0.0226, 0.0023, 0.0425, -0.0286, -0.0679, 0.0191, 0.0678, 0.0301, 0.142, 0.1366, 0.0366, -0.0746, -0.0858, 0.1336, -0.0738, 0.2338, -0.3484, 0.2, 0.1991, 0.1148, -0.0628, 0.1148, 0.1448, 0.0919, -0.2645, 0.097, 0.1204, -0.0148, 0.0898, -0.01, -0.1413, -0.034, 0.1518, 0.1655, -0.1381, 0.033, 0.0163, 0.0248, -0.1674, 0.0202, 0.1834, -0.1282, -0.065, 0.1216, 0.0506, -0.2409, 0.1017, -0.1693, 0.1242, 0.1661, 0.0651, -0.1442, 0.1766, 0.055, -0.0391, 0.0088, -0.0457, -0.1151, -0.1501, -0.0316, 0.1901, 0.1254, -0.1613, 0.0873, -0.0148, -0.2089, 0.2145, 0.0062, -0.0076, 0.0744, -0.1763, 0.2016, 0.162, 0.2642, -0.118, -0.1484, 0.0179, -0.0896, -0.0677, -0.023, -0.263, 0.0056, -0.036, 0.1879, -0.0834, -0.2536, 0.0418, -0.0651, 0.0167, 0.1317, 0.0047, -0.0016, -0.1334, -0.1382, 0.0937, 0.1303, 0.0645, 0.0534, -0.0298, 0.0976, 0.1149, 0.0646, 0.1398, 0.1059, -0.2002, -0.004, 0.0898, 0.0947, -0.0584, -0.0236, 0.0199, -0.0228, 0.0102, -0.0002, 0.0256, -0.2023, 0.0615, 0.0891, -0.0925, -0.0225, -0.0586, 0.0243, -0.1384, 0.1393, -0.0206, -0.085, -0.1646, -0.1177, -0.1381, 0.2971, -0.2867, -0.1304, -0.2274, -0.0408, -0.0374, -0.2126, -0.078, 0.0264, -0.0744, -0.0357, 0.1726, 0.0159, 0.1339, 0.1118, -0.0977, 0.0761, 0.2989, 0.0438, 0.0818, 0.0813, 0.093, 0.1717, -0.0657, -0.0647, 0.0602, -0.0034, 0.1075, 0.0793, -0.0193, 0.0666, 0.0418, -0.5082, 0.232, 0.0267, 0.0283, -0.2717, -0.1061, -0.1167, 0.1112, 0.1064, -0.1985, -0.2026, 0.13, -0.054, 0.0633, 0.1677, -0.2039, 0.1521, 0.2566, -0.0923, 0.0955, -0.0315, 0.2171, -0.0028, -0.1608, 0.0689, -0.0604, 0.0939, 0.0334, 0.0282, -0.0633, -0.0004, -0.0849, 0.0033, -0.0475, 0.1096, 0.097]\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0177, -0.0273, -0.0135, 0.0351, -0.0135, -0.0357, -0.0058, -0.0239, 0.0311, -0.015, 0.046, 0.0713, 0.0199, 0.1177, -0.0441, 0.0061, 0.0334, 0.015, -0.1213, 0.0951, -0.0248, 0.0579, 0.0164, -0.0854, -0.0057, -0.2043, -0.0105, 0.0987, 0.0102, 0.022, -0.0094, -0.0339, 0.1424, -0.0115, -0.0111, 0.0551, 0.0272, -0.0901, 0.049, -0.0315, 0.0028, -0.0097, 0.0617, 0.0173, -0.1601, 0.1213, 0.0135, 0.0359, -0.0365, 0.0771, -0.0066, -0.0964, -0.5744, 0.0209, 0.0193, 0.0463, -0.0365, -0.0268, 0.1612, 0.0241, -0.0049, 0.0039, 0.0862, 0.0059, -0.1359, 0.0722, 0.0101, 0.0269, 0.0008, 0.0695, -0.0395, 0.0234, 0.0017, -0.0012, 0.0806, 0.0108, 0.0248, -0.0638, -0.0591, 0.0374, 0.0009, -0.0823, -0.0843, -0.2126, 0.0201, 0.0672, -0.0022, -0.0099, 0.0751, 0.0265, 0.0394, -0.0169, 0.034, 0.029, 0.0211, 0.0267, 0.0472, -0.0088, -0.0015, 0.0405, -0.1484, -0.025, 0.0326, -0.0136, -0.1963, -0.0048, 0.1806, -0.0943, 0.2022, 0.0232, 0.0321, -0.0148, 0.006, 0.0183, 0.0258, -0.0937, -0.0035, -0.0076, 0.0148, -0.1691, 0.0519, -0.0441, -0.0046, 0.1673, -0.0117, 0.16, 0.0364, -0.0148, 0.0751, 0.019, -0.083, 0.0424, 0.1003, -0.0553, 0.0296, -0.0735, -0.0384, 0.0942, -0.0709, 0.0028, -0.04, -0.0545, 0.0755, 0.2531, 0.0444, -0.0242, -0.0157, -0.0609, 0.0407, 0.0239, 0.0153, -0.0288, -0.0555, -0.0653, -0.0181, 0.0356, -0.059, -0.0639, -0.0358, -0.0805, -0.0492, -0.0448, 0.0934, 0.0278, -0.4626, -0.0006, 0.0442, -0.0443, 0.0646, 0.0288, -0.1348, -0.0047, -0.0405, -0.0361, 0.0506, -0.0172, 0.1548, -0.4293, 0.0389, 0.0129, -0.0289, -0.0432, -0.0958, -0.019, 0.0415, -0.0265, 0.0261, -0.0644, 0.3606, 0.0125, 0.015, 0.1054, 0.0149, 0.0168, -0.1102, 0.0427, -0.0326, -0.0481, 0.1979, 0.0206, -0.0647, 0.032, -0.0018, -0.036, 0.1749, 0.0343, 0.0379, -0.0489, -0.0602, 0.0921, -0.0934, 0.0158, -0.0161, 0.0573, 0.0169, -0.0587, -0.073, 0.0644, 0.0019, -0.0086, -0.013, -0.0641, 0.1164, 0.018, 0.0429, -0.0438, -0.0018, 0.052, 0.0021, 0.0613, 0.0506, 0.0133, 0.1732, 0.1022, -0.0072, 0.102, -0.0276, 0.085, -0.1872, -0.0474, 0.0236, -0.0105, 0.0169, -0.0141, -0.0175, -0.0766, 0.0296, -0.0054, -0.0303, 0.3495, 0.0092, 0.1807, -0.0925, 0.0223, -0.2088, 0.0025, 0.0295, -0.0186, 0.0535, 0.0086, -0.2061, 0.0099, 0.015, 0.0017, -0.1505, -0.0323, 0.0321, 0.0308, 0.0344, -0.007, -0.0379, 0.0385, -0.0204, 0.0254, -0.0234, -0.0235, 0.0157, 0.0321, 0.0237, 0.036, -0.1336, -0.1436, 0.1274, -0.0019, 0.044, -0.0512, -0.0246, -0.0182, -0.0126, 0.0601, 0.0466, 0.0236, -0.0276, -0.0362, -0.0115, -0.1296, 0.0037, 0.1877, 0.0338, -0.0671]\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0568, -0.1339, 0.1573, 0.0057, 0.194, -0.2087, 0.0217, 0.0051, -0.0037, 0.0886, -0.025, -0.1547, -0.0509, 0.0492, 0.1833, -0.0597, 0.0121, 0.0852, 0.256, 0.0516, 0.0196, 0.1505, 0.0116, -0.0267, -0.037, -0.1625, 0.0446, -0.256, 0.0354, -0.0035, 0.1073, 0.0501, -0.0761, -0.0933, 0.0849, 0.0552, 0.0571, -0.3266, -0.0249, -0.0003, 0.1035, -0.0916, 0.1449, -0.1644, 0.1403, -0.1167, 0.0611, 0.0411, -0.0707, 0.0029, -0.0496, 0.0168, -0.7128, -0.0906, 0.1049, -0.0882, -0.0613, 0.0491, 0.1263, 0.0096, 0.0423, -0.0286, 0.1267, -0.0772, -0.0205, -0.0492, 0.0136, 0.0094, -0.143, 0.0167, -0.0316, -0.0583, -0.0147, 0.1385, -0.0844, -0.0134, 0.1329, 0.0116, -0.2098, 0.2224, 0.002, 0.1558, 0.0542, -0.174, 0.0897, -0.032, 0.05, 0.006, -0.2922, -0.1066, 0.0215, 0.0971, -0.0657, 0.0037, 0.0508, -0.174, -0.0422, -0.0416, 0.0016, -0.0965, -0.2728, -0.1393, 0.0569, -0.0341, -0.0352, -0.0207, 0.2142, 0.079, -0.209, -0.0819, -0.0115, -0.1229, 0.0603, -0.13, -0.0065, 0.0507, 0.0453, -0.0568, 0.0429, -0.2566, 0.0585, -0.0201, -0.1377, 0.0948, -0.4483, 0.2669, -0.1096, -0.0509, -0.2165, 0.0468, -0.1828, 0.1138, 0.0407, 0.042, -0.021, -0.2902, 0.0045, 0.0677, 0.121, -0.0588, 0.0427, 0.033, 0.0429, 0.3968, -0.0114, -0.0931, 0.1555, -0.0371, -0.0992, -0.0378, -0.0278, -0.1709, -0.1218, -0.0116, 0.073, 0.0756, -0.1181, -0.0068, -0.0803, 0.0444, 0.0408, 0.0389, -0.0052, 0.0638, -0.3224, 0.1537, 0.1595, 0.004, 0.0665, 0.0536, -0.0066, -0.0646, 0.0127, 0.0308, 0.1717, -0.0062, 0.3631, -0.3776, 0.0954, -0.0606, 0.1313, -0.0308, -0.0911, 0.079, -0.0056, 0.2828, 0.0027, -0.1449, 0.0889, -0.0392, 0.0279, 0.0361, -0.1109, 0.0228, -0.0363, 0.049, -0.0457, -0.1489, 0.3391, 0.0055, 0.0712, 0.0943, 0.0467, 0.0785, -0.0447, -0.0143, 0.0106, -0.1578, -0.2198, 0.0843, 0.074, -0.0401, -0.0525, 0.027, 0.0177, -0.0929, -0.0504, -0.0542, -0.0809, 0.052, 0.1789, 0.1612, 0.0008, 0.0301, 0.0396, -0.346, -0.0021, 0.0136, 0.049, -0.0123, -0.0853, 0.0537, 0.3109, -0.0686, 0.151, 0.0611, 0.0794, -0.1792, -0.2695, -0.0054, -0.0379, -0.0062, 0.0115, 0.006, 0.0348, 0.0524, -0.0755, 0.1379, 0.107, 0.4634, -0.0354, -0.0732, -0.0586, 0.0211, 0.238, -0.0239, 0.1504, 0.0316, -0.0489, 0.1382, 0.0628, 0.0233, -0.1273, -0.0893, -0.4087, -0.0572, 0.1639, -0.0343, -0.1047, 0.215, -0.0951, -0.0839, -0.0705, -0.0843, 0.0309, 0.0536, -0.0857, -0.0801, 0.0284, 0.0525, -0.013, -0.0412, 0.0942, -0.114, 0.0839, 0.0898, 0.1567, -0.024, 0.065, 0.0236, -0.0525, -0.0036, -0.003, -0.0604, 0.0575, 0.1172, -0.0517, 0.1603, 0.0341, 0.033]\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\u001b[A\n","03/28/2021 18:12:05 - INFO - bert_utils -     flaw_f1 = 0.6976744186046512\n","03/28/2021 18:12:05 - INFO - bert_utils -     flaw_precision = 0.8333333333333334\n","03/28/2021 18:12:05 - INFO - bert_utils -     flaw_recall = 0.6\n","03/28/2021 18:12:05 - INFO - bert_utils -     loss = 0.058999400585889816\n","Epoch:  60%|█████████████████████▌              | 15/25 [01:14<00:49,  4.92s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0.0109, 0.0054, 0.0025, -0.063, -0.0564, 0.0296, -0.0682, 0.0355, -0.0558, 0.0059, -0.0383, 0.0515, 0.0712, -0.0886, 0.1198, -0.0334, 0.0043, 0.0388, 0.0457, -0.0428, 0.0495, -0.0072, 0.0304, -0.0174, 0.0421, 0.0032, -0.039, -0.0322, -0.0113, 0.0603, 0.0074, 0.0102, 0.0481, -0.0294, 0.0271, 0.005, -0.0374, 0.0533, -0.1492, 0.0262, 0.0029, -0.0423, 0.0086, -0.0379, -0.0297, -0.0117, 0.0891, -0.5834, -0.0196, 0.038, 0.0106, -0.023, -0.028, 0.0464, 0.009, -0.0389, -0.033, -0.1084, 0.0001, 0.0224, 0.1006, 0.0005, 0.0152, -0.0146, -0.0575, -0.0436, -0.0181, -0.0158, -0.0427, 0.0685, -0.0116, 0.029, 0.0035, -0.1194, -0.0918, 0.0197, 0.1237, -0.0245, -0.3306, -0.0037, -0.0307, -0.025, 0.0059, 0.1157, -0.0114, -0.0106, -0.0137, 0.0109, 0.0819, 0.052, 0.0564, 0.0283, -0.0403, 0.0134, -0.0875, -0.0785, -0.0019, 0.0188, 0.0821, -0.095, -0.0039, 0.0945, -0.0765, 0.1313, -0.0581, 0.0405, -0.0103, 0.0653, 0.0289, 0.0409, -0.0256, -0.0071, 0.0012, -0.0567, -0.2729, 0.0702, -0.0213, 0.0021, 0.1257, 0.0059, 0.0772, 0.0403, -0.0276, -0.101, -0.0033, -0.0749, -0.0207, -0.0789, 0.0385, -0.0658, -0.1691, -0.0115, -0.0056, 0.1989, 0.1064, -0.0167, 0.0653, 0.0628, 0.3884, -0.0158, -0.0042, 0.0209, -0.0511, -0.0213, -0.0188, -0.009, 0.0099, -0.023, -0.0427, 0.0403, 0.0124, -0.0669, -0.0269, -0.0124, 0.0256, -0.041, -0.0619, 0.0162, -0.1176, -0.2902, 0.0084, 0.1346, -0.0781, 0.0389, -0.0321, -0.0553, -0.0213, -0.0142, 0.0333, 0.0863, -0.1304, 0.1478, -0.5894, 0.0465, 0.0001, 0.0436, -0.0236, 0.0123, 0.0241, 0.02, -0.2658, -0.0006, -0.0608, 0.3386, -0.0067, 0.0549, 0.0206, -0.0162, 0.0078, -0.0411, 0.0081, 0.0067, -0.0451, 0.1383, 0.0427, -0.0006, -0.0881, 0.0111, 0.0129, -0.0609, -0.0975, -0.0001, 0.1469, 0.0451, 0.0174, -0.0055, -0.0166, -0.0333, 0.0994, 0.055, -0.0201, -0.333, -0.0084, 0.155, -0.0631, 0.0423, 0.0196, -0.0049, 0.0698, -0.0575, -0.1492, 0.026, 0.0842, -0.0507, 0.1491, 0.039, 0.0039, 0.2166, -0.0059, 0.013, -0.0529, -0.0177, 0.1542, -0.1744, 0.0889, -0.074, -0.0335, 0.0167, -0.0347, 0.0433, -0.064, 0.0335, -0.0394, 0.0444, 0.3293, -0.0081, -0.0297, 0.0637, 0.0484, -0.3762, 0.0191, -0.0618, 0.0386, 0.0234, 0.0003, 0.0235, 0.033, 0.0034, -0.0156, -0.1348, 0.0016, 0.0838, -0.0008, -0.0768, -0.0085, -0.0156, 0.0373, -0.0186, 0.0309, 0.0197, -0.0242, 0.042, 0.0556, 0.0171, 0.0195, -0.1021, 0.0454, -0.031, -0.0079, 0.0583, -0.0137, -0.0559, -0.0094, 0.0233, 0.04, 0.0042, -0.0218, -0.0945, -0.0154, -0.029, -0.0141, 0.008, 0.1221, 0.0558, -0.1314]\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0568, -0.1339, 0.1573, 0.0057, 0.194, -0.2087, 0.0217, 0.0051, -0.0037, 0.0886, -0.025, -0.1547, -0.0509, 0.0492, 0.1833, -0.0597, 0.0121, 0.0852, 0.256, 0.0516, 0.0196, 0.1505, 0.0116, -0.0267, -0.037, -0.1625, 0.0446, -0.256, 0.0354, -0.0035, 0.1073, 0.0501, -0.0761, -0.0933, 0.0849, 0.0552, 0.0571, -0.3266, -0.0249, -0.0003, 0.1035, -0.0916, 0.1449, -0.1644, 0.1403, -0.1167, 0.0611, 0.0411, -0.0707, 0.0029, -0.0496, 0.0168, -0.7128, -0.0906, 0.1049, -0.0882, -0.0613, 0.0491, 0.1263, 0.0096, 0.0423, -0.0286, 0.1267, -0.0772, -0.0205, -0.0492, 0.0136, 0.0094, -0.143, 0.0167, -0.0316, -0.0583, -0.0147, 0.1385, -0.0844, -0.0134, 0.1329, 0.0116, -0.2098, 0.2224, 0.002, 0.1558, 0.0542, -0.174, 0.0897, -0.032, 0.05, 0.006, -0.2922, -0.1066, 0.0215, 0.0971, -0.0657, 0.0037, 0.0508, -0.174, -0.0422, -0.0416, 0.0016, -0.0965, -0.2728, -0.1393, 0.0569, -0.0341, -0.0352, -0.0207, 0.2142, 0.079, -0.209, -0.0819, -0.0115, -0.1229, 0.0603, -0.13, -0.0065, 0.0507, 0.0453, -0.0568, 0.0429, -0.2566, 0.0585, -0.0201, -0.1377, 0.0948, -0.4483, 0.2669, -0.1096, -0.0509, -0.2165, 0.0468, -0.1828, 0.1138, 0.0407, 0.042, -0.021, -0.2902, 0.0045, 0.0677, 0.121, -0.0588, 0.0427, 0.033, 0.0429, 0.3968, -0.0114, -0.0931, 0.1555, -0.0371, -0.0992, -0.0378, -0.0278, -0.1709, -0.1218, -0.0116, 0.073, 0.0756, -0.1181, -0.0068, -0.0803, 0.0444, 0.0408, 0.0389, -0.0052, 0.0638, -0.3224, 0.1537, 0.1595, 0.004, 0.0665, 0.0536, -0.0066, -0.0646, 0.0127, 0.0308, 0.1717, -0.0062, 0.3631, -0.3776, 0.0954, -0.0606, 0.1313, -0.0308, -0.0911, 0.079, -0.0056, 0.2828, 0.0027, -0.1449, 0.0889, -0.0392, 0.0279, 0.0361, -0.1109, 0.0228, -0.0363, 0.049, -0.0457, -0.1489, 0.3391, 0.0055, 0.0712, 0.0943, 0.0467, 0.0785, -0.0447, -0.0143, 0.0106, -0.1578, -0.2198, 0.0843, 0.074, -0.0401, -0.0525, 0.027, 0.0177, -0.0929, -0.0504, -0.0542, -0.0809, 0.052, 0.1789, 0.1612, 0.0008, 0.0301, 0.0396, -0.346, -0.0021, 0.0136, 0.049, -0.0123, -0.0853, 0.0537, 0.3109, -0.0686, 0.151, 0.0611, 0.0794, -0.1792, -0.2695, -0.0054, -0.0379, -0.0062, 0.0115, 0.006, 0.0348, 0.0524, -0.0755, 0.1379, 0.107, 0.4634, -0.0354, -0.0732, -0.0586, 0.0211, 0.238, -0.0239, 0.1504, 0.0316, -0.0489, 0.1382, 0.0628, 0.0233, -0.1273, -0.0893, -0.4087, -0.0572, 0.1639, -0.0343, -0.1047, 0.215, -0.0951, -0.0839, -0.0705, -0.0843, 0.0309, 0.0536, -0.0857, -0.0801, 0.0284, 0.0525, -0.013, -0.0412, 0.0942, -0.114, 0.0839, 0.0898, 0.1567, -0.024, 0.065, 0.0236, -0.0525, -0.0036, -0.003, -0.0604, 0.0575, 0.1172, -0.0517, 0.1603, 0.0341, 0.033]\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\u001b[A\n","03/28/2021 18:12:10 - INFO - bert_utils -     flaw_f1 = 0.9019607843137256\n","03/28/2021 18:12:10 - INFO - bert_utils -     flaw_precision = 0.92\n","03/28/2021 18:12:10 - INFO - bert_utils -     flaw_recall = 0.8846153846153846\n","03/28/2021 18:12:10 - INFO - bert_utils -     loss = 0.038466230034828186\n","Epoch:  64%|███████████████████████             | 16/25 [01:19<00:45,  5.01s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.42s/it]\u001b[A\n","03/28/2021 18:12:15 - INFO - bert_utils -     flaw_f1 = 0.9032258064516129\n","03/28/2021 18:12:15 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:15 - INFO - bert_utils -     flaw_recall = 0.8235294117647058\n","03/28/2021 18:12:15 - INFO - bert_utils -     loss = 0.030073976144194603\n","Epoch:  68%|████████████████████████▍           | 17/25 [01:24<00:39,  4.99s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0977, -0.0046, 0.1678, -0.2366, 0.096, -0.0685, -0.1005, 0.0104, -0.2248, -0.1104, -0.0225, 0.1047, 0.1715, -0.0981, -0.1278, -0.172, -0.1464, 0.0413, 0.2217, -0.1233, 0.0083, -0.0001, -0.0404, -0.1211, -0.0675, -0.0415, 0.1668, -0.0525, -0.0221, 0.033, -0.0224, -0.0258, 0.1263, 0.0212, -0.0878, -0.0212, 0.0781, -0.0119, 0.0874, 0.0193, -0.024, 0.1151, 0.0286, -0.0167, 0.0453, -0.1218, 0.0032, -0.0825, -0.1115, 0.1014, -0.0371, 0.1974, -0.7815, -0.0672, 0.0294, 0.0052, 0.0035, -0.0847, 0.0061, -0.0752, -0.0684, 0.0673, -0.1874, 0.0757, -0.4387, -0.0174, 0.0809, -0.02, 0.0018, 0.1324, -0.0337, -0.0411, 0.17, 0.3153, 0.0194, -0.0272, 0.1141, 0.0941, 0.1972, 0.2384, -0.0078, 0.0131, 0.0745, -0.2424, 0.0265, 0.0405, -0.1425, 0.0354, -0.2446, -0.0562, 0.1364, -0.0204, -0.046, 0.1489, 0.0747, 0.0372, 0.0702, -0.0465, -0.1113, -0.2096, -0.1664, -0.0771, 0.0402, 0.0347, -0.3003, -0.0309, 0.151, -0.1861, 0.1091, -0.2461, -0.0207, 0.1095, 0.021, -0.0239, -0.1748, -0.0801, -0.0092, 0.1564, -0.0733, -0.3962, 0.0037, -0.1287, 0.022, 0.1114, -0.1632, 0.1876, 0.0471, 0.0613, 0.0402, 0.0276, 0.1795, 0.2471, -0.0839, 0.0321, 0.0411, -0.0149, 0.0026, 0.2144, 0.0196, 0.0138, 0.0221, 0.0262, 0.0405, 0.2699, 0.1447, -0.1308, 0.1037, 0.159, 0.0208, -0.0643, -0.015, 0.0768, 0.0106, 0.0179, 0.062, 0.2155, 0.0385, -0.0043, -0.1243, -0.0494, -0.1242, -0.0394, -0.0041, 0.0435, 0.4762, -0.0185, 0.1576, 0.0315, -0.0019, -0.0857, 0.1446, 0.0885, 0.0451, -0.0665, 0.0509, 0.0814, 0.1838, -0.1182, -0.0709, 0.1544, 0.0081, -0.0903, -0.0498, -0.1137, -0.0396, 0.2853, 0.132, -0.1006, 0.3388, -0.077, 0.1302, 0.1694, -0.0511, 0.0214, 0.015, 0.0737, 0.0304, -0.0922, 0.215, -0.1299, 0.0955, 0.0302, -0.1777, -0.0824, 0.0342, 0.0227, 0.0413, -0.0219, -0.2044, 0.0451, -0.0646, -0.2535, 0.0122, -0.1319, 0.0349, 0.0311, -0.1455, 0.0431, -0.1364, -0.0079, 0.0205, -0.1821, 0.0237, -0.028, 0.1851, -0.1572, 0.0122, 0.0584, 0.0441, -0.0077, -0.1109, 0.039, 0.2915, -0.0956, 0.0431, -0.0056, 0.0395, 0.0331, -0.2264, -0.1199, -0.1122, 0.0995, 0.0369, -0.2666, -0.0484, -0.0905, 0.0507, -0.0015, 0.0612, 0.3071, -0.0275, 0.1221, -0.0302, -0.0878, 0.0682, -0.0117, 0.1408, 0.0152, 0.1823, 0.0063, 0.0173, 0.1291, -0.1599, -0.0053, 0.232, -0.0023, -0.045, -0.0407, -0.0718, 0.0484, -0.0575, 0.0106, 0.105, -0.0859, 0.1308, -0.0453, 0.0331, -0.0852, 0.0224, -0.0585, 0.0289, 0.0816, 0.0636, -0.073, 0.0029, -0.0036, -0.026, 0.1314, -0.1188, -0.0969, 0.0498, 0.1172, -0.0345, -0.0861, 0.0217, 0.0288, -0.0867, 0.2249, 0.0551, 0.1144]\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0437, -0.0144, -0.1032, -0.0202, -0.0252, 0.019, 0.0131, -0.0011, -0.0168, -0.0128, -0.0008, 0.0121, -0.0426, 0.0192, 0.0158, -0.1123, 0.0189, 0.0492, 0.1644, -0.0394, -0.0248, 0.0215, -0.0129, -0.0496, 0.005, 0.0421, -0.0329, -0.0741, -0.1048, -0.0552, 0.0227, -0.0345, -0.0368, -0.0959, -0.0059, 0.0261, -0.0136, 0.0411, -0.0187, 0.0081, -0.0292, -0.0747, -0.0101, -0.0148, 0.0238, 0.0097, -0.0126, 0.0149, -0.0238, -0.0384, 0.0375, 0.071, -0.5428, -0.0469, 0.0233, 0.0216, 0.0095, 0.0793, -0.043, 0.0157, 0.0183, -0.0651, -0.0323, -0.0266, 0.0534, -0.0686, 0.0119, -0.0291, -0.0892, -0.0047, 0.0317, 0.0145, 0.024, -0.0603, -0.0286, 0.0417, -0.0266, -0.0207, -0.0179, 0.0923, -0.0155, -0.0167, 0.0018, -0.167, -0.0138, -0.0397, 0.004, 0.069, 0.2466, 0.0105, -0.029, -0.0633, 0.0889, -0.0542, -0.0125, 0.0102, -0.0743, -0.0065, -0.0179, 0.0513, -0.0914, -0.0053, -0.0592, 0.0338, -0.1552, 0.002, -0.0752, 0.0243, -0.0747, -0.0947, -0.071, -0.1083, -0.0362, 0.0888, -0.007, -0.0735, 0.0336, -0.0083, 0.0342, -0.3091, -0.0721, -0.0342, -0.0242, -0.2222, -0.0489, 0.137, -0.0865, 0.0516, 0.03, -0.0035, 0.0154, -0.0025, -0.0195, -0.0189, -0.0473, -0.1336, -0.001, -0.0033, -0.0832, 0.0505, -0.0475, -0.0468, 0.0717, 0.1783, 0.0927, 0.0197, 0.0592, -0.0596, 0.0006, 0.0368, 0.0973, 0.0478, -0.021, -0.0317, -0.0101, 0.0674, 0.0452, 0.0379, -0.0198, -0.0358, -0.0723, -0.0105, -0.0479, 0.0109, -0.1331, -0.0287, 0.0535, -0.0281, -0.0316, -0.0065, -0.0265, 0.0235, 0.0284, 0.0521, 0.0836, -0.1519, 0.1407, -0.2958, 0.0723, -0.0243, 0.0768, -0.0225, 0.0749, 0.0447, 0.0244, 0.1271, -0.0649, 0.0196, 0.4245, 0.0026, 0.0269, 0.0098, -0.0427, 0.0266, 0.0082, -0.0294, -0.0156, -0.0197, 0.2449, -0.0039, 0.0266, -0.2229, -0.0209, -0.0469, 0.0384, 0.0065, 0.0036, 0.0006, -0.0314, 0.0044, 0.0778, 0.0198, -0.0378, -0.0136, 0.0212, -0.0012, 0.0102, 0.0594, 0.0661, -0.0734, 0.0559, 0.0097, 0.0514, 0.019, 0.025, -0.0928, 0.045, 0.005, -0.0685, -0.114, 0.1314, 0.0066, 0.2853, -0.0352, 0.0347, -0.365, -0.0628, 0.1728, -0.2572, 0.0756, -0.0777, 0.027, 0.0623, -0.0096, 0.0195, -0.0214, 0.0187, -0.0461, -0.0247, 0.3452, 0.0195, -0.0091, -0.0981, 0.0356, -0.0193, 0.0092, -0.0804, -0.0073, 0.0464, 0.0069, 0.0712, -0.0265, 0.0014, -0.0015, -0.3931, -0.0186, -0.0188, -0.009, -0.0198, -0.031, -0.051, -0.0472, 0.0292, 0.0231, -0.035, -0.005, 0.0204, 0.0888, -0.0003, 0.0259, 0.036, -0.2131, 0.0651, -0.0641, -0.0229, 0.0442, -0.0742, 0.0325, -0.0619, 0.0497, 0.0603, 0.0174, -0.0593, 0.0226, -0.0546, 0.013, 0.0049, 0.0448, -0.0326, -0.013]\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.27s/it]\u001b[A\n","03/28/2021 18:12:20 - INFO - bert_utils -     flaw_f1 = 0.7741935483870968\n","03/28/2021 18:12:20 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:20 - INFO - bert_utils -     flaw_recall = 0.631578947368421\n","03/28/2021 18:12:20 - INFO - bert_utils -     loss = 0.031606525182724\n","Epoch:  72%|█████████████████████████▉          | 18/25 [01:29<00:34,  4.97s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.1138, -0.0086, -0.0358, -0.1292, -0.1042, 0.2159, -0.0948, 0.0822, -0.0946, -0.171, -0.0546, 0.0883, 0.0276, -0.0162, -0.0555, -0.0449, 0.0677, -0.0055, 0.032, 0.0333, -0.0292, -0.0298, -0.0027, -0.0719, -0.0803, -0.0633, 0.08, -0.1627, -0.0032, 0.0804, -0.0792, -0.0398, 0.0383, -0.0989, 0.0301, 0.0452, 0.0522, 0.017, 0.0502, -0.1751, -0.157, 0.0337, -0.0665, -0.0323, -0.2169, 0.0254, -0.0678, -0.0346, 0.172, 0.0173, -0.1511, 0.2079, -0.6075, -0.0328, 0.0448, 0.0231, 0.055, -0.0562, -0.1088, 0.055, -0.0864, 0.0499, 0.0773, -0.0079, -0.1727, 0.1625, 0.0592, -0.0056, 0.0117, -0.0138, 0.0838, 0.0398, 0.1314, 0.0249, 0.0877, -0.0444, -0.0031, 0.087, 0.1316, 0.1192, 0.012, -0.0509, -0.071, -0.1927, 0.1316, 0.0606, -0.0168, 0.0333, -0.3717, -0.0845, 0.113, -0.107, -0.0638, -0.0904, -0.0427, -0.0332, 0.0154, 0.0617, -0.0326, -0.0663, -0.2623, 0.0032, 0.1143, -0.0267, -0.0092, 0.0015, 0.1084, 0.0445, -0.1566, -0.2638, 0.0556, -0.0351, -0.1247, -0.1002, -0.0026, 0.0088, 0.0066, -0.0078, 0.0688, -0.3622, -0.0004, 0.007, 0.0154, 0.0362, -0.0629, 0.2735, -0.06, -0.1482, -0.2061, -0.0985, -0.0046, -0.0336, -0.0839, -0.1252, 0.0349, 0.2803, -0.0787, -0.0596, 0.1422, -0.0371, -0.001, 0.0921, 0.0639, 0.3127, -0.0255, -0.092, 0.0804, 0.1018, 0.0166, -0.0152, -0.1705, -0.1767, 0.0526, -0.0765, 0.0071, 0.0952, 0.0343, 0.0264, -0.0458, 0.0504, -0.0682, 0.0182, -0.1167, -0.0675, -0.5676, -0.0136, 0.1074, 0.0511, -0.0001, -0.0653, 0.4411, 0.0548, 0.0549, -0.1922, 0.1494, -0.026, 0.2688, -0.1489, -0.0553, -0.0184, 0.168, -0.0901, -0.0618, 0.0372, -0.0696, -0.0856, 0.1078, -0.0344, 0.3902, 0.1063, 0.055, 0.0355, 0.0267, 0.0182, -0.1027, -0.025, 0.2072, -0.0574, 0.2542, 0.0828, 0.0413, 0.1961, 0.0883, -0.0486, -0.0859, 0.1809, -0.0025, -0.0874, -0.2206, 0.0862, 0.0158, 0.0037, 0.0219, 0.0988, -0.0611, -0.0771, -0.1614, -0.025, 0.0336, 0.0398, -0.0367, -0.0249, 0.0613, 0.0209, 0.0581, -0.1388, -0.016, 0.1251, -0.0173, 0.127, 0.0165, -0.0759, 0.3781, -0.0017, 0.0432, -0.1666, -0.0013, -0.0575, -0.2927, 0.1561, 0.0846, -0.0398, 0.0959, 0.0397, -0.07, -0.0758, 0.0061, 0.037, -0.0062, 0.361, -0.0637, -0.076, -0.0469, -0.0494, 0.2322, -0.0612, 0.1924, -0.0631, 0.0397, -0.1119, -0.0904, -0.006, -0.0383, -0.0325, -0.0527, -0.0844, 0.1913, 0.0233, -0.1868, 0.0358, -0.0443, -0.0688, -0.0387, -0.1202, -0.0492, 0.031, -0.0585, 0.0265, -0.0176, -0.0295, 0.1128, 0.0698, 0.07, 0.0197, 0.0228, 0.0289, -0.0864, -0.0787, -0.1648, 0.0724, -0.0009, -0.0348, 0.0142, 0.0709, 0.0269, 0.0954, -0.0761, 0.18, 0.0421, -0.0483]\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.27s/it]\u001b[A\n","03/28/2021 18:12:25 - INFO - bert_utils -     flaw_f1 = 0.7586206896551725\n","03/28/2021 18:12:25 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:25 - INFO - bert_utils -     flaw_recall = 0.6111111111111112\n","03/28/2021 18:12:25 - INFO - bert_utils -     loss = 0.03294838219881058\n","Epoch:  76%|███████████████████████████▎        | 19/25 [01:34<00:29,  4.96s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.1208, 0.0144, -0.0927, -0.0181, 0.1125, 0.0495, -0.0467, -0.0193, 0.0746, -0.0119, 0.0817, 0.0189, 0.0596, 0.0681, -0.0196, 0.1208, -0.0401, -0.0571, -0.1717, 0.1402, -0.0256, 0.0317, 0.0454, 0.1437, -0.0175, 0.0, 0.0317, 0.201, 0.0216, -0.0175, 0.0565, -0.0301, 0.0244, -0.045, 0.0697, -0.031, 0.0226, -0.1532, -0.0043, -0.0578, 0.0305, -0.0136, 0.0062, -0.0937, -0.0747, 0.0217, -0.0568, 0.0469, 0.0378, -0.0502, -0.1851, 0.1019, -0.5416, -0.0081, 0.0269, 0.0144, 0.0698, -0.1973, -0.1727, 0.0096, 0.0363, 0.0037, -0.0497, 0.0526, -0.0885, -0.0309, 0.0668, -0.0395, 0.0433, -0.0078, -0.0366, 0.0286, -0.0301, 0.0023, 0.0284, -0.0965, -0.0228, -0.0075, -0.0968, 0.2231, 0.0464, -0.0177, -0.0262, -0.2496, -0.0343, 0.0082, -0.087, -0.0224, -0.5577, 0.0062, 0.0702, 0.0396, -0.0915, 0.0162, -0.0607, 0.0235, 0.0355, -0.1029, -0.0998, -0.0685, -0.271, 0.0459, 0.0225, 0.0368, 0.0759, -0.0153, -0.3556, 0.1072, 0.1017, -0.048, 0.0321, 0.0134, 0.0368, 0.0471, -0.2595, 0.0088, 0.0406, 0.0368, -0.0217, -0.2414, -0.0309, -0.0005, -0.0251, 0.0478, 0.1212, 0.2134, 0.0031, 0.2574, 0.0242, 0.0058, 0.0395, 0.027, -0.1224, -0.0122, 0.0541, -0.256, -0.0568, 0.0002, 0.0708, -0.0554, -0.0341, -0.0603, -0.0489, 0.4024, -0.0091, -0.0236, 0.1828, -0.0233, 0.0515, -0.108, -0.0503, -0.0573, 0.1002, -0.0205, 0.0517, 0.0068, -0.068, 0.0079, 0.0352, 0.0006, 0.0082, 0.0527, -0.0963, -0.0826, 0.1342, 0.0436, 0.1095, 0.0198, -0.0052, 0.0599, -0.1023, 0.0427, 0.08, -0.0834, 0.1454, 0.1132, 0.2139, -0.6224, -0.0544, -0.0541, 0.0465, 0.036, 0.0616, 0.0823, 0.0118, -0.3318, 0.0023, -0.0795, 0.6474, 0.0441, -0.0659, -0.0212, -0.0356, 0.0759, 0.0593, 0.0551, 0.0249, 0.0295, 0.279, -0.0214, -0.0544, -0.0026, 0.0026, -0.0114, 0.0507, 0.023, -0.0792, -0.0548, -0.0457, 0.0767, -0.1153, 0.0243, 0.034, 0.0527, -0.0762, 0.0999, -0.045, -0.0452, 0.0266, -0.0179, -0.0063, -0.0617, 0.0071, 0.0549, 0.1069, -0.0226, -0.0059, -0.0535, -0.0097, -0.0183, 0.2572, -0.0157, 0.1969, 0.0353, -0.1263, -0.4035, 0.0456, -0.1514, -0.2378, 0.124, 0.0007, 0.0017, 0.0054, 0.0166, -0.0424, 0.033, 0.0231, -0.0127, 0.0388, 0.3186, 0.0278, -0.0014, -0.0062, 0.0322, 0.3341, 0.003, -0.1596, 0.0029, -0.0605, -0.0301, -0.1263, -0.0143, -0.0255, 0.0705, -0.8316, -0.0347, 0.0973, 0.0364, -0.019, -0.018, 0.0257, 0.0578, -0.0185, 0.0114, -0.0141, 0.0957, 0.1123, -0.0257, 0.0211, -0.0602, 0.1877, -0.0612, 0.0284, -0.0978, -0.0529, -0.0266, -0.0446, -0.0117, -0.1348, -0.0104, 0.0987, -0.0837, 0.1619, 0.05, -0.0111, 0.0056, -0.1176, 0.2829, 0.0629, 0.0723]\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0839, 0.0382, -0.1108, 0.0418, -0.0191, 0.1581, -0.0182, 0.0574, -0.0864, 0.0427, 0.1022, -0.0711, 0.0505, 0.0488, -0.0666, -0.1014, 0.0676, 0.0977, -0.1354, 0.0819, -0.0001, 0.1366, -0.071, 0.04, -0.0244, 0.0863, 0.0129, -0.2264, 0.0132, -0.1845, -0.1005, 0.0722, 0.0393, -0.0855, 0.0413, -0.1045, 0.065, -0.0166, 0.1502, -0.0352, 0.0771, -0.0824, -0.1802, 0.0184, -0.1091, 0.0243, -0.0091, -0.1028, -0.0099, 0.0345, 0.0325, 0.3405, -0.6737, -0.0297, 0.0188, -0.0256, -0.1056, 0.0104, 0.0458, -0.0169, 0.0757, -0.0534, -0.1105, -0.0027, 0.1807, -0.1049, 0.0421, 0.0484, -0.0371, 0.0172, 0.0086, -0.1292, -0.0454, -0.0359, -0.0262, 0.0779, 0.0578, -0.0272, -0.0398, 0.057, -0.0123, 0.0973, -0.0971, -0.1997, 0.0385, -0.0454, -0.0505, -0.0817, 0.2724, 0.0842, 0.0562, 0.0101, -0.0057, -0.1193, -0.0796, 0.0814, -0.0457, -0.084, 0.1044, 0.0162, -0.2209, -0.137, 0.0778, -0.1772, -0.0529, -0.0546, 0.103, 0.008, 0.0315, 0.094, 0.0053, -0.0557, 0.0331, -0.1311, -0.0538, -0.0128, -0.1017, -0.0731, -0.102, -0.2247, 0.0686, -0.0467, -0.1102, 0.1786, -0.2263, 0.2119, 0.1063, 0.0893, 0.1147, -0.0235, 0.0229, -0.0084, -0.0008, -0.0131, -0.0478, -0.2914, 0.0768, -0.0446, 0.0981, -0.0468, -0.053, -0.1576, -0.0075, 0.3016, -0.1322, -0.1272, 0.0803, 0.0594, 0.0183, 0.0009, -0.057, 0.0079, -0.0017, 0.0222, -0.042, -0.0511, 0.0492, -0.1309, -0.0179, -0.1224, -0.1487, 0.0536, 0.0283, 0.0269, 0.1121, 0.108, 0.0651, -0.0794, -0.002, 0.1156, 0.0159, -0.0186, 0.0271, -0.1147, -0.0095, -0.056, 0.3269, -0.0255, -0.0238, -0.0669, -0.0228, -0.1094, 0.0342, 0.0283, -0.0168, 0.0338, -0.0089, 0.1084, 0.019, -0.1894, -0.1229, -0.0557, -0.0551, 0.0829, -0.0882, 0.0817, 0.0927, 0.0584, 0.1822, -0.0861, 0.0825, 0.0111, -0.0728, -0.0015, 0.0952, 0.1117, 0.1838, -0.0249, 0.0914, 0.0199, 0.0693, 0.1073, -0.1251, -0.1747, -0.0744, -0.1413, -0.0534, -0.0247, -0.0113, -0.1281, -0.0008, 0.184, -0.0254, 0.0364, 0.1662, -0.0345, -0.0623, -0.0581, 0.1085, -0.3147, 0.1537, 0.068, 0.332, -0.1044, 0.1263, -0.2395, -0.0788, -0.1552, -0.2362, 0.15, -0.07, -0.0776, -0.0232, 0.0419, 0.0368, 0.0582, 0.0503, -0.042, 0.1468, 0.4063, -0.0531, 0.067, -0.0308, 0.1202, -0.1133, 0.0057, -0.1476, 0.0743, -0.0664, -0.1061, 0.0102, 0.0451, -0.0879, 0.0008, -0.0658, 0.0743, -0.288, -0.0033, -0.1804, 0.0048, -0.0975, 0.0069, 0.1446, -0.066, -0.0514, 0.0231, 0.0344, -0.033, -0.0296, -0.0736, -0.1621, -0.1, 0.1299, -0.0601, 0.0154, -0.0778, 0.0257, 0.0209, -0.0259, 0.0003, -0.0466, -0.0208, 0.0784, -0.0062, -0.0533, -0.1182, -0.1469, 0.0714, -0.0559, -0.1672]\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0.0109, 0.0054, 0.0025, -0.063, -0.0564, 0.0296, -0.0682, 0.0355, -0.0558, 0.0059, -0.0383, 0.0515, 0.0712, -0.0886, 0.1198, -0.0334, 0.0043, 0.0388, 0.0457, -0.0428, 0.0495, -0.0072, 0.0304, -0.0174, 0.0421, 0.0032, -0.039, -0.0322, -0.0113, 0.0603, 0.0074, 0.0102, 0.0481, -0.0294, 0.0271, 0.005, -0.0374, 0.0533, -0.1492, 0.0262, 0.0029, -0.0423, 0.0086, -0.0379, -0.0297, -0.0117, 0.0891, -0.5834, -0.0196, 0.038, 0.0106, -0.023, -0.028, 0.0464, 0.009, -0.0389, -0.033, -0.1084, 0.0001, 0.0224, 0.1006, 0.0005, 0.0152, -0.0146, -0.0575, -0.0436, -0.0181, -0.0158, -0.0427, 0.0685, -0.0116, 0.029, 0.0035, -0.1194, -0.0918, 0.0197, 0.1237, -0.0245, -0.3306, -0.0037, -0.0307, -0.025, 0.0059, 0.1157, -0.0114, -0.0106, -0.0137, 0.0109, 0.0819, 0.052, 0.0564, 0.0283, -0.0403, 0.0134, -0.0875, -0.0785, -0.0019, 0.0188, 0.0821, -0.095, -0.0039, 0.0945, -0.0765, 0.1313, -0.0581, 0.0405, -0.0103, 0.0653, 0.0289, 0.0409, -0.0256, -0.0071, 0.0012, -0.0567, -0.2729, 0.0702, -0.0213, 0.0021, 0.1257, 0.0059, 0.0772, 0.0403, -0.0276, -0.101, -0.0033, -0.0749, -0.0207, -0.0789, 0.0385, -0.0658, -0.1691, -0.0115, -0.0056, 0.1989, 0.1064, -0.0167, 0.0653, 0.0628, 0.3884, -0.0158, -0.0042, 0.0209, -0.0511, -0.0213, -0.0188, -0.009, 0.0099, -0.023, -0.0427, 0.0403, 0.0124, -0.0669, -0.0269, -0.0124, 0.0256, -0.041, -0.0619, 0.0162, -0.1176, -0.2902, 0.0084, 0.1346, -0.0781, 0.0389, -0.0321, -0.0553, -0.0213, -0.0142, 0.0333, 0.0863, -0.1304, 0.1478, -0.5894, 0.0465, 0.0001, 0.0436, -0.0236, 0.0123, 0.0241, 0.02, -0.2658, -0.0006, -0.0608, 0.3386, -0.0067, 0.0549, 0.0206, -0.0162, 0.0078, -0.0411, 0.0081, 0.0067, -0.0451, 0.1383, 0.0427, -0.0006, -0.0881, 0.0111, 0.0129, -0.0609, -0.0975, -0.0001, 0.1469, 0.0451, 0.0174, -0.0055, -0.0166, -0.0333, 0.0994, 0.055, -0.0201, -0.333, -0.0084, 0.155, -0.0631, 0.0423, 0.0196, -0.0049, 0.0698, -0.0575, -0.1492, 0.026, 0.0842, -0.0507, 0.1491, 0.039, 0.0039, 0.2166, -0.0059, 0.013, -0.0529, -0.0177, 0.1542, -0.1744, 0.0889, -0.074, -0.0335, 0.0167, -0.0347, 0.0433, -0.064, 0.0335, -0.0394, 0.0444, 0.3293, -0.0081, -0.0297, 0.0637, 0.0484, -0.3762, 0.0191, -0.0618, 0.0386, 0.0234, 0.0003, 0.0235, 0.033, 0.0034, -0.0156, -0.1348, 0.0016, 0.0838, -0.0008, -0.0768, -0.0085, -0.0156, 0.0373, -0.0186, 0.0309, 0.0197, -0.0242, 0.042, 0.0556, 0.0171, 0.0195, -0.1021, 0.0454, -0.031, -0.0079, 0.0583, -0.0137, -0.0559, -0.0094, 0.0233, 0.04, 0.0042, -0.0218, -0.0945, -0.0154, -0.029, -0.0141, 0.008, 0.1221, 0.0558, -0.1314]\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0541, -0.1693, -0.2085, 0.0336, 0.0164, 0.0185, 0.0572, 0.1305, 0.1083, 0.1635, -0.0928, -0.1733, -0.2562, 0.0145, 0.1272, 0.0667, -0.0638, -0.0656, -0.1194, 0.2281, -0.2363, 0.0972, -0.0492, -0.0691, -0.0449, 0.0405, 0.0308, 0.1126, -0.0709, 0.0084, 0.0317, 0.0308, 0.0168, 0.1272, -0.0534, -0.0581, 0.1446, -0.0116, -0.043, -0.022, -0.0283, 0.1325, 0.0688, 0.0994, -0.0853, -0.0225, 0.2092, -0.0275, 0.2151, 0.172, -0.1051, 0.0822, -0.5221, 0.1069, -0.0378, 0.0731, -0.0305, -0.1683, -0.0088, -0.0429, 0.0131, -0.1097, -0.0022, 0.0002, -0.0043, -0.096, -0.1099, -0.0747, -0.0131, 0.2035, -0.1784, -0.0369, 0.1545, -0.0019, 0.135, -0.0754, -0.1441, -0.031, -0.0962, -0.0743, 0.0001, -0.0974, 0.0039, -0.1964, 0.1196, 0.0615, -0.0953, 0.0185, 0.2042, -0.0597, -0.1038, -0.1591, 0.0298, -0.0939, -0.0512, -0.1588, -0.1518, 0.102, -0.0392, 0.2233, -0.2181, 0.1497, -0.0828, -0.0198, -0.1274, 0.0733, -0.1035, -0.0701, -0.0769, -0.0999, 0.0275, 0.0737, 0.1603, -0.1997, 0.1078, 0.0825, -0.2304, -0.0463, 0.1015, -0.4098, -0.0087, 0.0552, 0.1142, 0.1692, 0.0143, 0.2033, -0.0752, 0.022, 0.0012, -0.0148, -0.0752, 0.0268, -0.0599, -0.0152, -0.0523, 0.0308, -0.0994, 0.047, -0.0372, 0.0267, 0.0289, -0.1426, -0.1549, 0.1534, -0.0847, 0.0472, 0.0623, -0.036, 0.111, -0.0268, -0.1277, 0.0663, -0.0618, 0.1483, -0.1098, 0.0847, -0.2946, 0.0628, 0.1324, -0.0748, -0.084, 0.0243, -0.0539, 0.0629, 0.0344, -0.0506, 0.0951, -0.084, 0.0219, 0.0758, -0.1297, 0.0955, -0.1427, 0.1777, 0.1345, -0.077, 0.2827, -0.0153, 0.1393, 0.1167, 0.1465, -0.0822, -0.0677, -0.1025, 0.2061, 0.1091, -0.0583, 0.0067, -0.2186, 0.088, 0.0278, 0.0258, -0.1377, 0.0832, 0.1353, 0.2988, -0.035, 0.0496, 0.1578, -0.0074, 0.0041, 0.0927, -0.0824, -0.1153, -0.0026, 0.0871, 0.0515, -0.1433, -0.1, 0.1324, 0.0407, -0.0855, -0.153, 0.0834, -0.0173, 0.0197, -0.1147, -0.0146, -0.1977, -0.0408, 0.0307, -0.09, -0.0107, -0.0532, 0.0155, 0.0233, 0.1314, -0.0075, -0.1629, -0.1442, -0.1025, 0.0849, 0.3512, -0.1586, 0.0304, -0.0677, 0.1338, 0.092, -0.2702, 0.2873, -0.0466, 0.086, 0.1871, 0.0568, -0.0387, -0.1473, 0.0813, -0.0127, -0.0076, 0.3465, -0.0328, 0.0207, 0.05, -0.0542, 0.1882, -0.0193, -0.043, 0.1066, 0.0563, 0.0456, 0.009, 0.1324, 0.2351, 0.1135, -0.7036, 0.0765, -0.0756, 0.0219, 0.0337, 0.0331, -0.1379, -0.0574, -0.1276, 0.0043, -0.1431, 0.082, -0.0956, -0.0155, -0.0161, -0.0401, 0.0976, -0.0138, -0.0022, -0.1148, 0.043, -0.1129, 0.0314, 0.0043, 0.0565, 0.0013, -0.2174, -0.0845, -0.1171, 0.004, -0.1723, -0.0468, 0.0056, 0.059, -0.1025, -0.1085]\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.33s/it]\u001b[A\n","03/28/2021 18:12:30 - INFO - bert_utils -     flaw_f1 = 0.8444444444444443\n","03/28/2021 18:12:30 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:30 - INFO - bert_utils -     flaw_recall = 0.7307692307692307\n","03/28/2021 18:12:30 - INFO - bert_utils -     loss = 0.0416310578584671\n","Epoch:  80%|████████████████████████████▊       | 20/25 [01:39<00:24,  4.95s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.44s/it]\u001b[A\n","03/28/2021 18:12:35 - INFO - bert_utils -     flaw_f1 = 0.972972972972973\n","03/28/2021 18:12:35 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:35 - INFO - bert_utils -     flaw_recall = 0.9473684210526315\n","03/28/2021 18:12:35 - INFO - bert_utils -     loss = 0.013721185736358166\n","Epoch:  84%|██████████████████████████████▏     | 21/25 [01:44<00:19,  4.97s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0806, -0.0063, 0.0875, 0.0152, -0.068, -0.0948, 0.0018, 0.0061, -0.0728, -0.0068, -0.0113, 0.0812, 0.0365, -0.0536, -0.0777, 0.0078, 0.0177, 0.0004, 0.0115, -0.0501, 0.0093, 0.0002, 0.0282, 0.0255, -0.114, -0.1124, 0.0629, 0.1254, -0.0207, 0.0129, 0.0243, -0.0201, 0.0887, -0.0181, -0.007, 0.0058, -0.0462, -0.0616, 0.083, -0.0028, -0.0745, 0.0229, 0.0034, -0.0238, -0.0119, -0.0123, -0.0423, -0.0241, 0.0103, -0.019, 0.0154, 0.0061, -0.6269, 0.0171, -0.0113, -0.0304, 0.007, -0.1632, -0.3254, 0.0079, 0.044, -0.0183, 0.0466, -0.0078, -0.0634, -0.105, -0.0265, -0.0244, -0.0143, 0.0176, 0.0104, -0.0365, 0.0041, 0.0303, 0.0076, 0.023, -0.0337, -0.0088, 0.0106, 0.1418, -0.0157, -0.0251, 0.0121, -0.1602, 0.0105, 0.0241, -0.0038, 0.0239, -0.1055, -0.0054, -0.0658, -0.04, 0.0187, 0.099, 0.0372, -0.0159, -0.0113, 0.0156, 0.0953, -0.0334, -0.0971, -0.0237, 0.0123, 0.0271, -0.2827, -0.0086, 0.0576, 0.1455, 0.0356, -0.0489, 0.0021, 0.1179, -0.0209, 0.0518, 0.0245, -0.1199, 0.0084, 0.018, 0.0371, -0.3544, -0.0397, -0.0426, 0.0044, 0.0544, 0.0687, 0.0631, -0.0555, 0.0517, 0.0679, 0.0582, 0.045, -0.0167, 0.0014, 0.0793, 0.021, -0.3372, -0.0326, -0.0321, 0.055, -0.0962, -0.0316, 0.0132, 0.0309, 0.3101, -0.0624, 0.0419, 0.038, -0.113, -0.0231, 0.0135, 0.0323, 0.0158, 0.0779, -0.0368, 0.0279, 0.0072, -0.0174, 0.0178, 0.0026, -0.0547, 0.0492, 0.0599, 0.0472, -0.0817, -0.2665, 0.0267, -0.0128, 0.0237, -0.0342, -0.0213, -0.233, 0.0408, 0.0358, -0.0759, 0.2026, -0.0613, 0.0854, -0.3599, 0.0217, -0.0333, 0.0411, 0.0109, 0.0316, 0.0149, 0.0561, -0.0965, -0.0061, -0.1178, -0.0211, 0.0052, -0.0192, -0.0625, 0.0009, -0.0206, -0.0285, -0.0156, -0.0162, -0.0764, 0.1422, -0.0007, 0.0148, -0.0405, -0.0455, -0.0352, 0.0295, -0.0271, 0.0716, -0.0552, -0.0059, -0.0261, 0.0505, -0.0137, -0.0375, 0.0311, 0.0322, -0.0155, -0.1239, -0.0108, -0.0179, 0.0028, 0.0179, 0.0319, 0.0337, -0.0174, 0.0526, -0.0824, -0.0206, 0.0564, -0.0256, -0.2518, -0.0051, -0.0165, 0.2127, -0.0211, 0.0346, 0.2194, 0.0061, 0.0724, -0.086, -0.0035, -0.0597, 0.0046, 0.034, -0.0349, -0.0014, 0.038, -0.0007, -0.0122, -0.0254, 0.2632, -0.0164, 0.0878, -0.0171, 0.0378, 0.0345, 0.0363, 0.0725, 0.0042, -0.0122, 0.0374, -0.0208, -0.0353, -0.0012, 0.0018, -0.1729, 0.0315, -0.0483, 0.0162, 0.0853, -0.0103, -0.0061, -0.02, -0.0212, 0.0173, 0.0503, 0.0044, -0.087, 0.0067, -0.0138, 0.0075, 0.1034, -0.3706, -0.0438, 0.0262, 0.0773, -0.0511, -0.0179, 0.0647, 0.1465, 0.0517, 0.069, -0.0139, -0.025, -0.0271, -0.0258, -0.0259, -0.0265, 0.2192, 0.0188, -0.0608]\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0736, 0.0135, -0.1083, 0.1089, -0.1087, 0.0232, 0.1087, -0.0318, 0.0208, -0.011, -0.2369, -0.2483, -0.0633, -0.0387, 0.0319, 0.0891, -0.0856, -0.2158, -0.0927, 0.1301, -0.2474, -0.0753, -0.0966, 0.1835, 0.0419, 0.0195, -0.0634, 0.0602, -0.1821, 0.0648, 0.0741, -0.0607, 0.0716, -0.0231, 0.1012, 0.1186, -0.0177, 0.0323, -0.1141, -0.0585, -0.2028, 0.027, 0.0391, -0.0195, -0.0174, -0.1147, 0.0162, -0.0021, 0.0755, 0.0354, -0.077, -0.1115, -0.5843, -0.0472, 0.0425, 0.0145, -0.0537, 0.1364, -0.0622, -0.1121, 0.1621, -0.024, 0.0191, 0.1304, 0.1066, -0.0098, -0.1245, -0.0265, -0.033, 0.1369, -0.0176, 0.0889, 0.0956, -0.108, -0.0504, -0.0584, -0.0176, -0.034, 0.1054, -0.1764, 0.0025, -0.1407, 0.0732, -0.2656, 0.113, 0.0903, 0.0129, -0.1572, 0.2595, -0.1947, -0.0683, -0.1396, 0.1252, -0.1358, 0.092, 0.0609, 0.0963, -0.0151, -0.1351, 0.0229, -0.232, 0.019, 0.1418, -0.1494, -0.1934, 0.067, -0.1161, 0.1013, 0.0036, 0.0567, -0.0106, 0.0348, 0.0811, 0.0098, 0.1119, 0.0083, 0.0455, 0.1506, 0.0196, -0.4059, 0.0304, -0.1542, -0.0105, 0.2264, 0.0355, 0.1976, -0.0419, -0.029, -0.0962, 0.1047, 0.0111, -0.1088, -0.0032, 0.0993, 0.0412, 0.0858, -0.0176, -0.004, 0.1006, -0.0032, -0.0923, -0.0907, -0.2245, 0.2384, 0.0644, -0.0213, -0.0848, -0.022, 0.0247, -0.0943, 0.0088, 0.0523, 0.0838, -0.2077, -0.2299, -0.0537, -0.1127, 0.1194, 0.0332, 0.1894, 0.1053, -0.0035, -0.0954, 0.0303, 0.0863, 0.1312, 0.135, 0.0811, 0.1031, 0.0065, -0.1305, -0.1352, -0.0962, 0.0314, -0.0394, -0.1125, 0.2902, -0.1142, -0.0299, 0.0938, -0.1445, 0.0181, -0.0421, -0.0773, 0.0154, 0.0592, -0.0572, -0.004, -0.2083, 0.1658, 0.0263, 0.0904, 0.0257, 0.0339, 0.1064, -0.0344, -0.043, -0.077, 0.2063, 0.0351, -0.062, -0.0945, -0.1323, -0.0476, 0.0072, 0.1318, 0.0076, 0.0175, -0.0133, 0.0833, 0.0836, -0.0544, -0.0933, 0.0908, -0.0781, -0.1136, -0.1556, 0.0541, -0.029, 0.0579, 0.0779, -0.0817, 0.0241, -0.1043, 0.344, -0.1298, 0.0768, -0.3173, 0.0591, -0.1113, -0.1582, -0.0202, 0.3818, -0.1023, -0.0764, -0.0619, -0.1192, 0.0391, -0.3044, 0.0943, -0.1769, -0.1227, 0.1711, -0.0874, 0.0644, 0.0461, -0.0147, -0.009, -0.0068, 0.3903, -0.0584, 0.1163, -0.0564, 0.0528, 0.1174, -0.0036, -0.0747, 0.2458, -0.0048, 0.0845, -0.1193, 0.2023, 0.0851, -0.0029, -0.6706, 0.0114, -0.0332, -0.1013, 0.0241, 0.1329, 0.074, -0.0061, -0.0353, -0.1578, 0.0207, -0.106, -0.0048, 0.0931, -0.0634, -0.0418, 0.072, 0.1157, 0.1244, 0.0948, -0.1263, -0.1257, -0.0741, 0.085, 0.0408, 0.0385, 0.02, 0.054, 0.1094, -0.0355, 0.1336, -0.0817, -0.0935, 0.071, 0.0632, -0.0373]\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0495, 0.0411, 0.0041, 0.0309, -0.0044, -0.1151, 0.006, 0.017, 0.0045, -0.0288, 0.017, 0.0007, 0.0533, 0.0094, -0.0609, -0.0267, 0.0497, 0.0474, 0.0054, 0.0511, -0.0715, 0.0876, 0.055, -0.001, -0.0746, 0.0008, 0.0258, -0.1404, 0.0022, 0.0469, 0.0114, 0.0083, -0.0127, -0.0453, 0.0011, -0.008, -0.013, -0.1271, 0.002, 0.0389, -0.0395, -0.0295, -0.0308, 0.0348, -0.1388, -0.0647, 0.0302, 0.0184, 0.0499, 0.0168, -0.0176, 0.089, -0.5547, 0.0144, 0.03, 0.0127, 0.0345, 0.1792, 0.0629, -0.0242, -0.0491, -0.0397, -0.0014, -0.0571, 0.0906, -0.0009, 0.0266, -0.0018, 0.0308, -0.0057, 0.0569, 0.0273, -0.0338, 0.1003, 0.0299, 0.0115, 0.0717, 0.0319, -0.0726, 0.1526, -0.0026, -0.1321, -0.0287, -0.2439, 0.0073, -0.0062, 0.0101, -0.0128, -0.0106, 0.0202, -0.0165, -0.0867, 0.0493, -0.0916, 0.0507, 0.1032, 0.0108, 0.0881, 0.0655, -0.0127, -0.0895, -0.0348, 0.0439, 0.0069, -0.3768, -0.0176, 0.1296, 0.0027, 0.2343, -0.0009, 0.0337, 0.0613, -0.0369, 0.0564, -0.0901, -0.0046, 0.036, 0.0341, -0.0171, -0.1717, -0.0041, -0.0553, -0.0661, 0.0957, -0.0804, 0.0868, -0.0181, -0.0602, -0.1523, -0.0104, -0.0034, -0.0547, 0.0094, -0.0223, -0.0184, -0.3151, -0.0358, 0.0354, 0.0393, 0.0526, 0.001, -0.0163, 0.0497, 0.2518, -0.0173, -0.0036, 0.018, -0.1081, 0.0368, -0.0141, -0.0436, 0.0291, -0.0366, -0.0523, 0.0464, 0.0018, -0.0183, 0.0766, 0.0156, 0.0276, 0.0522, -0.0221, 0.0408, -0.0703, -0.2291, -0.003, 0.0343, -0.0961, -0.0092, 0.0222, 0.0166, -0.0344, 0.0463, 0.0186, 0.0283, -0.0522, 0.0369, -0.4955, 0.0276, -0.0247, 0.0257, 0.0632, -0.0232, -0.0063, -0.0076, -0.3897, -0.0108, -0.0612, 0.1962, -0.006, -0.0353, -0.0994, -0.0124, -0.0031, 0.0427, 0.0134, -0.0043, -0.0102, 0.21, 0.0163, -0.0155, 0.1707, -0.0339, -0.0125, 0.02, 0.0148, -0.0342, -0.0254, 0.0001, 0.0412, -0.0258, -0.0169, 0.0113, 0.0031, 0.0075, -0.0059, 0.1255, 0.0225, 0.0005, 0.0462, 0.0096, -0.0664, 0.0138, 0.0511, 0.0372, 0.0073, -0.0187, 0.0186, -0.0021, -0.012, 0.0007, -0.0026, 0.1725, 0.079, 0.0265, -0.0066, -0.0765, 0.2336, -0.1445, 0.0072, -0.026, -0.0147, 0.0521, 0.0011, -0.0244, 0.0583, -0.0207, 0.032, 0.0294, 0.483, -0.0444, -0.087, -0.0754, 0.0276, -0.0606, -0.0227, -0.0057, -0.0298, 0.054, -0.0607, -0.0746, 0.0666, -0.024, -0.0399, -0.2321, 0.0054, 0.0233, 0.046, 0.1874, -0.053, -0.0285, -0.0521, -0.0146, 0.0264, -0.0093, 0.025, -0.0552, 0.0152, 0.0242, -0.0577, 0.006, 0.0511, 0.023, -0.0345, 0.0134, -0.0042, -0.1267, -0.1572, -0.0783, 0.0706, 0.0004, -0.0142, -0.0976, -0.0489, -0.0625, -0.0327, 0.007, 0.2371, -0.0298, -0.0284]\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0568, -0.1339, 0.1573, 0.0057, 0.194, -0.2087, 0.0217, 0.0051, -0.0037, 0.0886, -0.025, -0.1547, -0.0509, 0.0492, 0.1833, -0.0597, 0.0121, 0.0852, 0.256, 0.0516, 0.0196, 0.1505, 0.0116, -0.0267, -0.037, -0.1625, 0.0446, -0.256, 0.0354, -0.0035, 0.1073, 0.0501, -0.0761, -0.0933, 0.0849, 0.0552, 0.0571, -0.3266, -0.0249, -0.0003, 0.1035, -0.0916, 0.1449, -0.1644, 0.1403, -0.1167, 0.0611, 0.0411, -0.0707, 0.0029, -0.0496, 0.0168, -0.7128, -0.0906, 0.1049, -0.0882, -0.0613, 0.0491, 0.1263, 0.0096, 0.0423, -0.0286, 0.1267, -0.0772, -0.0205, -0.0492, 0.0136, 0.0094, -0.143, 0.0167, -0.0316, -0.0583, -0.0147, 0.1385, -0.0844, -0.0134, 0.1329, 0.0116, -0.2098, 0.2224, 0.002, 0.1558, 0.0542, -0.174, 0.0897, -0.032, 0.05, 0.006, -0.2922, -0.1066, 0.0215, 0.0971, -0.0657, 0.0037, 0.0508, -0.174, -0.0422, -0.0416, 0.0016, -0.0965, -0.2728, -0.1393, 0.0569, -0.0341, -0.0352, -0.0207, 0.2142, 0.079, -0.209, -0.0819, -0.0115, -0.1229, 0.0603, -0.13, -0.0065, 0.0507, 0.0453, -0.0568, 0.0429, -0.2566, 0.0585, -0.0201, -0.1377, 0.0948, -0.4483, 0.2669, -0.1096, -0.0509, -0.2165, 0.0468, -0.1828, 0.1138, 0.0407, 0.042, -0.021, -0.2902, 0.0045, 0.0677, 0.121, -0.0588, 0.0427, 0.033, 0.0429, 0.3968, -0.0114, -0.0931, 0.1555, -0.0371, -0.0992, -0.0378, -0.0278, -0.1709, -0.1218, -0.0116, 0.073, 0.0756, -0.1181, -0.0068, -0.0803, 0.0444, 0.0408, 0.0389, -0.0052, 0.0638, -0.3224, 0.1537, 0.1595, 0.004, 0.0665, 0.0536, -0.0066, -0.0646, 0.0127, 0.0308, 0.1717, -0.0062, 0.3631, -0.3776, 0.0954, -0.0606, 0.1313, -0.0308, -0.0911, 0.079, -0.0056, 0.2828, 0.0027, -0.1449, 0.0889, -0.0392, 0.0279, 0.0361, -0.1109, 0.0228, -0.0363, 0.049, -0.0457, -0.1489, 0.3391, 0.0055, 0.0712, 0.0943, 0.0467, 0.0785, -0.0447, -0.0143, 0.0106, -0.1578, -0.2198, 0.0843, 0.074, -0.0401, -0.0525, 0.027, 0.0177, -0.0929, -0.0504, -0.0542, -0.0809, 0.052, 0.1789, 0.1612, 0.0008, 0.0301, 0.0396, -0.346, -0.0021, 0.0136, 0.049, -0.0123, -0.0853, 0.0537, 0.3109, -0.0686, 0.151, 0.0611, 0.0794, -0.1792, -0.2695, -0.0054, -0.0379, -0.0062, 0.0115, 0.006, 0.0348, 0.0524, -0.0755, 0.1379, 0.107, 0.4634, -0.0354, -0.0732, -0.0586, 0.0211, 0.238, -0.0239, 0.1504, 0.0316, -0.0489, 0.1382, 0.0628, 0.0233, -0.1273, -0.0893, -0.4087, -0.0572, 0.1639, -0.0343, -0.1047, 0.215, -0.0951, -0.0839, -0.0705, -0.0843, 0.0309, 0.0536, -0.0857, -0.0801, 0.0284, 0.0525, -0.013, -0.0412, 0.0942, -0.114, 0.0839, 0.0898, 0.1567, -0.024, 0.065, 0.0236, -0.0525, -0.0036, -0.003, -0.0604, 0.0575, 0.1172, -0.0517, 0.1603, 0.0341, 0.033]\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.44s/it]\u001b[A\n","03/28/2021 18:12:40 - INFO - bert_utils -     flaw_f1 = 0.8421052631578947\n","03/28/2021 18:12:40 - INFO - bert_utils -     flaw_precision = 0.9411764705882353\n","03/28/2021 18:12:40 - INFO - bert_utils -     flaw_recall = 0.7619047619047619\n","03/28/2021 18:12:40 - INFO - bert_utils -     loss = 0.0362454392015934\n","Epoch:  88%|███████████████████████████████▋    | 22/25 [01:49<00:14,  4.98s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0063, -0.0253, -0.0338, 0.0178, -0.0966, 0.0946, 0.0328, -0.0227, 0.0628, -0.0189, -0.0063, 0.028, 0.0215, -0.0924, -0.0071, -0.0246, 0.0363, -0.023, -0.0744, -0.0124, 0.0055, 0.0781, 0.0156, -0.1322, -0.049, 0.0756, 0.0529, 0.006, -0.0194, 0.0552, -0.0176, 0.0049, -0.1204, -0.0272, 0.0189, 0.0482, -0.0392, -0.105, 0.0358, 0.0432, -0.0006, -0.0452, 0.0313, 0.052, 0.03, 0.0132, -0.0345, -0.0012, -0.0105, 0.0289, -0.0064, 0.0337, -0.6136, 0.0142, 0.0334, 0.0008, -0.0354, -0.0456, -0.118, -0.0009, 0.0319, 0.0059, -0.0208, -0.0383, -0.0216, -0.0103, 0.0015, 0.0315, -0.0042, 0.0418, 0.0161, -0.0241, -0.0046, -0.1596, 0.0238, 0.0092, -0.0144, 0.0292, 0.0148, 0.058, 0.015, -0.02, 0.0254, -0.1357, 0.0079, -0.0345, 0.0288, 0.0071, 0.3238, 0.0305, 0.0306, 0.0288, 0.027, 0.0484, 0.0239, -0.0059, 0.0288, 0.0114, 0.0242, -0.0037, -0.0958, -0.027, 0.0234, 0.0275, -0.1558, -0.0034, 0.0985, -0.0422, 0.0911, 0.0191, -0.0207, 0.1338, -0.0164, 0.0123, 0.0567, -0.0961, 0.0141, 0.0091, 0.008, -0.118, -0.0499, -0.0656, 0.0867, 0.0707, -0.0513, 0.1399, -0.0287, -0.0949, -0.0987, 0.046, 0.0002, 0.0301, 0.0377, -0.0029, 0.0053, -0.0905, 0.0063, 0.021, -0.0647, -0.0603, 0.0118, -0.0129, 0.0166, 0.2834, -0.0506, -0.0332, 0.0189, -0.0091, 0.0069, 0.031, 0.1842, -0.011, 0.0169, -0.0496, -0.0089, 0.0083, 0.0131, 0.0724, 0.0011, 0.0095, 0.0181, 0.0133, 0.0425, -0.0187, -0.2456, -0.0242, 0.0195, -0.0001, -0.0793, -0.042, -0.026, -0.0363, -0.0072, 0.013, 0.1629, -0.0843, 0.1183, -0.2981, 0.0171, 0.008, 0.0109, 0.0102, 0.0291, -0.0157, -0.0113, -0.0126, -0.0179, 0.0381, 0.3813, 0.0404, 0.0459, -0.0778, -0.0129, -0.0218, -0.0194, 0.0301, -0.0795, -0.0375, 0.234, 0.0163, 0.0039, -0.0551, -0.0511, -0.003, 0.0312, -0.0064, 0.1368, -0.0918, -0.051, 0.0165, -0.034, 0.0267, -0.0197, 0.0153, 0.0328, 0.022, -0.0538, -0.002, 0.0477, 0.0268, -0.0481, -0.0166, 0.0202, -0.0226, -0.0569, 0.1067, 0.0382, 0.0267, -0.0204, 0.0269, 0.0877, -0.0313, 0.1969, -0.0524, -0.0059, -0.1442, -0.0296, 0.0781, -0.1937, -0.0414, -0.0156, -0.012, 0.0127, -0.0086, 0.0296, -0.0002, 0.0592, -0.0103, -0.0069, 0.3157, 0.0263, 0.0313, -0.1475, 0.021, -0.0327, 0.0169, 0.0181, -0.0009, 0.0492, -0.0543, -0.0555, 0.0065, -0.0051, 0.0113, -0.1789, 0.024, -0.001, 0.0108, -0.0587, 0.0118, -0.0764, -0.0438, 0.0592, 0.0092, -0.0187, -0.016, -0.0391, 0.0236, 0.016, 0.0025, -0.0836, -0.1061, 0.0585, 0.0568, 0.0207, 0.0205, 0.0175, 0.0661, 0.0756, 0.0728, -0.0402, 0.0075, 0.0015, -0.014, -0.0487, 0.0517, 0.0088, 0.1155, 0.0073, 0.0168]\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.018, 0.0137, 0.0563, 0.0203, -0.0224, -0.0579, 0.0017, -0.0083, -0.0074, 0.0621, 0.0457, -0.0904, 0.0844, 0.013, -0.1144, 0.0202, -0.0783, -0.0346, -0.0351, 0.0287, -0.0507, -0.0797, 0.0259, 0.0543, 0.0421, -0.1147, -0.0449, -0.0618, -0.0264, 0.0732, 0.0393, -0.0798, -0.0383, -0.0212, 0.0242, -0.0619, 0.0028, -0.0545, -0.1246, 0.0882, -0.0696, 0.0284, -0.1216, 0.0243, 0.0088, 0.0228, -0.0419, 0.0312, 0.0095, -0.0116, -0.0563, -0.0754, -0.6494, -0.0508, 0.0136, -0.0307, -0.0149, -0.052, 0.0104, 0.0411, 0.0321, -0.0849, 0.0666, -0.0499, -0.0406, -0.0305, 0.0453, 0.0657, -0.0713, 0.0026, -0.0071, -0.0073, -0.012, -0.0021, 0.0152, 0.0728, -0.0177, 0.0067, 0.0472, 0.2579, 0.0086, 0.0526, -0.0531, -0.2221, -0.005, -0.0434, 0.0486, 0.0129, -0.0179, 0.0281, -0.0121, 0.0198, 0.0175, -0.026, -0.0251, -0.0165, 0.0305, 0.0568, 0.0422, -0.0578, -0.1283, -0.0809, -0.0824, 0.0095, 0.0213, 0.0042, 0.1039, 0.2486, 0.0316, 0.0603, 0.0895, 0.0652, 0.0716, 0.0304, -0.2216, -0.0121, 0.0162, -0.0474, -0.0523, -0.3365, -0.0059, -0.0389, -0.0493, 0.054, 0.0473, 0.1572, -0.0403, 0.2288, -0.1701, 0.0341, 0.1491, -0.042, 0.0274, -0.0687, 0.0232, -0.2326, -0.0087, -0.0353, 0.0201, -0.0392, -0.0263, -0.0005, 0.0106, 0.3222, 0.1188, -0.0025, 0.1381, -0.0301, -0.1146, 0.0012, -0.0458, 0.0793, 0.0299, -0.0276, 0.0263, -0.0017, -0.0616, -0.0363, 0.0757, -0.0887, -0.0505, 0.0739, 0.0222, 0.0485, -0.1574, 0.0128, 0.115, -0.0137, 0.0246, 0.0834, 0.062, 0.0275, -0.0376, -0.0543, 0.242, 0.0166, 0.1765, -0.1171, 0.0097, 0.0407, 0.0245, 0.0482, 0.0543, 0.0164, 0.0306, 0.0662, -0.0758, -0.0842, -0.1406, 0.0708, -0.0021, -0.032, 0.0435, 0.0469, -0.0045, 0.0869, 0.0144, 0.0109, 0.1798, -0.0279, 0.0251, -0.0165, -0.0496, -0.0203, 0.0504, 0.0375, 0.055, -0.0736, -0.1372, 0.0084, 0.0426, -0.001, -0.0611, -0.003, 0.0637, -0.0185, 0.0292, 0.022, -0.0052, 0.0605, 0.1659, -0.0197, -0.073, 0.0347, 0.0514, -0.0462, -0.033, 0.1258, -0.0531, -0.1589, -0.0904, -0.0152, 0.3261, -0.1132, -0.0136, 0.0702, -0.02, 0.0705, -0.1598, 0.0617, -0.0651, 0.0309, 0.0323, -0.0752, -0.0333, 0.0489, -0.0132, 0.0433, 0.0011, 0.3065, 0.0577, 0.1041, 0.0583, -0.0108, -0.1463, -0.0195, -0.0661, 0.0006, -0.0628, 0.0766, -0.1766, -0.1066, -0.0112, -0.0637, -0.3468, -0.0268, -0.0201, -0.0342, 0.05, 0.047, -0.0543, 0.0142, -0.102, -0.0296, -0.0426, 0.0159, 0.0272, -0.065, -0.0514, -0.0284, 0.0657, -0.0163, 0.0494, -0.0867, 0.022, 0.0482, 0.0015, 0.0358, 0.0483, 0.0545, -0.0117, -0.0571, 0.014, -0.047, 0.0189, -0.0918, -0.0813, 0.2471, 0.0845, -0.0592]\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.39s/it]\u001b[A\n","03/28/2021 18:12:45 - INFO - bert_utils -     flaw_f1 = 0.896551724137931\n","03/28/2021 18:12:45 - INFO - bert_utils -     flaw_precision = 0.9285714285714286\n","03/28/2021 18:12:45 - INFO - bert_utils -     flaw_recall = 0.8666666666666667\n","03/28/2021 18:12:45 - INFO - bert_utils -     loss = 0.02710302732884884\n","Epoch:  92%|█████████████████████████████████   | 23/25 [01:54<00:09,  4.96s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0736, 0.0135, -0.1083, 0.1089, -0.1087, 0.0232, 0.1087, -0.0318, 0.0208, -0.011, -0.2369, -0.2483, -0.0633, -0.0387, 0.0319, 0.0891, -0.0856, -0.2158, -0.0927, 0.1301, -0.2474, -0.0753, -0.0966, 0.1835, 0.0419, 0.0195, -0.0634, 0.0602, -0.1821, 0.0648, 0.0741, -0.0607, 0.0716, -0.0231, 0.1012, 0.1186, -0.0177, 0.0323, -0.1141, -0.0585, -0.2028, 0.027, 0.0391, -0.0195, -0.0174, -0.1147, 0.0162, -0.0021, 0.0755, 0.0354, -0.077, -0.1115, -0.5843, -0.0472, 0.0425, 0.0145, -0.0537, 0.1364, -0.0622, -0.1121, 0.1621, -0.024, 0.0191, 0.1304, 0.1066, -0.0098, -0.1245, -0.0265, -0.033, 0.1369, -0.0176, 0.0889, 0.0956, -0.108, -0.0504, -0.0584, -0.0176, -0.034, 0.1054, -0.1764, 0.0025, -0.1407, 0.0732, -0.2656, 0.113, 0.0903, 0.0129, -0.1572, 0.2595, -0.1947, -0.0683, -0.1396, 0.1252, -0.1358, 0.092, 0.0609, 0.0963, -0.0151, -0.1351, 0.0229, -0.232, 0.019, 0.1418, -0.1494, -0.1934, 0.067, -0.1161, 0.1013, 0.0036, 0.0567, -0.0106, 0.0348, 0.0811, 0.0098, 0.1119, 0.0083, 0.0455, 0.1506, 0.0196, -0.4059, 0.0304, -0.1542, -0.0105, 0.2264, 0.0355, 0.1976, -0.0419, -0.029, -0.0962, 0.1047, 0.0111, -0.1088, -0.0032, 0.0993, 0.0412, 0.0858, -0.0176, -0.004, 0.1006, -0.0032, -0.0923, -0.0907, -0.2245, 0.2384, 0.0644, -0.0213, -0.0848, -0.022, 0.0247, -0.0943, 0.0088, 0.0523, 0.0838, -0.2077, -0.2299, -0.0537, -0.1127, 0.1194, 0.0332, 0.1894, 0.1053, -0.0035, -0.0954, 0.0303, 0.0863, 0.1312, 0.135, 0.0811, 0.1031, 0.0065, -0.1305, -0.1352, -0.0962, 0.0314, -0.0394, -0.1125, 0.2902, -0.1142, -0.0299, 0.0938, -0.1445, 0.0181, -0.0421, -0.0773, 0.0154, 0.0592, -0.0572, -0.004, -0.2083, 0.1658, 0.0263, 0.0904, 0.0257, 0.0339, 0.1064, -0.0344, -0.043, -0.077, 0.2063, 0.0351, -0.062, -0.0945, -0.1323, -0.0476, 0.0072, 0.1318, 0.0076, 0.0175, -0.0133, 0.0833, 0.0836, -0.0544, -0.0933, 0.0908, -0.0781, -0.1136, -0.1556, 0.0541, -0.029, 0.0579, 0.0779, -0.0817, 0.0241, -0.1043, 0.344, -0.1298, 0.0768, -0.3173, 0.0591, -0.1113, -0.1582, -0.0202, 0.3818, -0.1023, -0.0764, -0.0619, -0.1192, 0.0391, -0.3044, 0.0943, -0.1769, -0.1227, 0.1711, -0.0874, 0.0644, 0.0461, -0.0147, -0.009, -0.0068, 0.3903, -0.0584, 0.1163, -0.0564, 0.0528, 0.1174, -0.0036, -0.0747, 0.2458, -0.0048, 0.0845, -0.1193, 0.2023, 0.0851, -0.0029, -0.6706, 0.0114, -0.0332, -0.1013, 0.0241, 0.1329, 0.074, -0.0061, -0.0353, -0.1578, 0.0207, -0.106, -0.0048, 0.0931, -0.0634, -0.0418, 0.072, 0.1157, 0.1244, 0.0948, -0.1263, -0.1257, -0.0741, 0.085, 0.0408, 0.0385, 0.02, 0.054, 0.1094, -0.0355, 0.1336, -0.0817, -0.0935, 0.071, 0.0632, -0.0373]\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.33s/it]\u001b[A\n","03/28/2021 18:12:50 - INFO - bert_utils -     flaw_f1 = 0.9714285714285714\n","03/28/2021 18:12:50 - INFO - bert_utils -     flaw_precision = 1.0\n","03/28/2021 18:12:50 - INFO - bert_utils -     flaw_recall = 0.9444444444444444\n","03/28/2021 18:12:50 - INFO - bert_utils -     loss = 0.011465275660157204\n","Epoch:  96%|██████████████████████████████████▌ | 24/25 [01:59<00:04,  4.94s/it]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[Aexamples:  [[37  1 18 38 39 34 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]\n"," [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  1\n","tok_id :  18\n","tok_id :  38\n","tok_id :  39\n","tok_id :  34\n","tok_id :  40\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [0.0806, -0.0063, 0.0875, 0.0152, -0.068, -0.0948, 0.0018, 0.0061, -0.0728, -0.0068, -0.0113, 0.0812, 0.0365, -0.0536, -0.0777, 0.0078, 0.0177, 0.0004, 0.0115, -0.0501, 0.0093, 0.0002, 0.0282, 0.0255, -0.114, -0.1124, 0.0629, 0.1254, -0.0207, 0.0129, 0.0243, -0.0201, 0.0887, -0.0181, -0.007, 0.0058, -0.0462, -0.0616, 0.083, -0.0028, -0.0745, 0.0229, 0.0034, -0.0238, -0.0119, -0.0123, -0.0423, -0.0241, 0.0103, -0.019, 0.0154, 0.0061, -0.6269, 0.0171, -0.0113, -0.0304, 0.007, -0.1632, -0.3254, 0.0079, 0.044, -0.0183, 0.0466, -0.0078, -0.0634, -0.105, -0.0265, -0.0244, -0.0143, 0.0176, 0.0104, -0.0365, 0.0041, 0.0303, 0.0076, 0.023, -0.0337, -0.0088, 0.0106, 0.1418, -0.0157, -0.0251, 0.0121, -0.1602, 0.0105, 0.0241, -0.0038, 0.0239, -0.1055, -0.0054, -0.0658, -0.04, 0.0187, 0.099, 0.0372, -0.0159, -0.0113, 0.0156, 0.0953, -0.0334, -0.0971, -0.0237, 0.0123, 0.0271, -0.2827, -0.0086, 0.0576, 0.1455, 0.0356, -0.0489, 0.0021, 0.1179, -0.0209, 0.0518, 0.0245, -0.1199, 0.0084, 0.018, 0.0371, -0.3544, -0.0397, -0.0426, 0.0044, 0.0544, 0.0687, 0.0631, -0.0555, 0.0517, 0.0679, 0.0582, 0.045, -0.0167, 0.0014, 0.0793, 0.021, -0.3372, -0.0326, -0.0321, 0.055, -0.0962, -0.0316, 0.0132, 0.0309, 0.3101, -0.0624, 0.0419, 0.038, -0.113, -0.0231, 0.0135, 0.0323, 0.0158, 0.0779, -0.0368, 0.0279, 0.0072, -0.0174, 0.0178, 0.0026, -0.0547, 0.0492, 0.0599, 0.0472, -0.0817, -0.2665, 0.0267, -0.0128, 0.0237, -0.0342, -0.0213, -0.233, 0.0408, 0.0358, -0.0759, 0.2026, -0.0613, 0.0854, -0.3599, 0.0217, -0.0333, 0.0411, 0.0109, 0.0316, 0.0149, 0.0561, -0.0965, -0.0061, -0.1178, -0.0211, 0.0052, -0.0192, -0.0625, 0.0009, -0.0206, -0.0285, -0.0156, -0.0162, -0.0764, 0.1422, -0.0007, 0.0148, -0.0405, -0.0455, -0.0352, 0.0295, -0.0271, 0.0716, -0.0552, -0.0059, -0.0261, 0.0505, -0.0137, -0.0375, 0.0311, 0.0322, -0.0155, -0.1239, -0.0108, -0.0179, 0.0028, 0.0179, 0.0319, 0.0337, -0.0174, 0.0526, -0.0824, -0.0206, 0.0564, -0.0256, -0.2518, -0.0051, -0.0165, 0.2127, -0.0211, 0.0346, 0.2194, 0.0061, 0.0724, -0.086, -0.0035, -0.0597, 0.0046, 0.034, -0.0349, -0.0014, 0.038, -0.0007, -0.0122, -0.0254, 0.2632, -0.0164, 0.0878, -0.0171, 0.0378, 0.0345, 0.0363, 0.0725, 0.0042, -0.0122, 0.0374, -0.0208, -0.0353, -0.0012, 0.0018, -0.1729, 0.0315, -0.0483, 0.0162, 0.0853, -0.0103, -0.0061, -0.02, -0.0212, 0.0173, 0.0503, 0.0044, -0.087, 0.0067, -0.0138, 0.0075, 0.1034, -0.3706, -0.0438, 0.0262, 0.0773, -0.0511, -0.0179, 0.0647, 0.1465, 0.0517, 0.069, -0.0139, -0.025, -0.0271, -0.0258, -0.0259, -0.0265, 0.2192, 0.0188, -0.0608]\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","type of n;  <class 'int'>\n","Value of n:  20\n","type of emb:  <class 'list'>\n","PRINTING emb:  [-0.0714, -0.0961, -0.0543, -0.0089, -0.2126, 0.0534, -0.0409, -0.0517, 0.0936, 0.0113, -0.0077, 0.0641, 0.028, -0.0538, -0.0051, 0.0561, 0.0192, 0.0961, -0.1569, -0.0303, -0.0262, 0.1071, -0.0304, 0.0608, 0.0011, 0.0113, 0.0132, 0.0873, -0.1031, 0.0007, -0.0051, -0.0083, 0.0536, -0.01, -0.0242, -0.0871, 0.0095, -0.1401, 0.1626, 0.1108, -0.046, -0.0355, 0.0494, -0.0833, -0.1691, -0.0572, -0.0177, 0.0182, -0.0231, 0.0958, -0.104, 0.1079, -0.6455, 0.0117, 0.0832, 0.0286, 0.0462, -0.1509, -0.105, 0.0951, 0.0165, -0.0461, 0.1536, 0.0066, -0.0306, -0.0726, 0.0172, 0.0218, -0.0003, 0.1357, 0.0563, -0.0314, -0.0016, -0.043, -0.0665, 0.023, -0.0472, 0.0139, -0.0075, 0.0932, -0.0308, 0.0108, -0.056, -0.0816, 0.0244, 0.0641, 0.0434, 0.0032, 0.0787, -0.016, 0.0846, -0.0269, -0.0187, -0.0196, 0.0377, 0.0367, 0.1086, -0.0619, 0.0509, -0.0416, -0.1743, -0.0484, 0.0183, -0.0438, -0.0205, -0.0508, 0.2531, 0.1169, 0.2239, 0.0411, 0.0208, -0.0894, 0.0068, -0.0164, 0.116, -0.0687, 0.067, 0.0126, -0.0462, -0.2124, -0.0494, -0.0472, 0.073, 0.0385, 0.0644, 0.1669, -0.0893, -0.0032, 0.018, -0.0342, 0.0007, -0.0776, -0.0142, 0.0953, 0.0547, -0.2087, 0.0013, 0.0145, 0.0371, -0.0862, 0.0353, 0.0633, 0.0045, 0.0181, -0.0292, -0.048, 0.028, -0.0609, -0.0332, -0.0737, -0.1139, -0.0433, 0.0112, -0.0111, 0.0243, 0.0539, -0.0256, -0.0527, 0.0396, 0.0706, 0.0599, 0.014, 0.0007, 0.0058, -0.266, -0.0098, 0.0898, -0.0116, -0.0605, 0.0734, -0.0521, -0.0381, -0.0274, -0.0119, -0.0701, 0.0655, 0.1867, -0.0171, -0.001, -0.0367, 0.1, 0.026, -0.0961, -0.0065, -0.0965, 0.0331, -0.0445, -0.0174, 0.1617, 0.0258, -0.057, -0.0033, -0.064, -0.0004, -0.06, -0.0128, -0.0828, -0.1367, 0.2173, -0.0022, -0.0002, 0.0711, -0.0041, -0.0464, -0.0103, -0.0046, 0.0848, -0.0209, 0.0285, -0.1046, 0.0644, 0.0154, 0.0258, 0.0321, -0.0336, 0.0297, -0.1423, -0.0162, -0.1225, 0.0689, -0.0201, -0.0638, -0.0158, -0.0874, -0.0346, -0.0147, -0.0292, -0.012, -0.0561, -0.2855, 0.0462, 0.0129, 0.3125, -0.0379, -0.0039, 0.1701, 0.058, -0.1349, -0.1757, 0.1266, 0.0278, -0.0082, 0.0778, 0.0061, -0.0048, 0.069, -0.0048, 0.0676, 0.0007, 0.2385, -0.0287, -0.0028, -0.0125, 0.1112, -0.072, -0.0043, -0.0473, 0.0015, 0.0212, 0.0609, -0.0768, -0.0851, -0.0099, 0.0148, 0.0575, -0.0646, -0.0574, 0.0263, -0.057, -0.0036, 0.0167, -0.0115, 0.0138, -0.0927, 0.0076, 0.0136, -0.031, -0.0415, 0.0268, -0.0569, 0.0278, -0.0867, -0.0369, -0.0925, 0.0416, -0.0016, -0.1105, 0.0234, 0.0871, 0.0567, -0.0422, -0.0631, 0.0621, -0.0207, -0.1323, -0.053, 0.0177, 0.0577, 0.0075, -0.0248]\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","\n","Iteration: 100%|██████████████████████████████████| 1/1 [00:04<00:00,  4.26s/it]\u001b[A\n","03/28/2021 18:12:54 - INFO - bert_utils -     flaw_f1 = 0.8070175438596493\n","03/28/2021 18:12:54 - INFO - bert_utils -     flaw_precision = 0.9583333333333334\n","03/28/2021 18:12:54 - INFO - bert_utils -     flaw_recall = 0.696969696969697\n","03/28/2021 18:12:54 - INFO - bert_utils -     loss = 0.06222153455018997\n","Epoch: 100%|████████████████████████████████████| 25/25 [02:04<00:00,  4.97s/it]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"tags":[],"id":"qfBBCyjh64jp","outputId":"371f463c-3c0c-46de-bc6c-3156f9ba3085"},"source":["# Modified : Run this : \n","#Generator Train : Embedding Estimator train \n","!python bert_generator.py \\\n","--task_name sst-2\\\n","--do_train\\\n","--do_lower_case\\\n","--data_dir data/sst-2/\\\n","--bert_model bert-base-uncased\\\n","--max_seq_length 64\\\n","--train_batch_size 8\\\n","--learning_rate 2e-5\\\n","--num_train_epochs 25\\\n","--output_dir ./tmp/gnrt/\\\n","--no_cuda"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","03/28/2021 17:59:16 - INFO - bert_utils -   device: cpu n_gpu: 2, distributed training: False, 16-bits training: False\n","03/28/2021 17:59:17 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","03/28/2021 17:59:17 - INFO - bert_utils -   loading embeddings ... \n","03/28/2021 17:59:27 - INFO - bert_utils -   loading p index ...\n","03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n","03/28/2021 17:59:27 - INFO - bert_utils -   tokens: that loves its characters and communicates something rather beautiful about human nature\n","03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 7459, 2049, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 2008, 103, 2049, 3494, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2008, 7459, 103, 3494, 1998, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [7459, 2049, 103, 1998, 10639, 2015, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2049, 3494, 103, 10639, 2015, 2242, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3494, 1998, 103, 2242, 2738, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1998, 10639, 2015, 103, 2738, 3376, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [10639, 2015, 2242, 103, 3376, 2055, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2242, 2738, 103, 2055, 2529, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2738, 3376, 103, 2529, 3267, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3376, 2055, 103, 3267, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2055, 2529, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n","03/28/2021 17:59:27 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the same throughout\n","03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 12580, 8510, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 3464, 103, 8510, 2000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3464, 12580, 103, 2000, 3961, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [12580, 8510, 103, 3961, 1996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [8510, 2000, 103, 1996, 2168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2000, 3961, 103, 2168, 2802, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [3961, 1996, 103, 2802, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1996, 2168, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 17:59:27 - INFO - bert_utils -   *** Example ***\n","03/28/2021 17:59:27 - INFO - bert_utils -   tokens: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up\n","03/28/2021 17:59:27 - INFO - bert_utils -   token_ids: 21 18 22 23 24 18 25 26 27 28\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_ids: [101, 101, 103, 1996, 5409, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [101, 2006, 103, 5409, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 0, 0, 0, 0] [2006, 1996, 103, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 0, 0] [1996, 5409, 103, 18856, 17322, 2015, 1996, 0, 0, 0, 0, 0, 0, 0, 0, 0] [5409, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 103, 1996, 16587, 0, 0, 0, 0] [7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 103, 16587, 2071, 0, 0] [18856, 17322, 2015, 1996, 103, 2071, 2852, 24225, 0, 0, 0, 0, 0, 0, 0, 0] [1996, 16587, 103, 2852, 24225, 2039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [16587, 2071, 103, 2039, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [2071, 2852, 24225, 103, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","03/28/2021 17:59:27 - INFO - bert_utils -   ngram_labels: 21 18 22 23 24 18 25 26 27 28 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 17:59:27 - INFO - bert_utils -   ***** Running training *****\n","03/28/2021 17:59:27 - INFO - bert_utils -     Num examples = 5\n","03/28/2021 17:59:27 - INFO - bert_utils -     Num token vocab = 59\n","03/28/2021 17:59:27 - INFO - bert_utils -     Batch size = 8\n","03/28/2021 17:59:27 - INFO - bert_utils -     Num steps = 0\n","train_features:  <class 'list'>\n","train_features 1st element :  64\n","train_features length:  5\n","Type of ngram embeddings <class 'list'>\n","args max ngram length:  64\n","length of ngram_embeddings list:  5\n","printing it:    64\n","printing it:    64\n","printing it:    63\n","printing it:  [[0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0.0109, 0.0054, 0.0025, -0.063, -0.0564, 0.0296, -0.0682, 0.0355, -0.0558, 0.0059, -0.0383, 0.0515, 0.0712, -0.0886, 0.1198, -0.0334, 0.0043, 0.0388, 0.0457, -0.0428, 0.0495, -0.0072, 0.0304, -0.0174, 0.0421, 0.0032, -0.039, -0.0322, -0.0113, 0.0603, 0.0074, 0.0102, 0.0481, -0.0294, 0.0271, 0.005, -0.0374, 0.0533, -0.1492, 0.0262, 0.0029, -0.0423, 0.0086, -0.0379, -0.0297, -0.0117, 0.0891, -0.5834, -0.0196, 0.038, 0.0106, -0.023, -0.028, 0.0464, 0.009, -0.0389, -0.033, -0.1084, 0.0001, 0.0224, 0.1006, 0.0005, 0.0152, -0.0146, -0.0575, -0.0436, -0.0181, -0.0158, -0.0427, 0.0685, -0.0116, 0.029, 0.0035, -0.1194, -0.0918, 0.0197, 0.1237, -0.0245, -0.3306, -0.0037, -0.0307, -0.025, 0.0059, 0.1157, -0.0114, -0.0106, -0.0137, 0.0109, 0.0819, 0.052, 0.0564, 0.0283, -0.0403, 0.0134, -0.0875, -0.0785, -0.0019, 0.0188, 0.0821, -0.095, -0.0039, 0.0945, -0.0765, 0.1313, -0.0581, 0.0405, -0.0103, 0.0653, 0.0289, 0.0409, -0.0256, -0.0071, 0.0012, -0.0567, -0.2729, 0.0702, -0.0213, 0.0021, 0.1257, 0.0059, 0.0772, 0.0403, -0.0276, -0.101, -0.0033, -0.0749, -0.0207, -0.0789, 0.0385, -0.0658, -0.1691, -0.0115, -0.0056, 0.1989, 0.1064, -0.0167, 0.0653, 0.0628, 0.3884, -0.0158, -0.0042, 0.0209, -0.0511, -0.0213, -0.0188, -0.009, 0.0099, -0.023, -0.0427, 0.0403, 0.0124, -0.0669, -0.0269, -0.0124, 0.0256, -0.041, -0.0619, 0.0162, -0.1176, -0.2902, 0.0084, 0.1346, -0.0781, 0.0389, -0.0321, -0.0553, -0.0213, -0.0142, 0.0333, 0.0863, -0.1304, 0.1478, -0.5894, 0.0465, 0.0001, 0.0436, -0.0236, 0.0123, 0.0241, 0.02, -0.2658, -0.0006, -0.0608, 0.3386, -0.0067, 0.0549, 0.0206, -0.0162, 0.0078, -0.0411, 0.0081, 0.0067, -0.0451, 0.1383, 0.0427, -0.0006, -0.0881, 0.0111, 0.0129, -0.0609, -0.0975, -0.0001, 0.1469, 0.0451, 0.0174, -0.0055, -0.0166, -0.0333, 0.0994, 0.055, -0.0201, -0.333, -0.0084, 0.155, -0.0631, 0.0423, 0.0196, -0.0049, 0.0698, -0.0575, -0.1492, 0.026, 0.0842, -0.0507, 0.1491, 0.039, 0.0039, 0.2166, -0.0059, 0.013, -0.0529, -0.0177, 0.1542, -0.1744, 0.0889, -0.074, -0.0335, 0.0167, -0.0347, 0.0433, -0.064, 0.0335, -0.0394, 0.0444, 0.3293, -0.0081, -0.0297, 0.0637, 0.0484, -0.3762, 0.0191, -0.0618, 0.0386, 0.0234, 0.0003, 0.0235, 0.033, 0.0034, -0.0156, -0.1348, 0.0016, 0.0838, -0.0008, -0.0768, -0.0085, -0.0156, 0.0373, -0.0186, 0.0309, 0.0197, -0.0242, 0.042, 0.0556, 0.0171, 0.0195, -0.1021, 0.0454, -0.031, -0.0079, 0.0583, -0.0137, -0.0559, -0.0094, 0.0233, 0.04, 0.0042, -0.0218, -0.0945, -0.0154, -0.029, -0.0141, 0.008, 0.1221, 0.0558, -0.1314], [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259], [0.0143, -0.0543, -0.1051, 0.0771, -0.0517, 0.159, 0.0023, 0.0041, 0.1165, -0.2022, -0.0472, -0.094, 0.0627, -0.0514, 0.1361, -0.0619, 0.0916, -0.1894, 0.3032, -0.0725, -0.0115, -0.0403, 0.0834, 0.0448, 0.0543, 0.1341, 0.0924, -0.1008, 0.0358, 0.0204, 0.0382, -0.1205, -0.1139, 0.0288, 0.1009, 0.084, -0.0333, -0.1662, 0.0438, -0.0211, 0.119, -0.0011, -0.1672, -0.1107, -0.0335, 0.0294, 0.0254, -0.018, -0.0518, 0.0907, 0.196, 0.0684, -0.7173, -0.1312, 0.0114, -0.0293, 0.0415, -0.0909, -0.0204, 0.019, -0.0496, 0.0192, 0.0837, -0.018, -0.0228, -0.2671, 0.1097, 0.1041, -0.0912, -0.0907, 0.001, -0.0216, -0.0183, -0.1755, 0.0833, 0.0609, 0.1786, -0.1091, -0.1792, 0.0464, -0.033, 0.0056, -0.062, -0.2019, -0.0117, -0.1002, -0.0035, -0.1013, 0.148, -0.0883, 0.0249, 0.0818, -0.1695, -0.1593, -0.0177, 0.0438, 0.018, -0.0386, 0.0306, 0.0984, -0.178, 0.1209, 0.0429, 0.0647, 0.0041, -0.0447, 0.2673, 0.0551, -0.0338, 0.0052, -0.2155, 0.1378, -0.071, -0.1129, -0.082, -0.1429, -0.1168, 0.1869, 0.0499, -0.2497, 0.0585, 0.0677, -0.1465, 0.1205, 0.1833, 0.2309, -0.1143, 0.1156, -0.0714, 0.1972, 0.0479, -0.0762, 0.0756, 0.1547, 0.1854, -0.4435, -0.0086, 0.0655, -0.0631, -0.2148, 0.0738, 0.1081, -0.1484, 0.2601, 0.0103, -0.1344, 0.0201, -0.0311, -0.0352, 0.069, 0.102, -0.1283, -0.0474, 0.1745, 0.0474, 0.1155, -0.0341, 0.2, -0.0765, 0.005, -0.0102, 0.0832, -0.0961, 0.03, 0.0569, 0.1179, 0.0911, -0.1376, 0.0509, -0.1254, 0.2475, -0.1036, -0.151, -0.0602, -0.0197, -0.0109, 0.2656, 0.1874, -0.124, -0.125, 0.0859, 0.0337, -0.145, -0.0752, -0.0396, 0.0002, -0.1337, -0.0621, 0.1229, 0.0378, 0.0574, 0.1162, -0.0134, -0.0242, -0.0272, 0.0871, -0.0428, -0.0971, 0.1347, 0.0422, -0.1023, 0.1704, -0.0811, 0.1585, 0.2222, 0.1999, 0.2117, -0.0486, 0.0009, 0.0811, 0.041, 0.0708, -0.0067, -0.0618, -0.0936, -0.0176, -0.103, -0.0052, 0.1627, 0.079, 0.082, -0.2438, -0.1182, -0.016, 0.1215, -0.0091, -0.0517, -0.03, -0.0589, -0.2315, -0.1286, -0.1199, 0.2866, 0.0122, 0.1046, 0.0232, 0.0164, -0.0132, -0.2337, -0.0218, -0.0405, -0.019, -0.0144, 0.0227, 0.0262, -0.0486, 0.0407, 0.0409, -0.0732, 0.4246, 0.103, 0.0808, -0.1494, 0.1471, 0.2057, -0.0533, -0.1855, 0.0616, 0.0348, 0.121, 0.271, -0.0521, 0.1651, 0.025, -0.0339, -0.07, 0.1155, -0.0905, -0.2679, 0.0552, -0.0485, 0.0351, 0.0582, -0.0744, -0.0191, 0.0294, 0.0786, -0.0611, -0.0638, 0.1417, 0.0177, -0.1093, -0.0368, -0.1943, -0.024, 0.1895, 0.088, -0.0517, -0.2148, 0.0574, 0.0077, 0.1147, 0.0485, -0.05, 0.176, -0.1182, 0.0062, 0.1549, -0.0883, -0.0034], [-0.1516, 0.0261, -0.1214, -0.0554, 0.1189, -0.0318, 0.0663, 0.007, 0.0326, 0.2224, -0.1209, -0.023, -0.0409, -0.1631, 0.1822, -0.1053, 0.127, -0.0679, 0.0279, -0.0594, -0.2264, 0.1447, 0.1304, -0.0832, 0.0398, 0.1593, -0.0254, 0.0029, -0.0111, 0.0802, -0.0959, -0.0435, 0.1217, -0.0023, 0.0125, 0.0479, -0.1467, -0.0639, 0.0946, 0.0305, -0.0139, -0.0694, -0.0195, -0.0004, -0.0273, -0.1561, 0.0345, -0.0749, 0.1438, 0.0614, -0.0707, -0.0107, -0.6549, 0.2256, 0.0823, 0.0814, -0.1385, -0.3124, 0.2189, 0.0438, 0.2011, 0.0777, -0.1083, -0.1452, 0.1372, 0.0356, -0.1708, -0.0939, -0.1239, -0.1943, -0.053, -0.0218, 0.1476, -0.2356, 0.0103, -0.0558, -0.0901, -0.0388, 0.0276, -0.0392, 0.0675, 0.0791, -0.1464, -0.1893, -0.0815, 0.0636, -0.0182, -0.2243, 0.2415, 0.0856, -0.009, -0.0234, 0.1402, -0.0363, -0.042, -0.3344, 0.2943, 0.0946, -0.2424, 0.0413, 0.1328, 0.181, -0.0299, 0.1109, 0.0168, 0.2168, -0.0956, 0.0227, 0.0275, 0.2631, -0.0734, 0.1344, -0.18, -0.2053, 0.1573, 0.0621, -0.0574, 0.0598, 0.1778, -0.4325, 0.0107, -0.0715, -0.2109, 0.1728, 0.0044, -0.2923, -0.1781, -0.0571, -0.0941, 0.1887, -0.0802, -0.1051, 0.1524, 0.0908, -0.0678, -0.0649, 0.1076, -0.0581, 0.213, -0.0256, 0.196, -0.1287, -0.1607, 0.0976, -0.1216, 0.0457, -0.0005, -0.1337, 0.0939, 0.081, -0.0775, -0.0077, -0.1135, 0.0251, -0.0352, 0.0153, 0.1876, 0.2257, 0.1956, -0.0751, 0.2021, -0.1221, 0.0264, 0.1743, 0.2078, -0.0412, 0.0684, 0.1525, -0.3519, -0.1587, -0.1308, 0.0614, -0.0619, 0.0761, 0.0409, -0.0189, 0.2209, -0.1827, -0.1356, 0.2158, 0.1883, 0.0224, -0.0051, -0.02, 0.0268, 0.0857, -0.1256, -0.0379, -0.2222, 0.0388, -0.2013, -0.1436, -0.2269, 0.1731, 0.0511, 0.3046, 0.0562, -0.1082, 0.2207, -0.0739, -0.1245, 0.2508, 0.0067, -0.1141, 0.2315, 0.0051, -0.0013, 0.0404, -0.1076, -0.0516, 0.0938, 0.1965, -0.1102, -0.0337, 0.1085, -0.0337, -0.2225, 0.0302, -0.1936, -0.199, 0.1831, 0.0049, 0.0237, -0.2115, -0.122, -0.1782, 0.0604, -0.1211, 0.0262, -0.1189, -0.2392, -0.0616, 0.3307, -0.3224, 0.0041, -0.0311, 0.0268, -0.0678, -0.2845, 0.0837, -0.0811, -0.0565, 0.1536, -0.007, 0.009, -0.0248, -0.0004, 0.102, -0.1167, 0.3847, 0.0886, 0.0723, 0.1374, 0.0931, 0.0807, -0.2107, -0.1093, 0.1742, 0.0211, 0.0566, 0.015, 0.114, 0.0128, -0.0062, -0.7042, 0.0187, -0.01, -0.0832, -0.1644, 0.1549, -0.2233, 0.0695, -0.0517, 0.0135, 0.0209, 0.0536, -0.1664, -0.0425, 0.1485, 0.0098, 0.222, -0.0455, -0.0158, -0.2935, 0.0842, 0.0782, 0.0276, 0.0811, 0.1665, -0.0177, -0.009, 0.2296, -0.0601, 0.0667, -0.0205, 0.0278, 0.0135, 0.1152, 0.068, -0.1444], [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1237, 0.0301, 0.0248, -0.0303, 0.0174, 0.0063, 0.0184, 0.0217, -0.0257, 0.035, -0.0242, 0.0029, 0.0188, -0.057, 0.0252, -0.021, -0.0008, 0.036, -0.0729, -0.0665, 0.0989, 0.0676, 0.0852, -0.0089, 0.0313, -0.0069, -0.0032, -0.0462, 0.0497, 0.0261, 0.0268, -0.031, -0.1361, -0.0062, 0.0375, -0.032, -0.0106, 0.0534, -0.0187, 0.0638, 0.0094, 0.0047, -0.053, 0.0093, -0.0087, 0.0004, 0.0493, -0.6296, 0.0222, 0.019, 0.0268, -0.0426, 0.0057, -0.1683, 0.0244, -0.0213, -0.0181, 0.0421, -0.0309, -0.0089, 0.0032, 0.0108, -0.0049, 0.0258, 0.0278, -0.0163, 0.02, 0.0164, -0.0954, -0.0032, 0.0043, 0.0104, -0.0088, 0.0007, 0.035, -0.0206, -0.0083, -0.0114, -0.1869, 0.0258, 0.001, 0.0085, 0.0151, 0.2125, 0.0071, 0.0319, -0.0482, 0.0621, 0.0626, 0.0159, -0.0013, 0.0087, 0.0686, -0.0034, 0.0238, -0.0452, -0.0198, 0.0112, 0.0109, -0.1022, -0.0272, 0.2337, -0.0465, 0.1592, -0.0407, -0.1029, -0.0487, -0.0676, 0.0676, -0.0328, 0.0323, 0.0077, 0.019, 0.0017, -0.2974, 0.0011, -0.0356, 0.0693, -0.048, -0.0821, -0.0644, -0.0284, -0.0191, -0.0233, 0.0353, -0.0463, 0.0656, 0.0019, -0.0212, -0.0309, -0.3534, -0.0309, 0.0076, -0.0419, 0.0457, -0.0306, 0.0357, 0.0667, 0.3659, 0.0149, -0.0443, 0.0068, -0.0378, 0.0146, 0.0215, 0.1081, 0.0124, -0.0437, -0.043, 0.0258, 0.0213, -0.0309, -0.0018, -0.0067, 0.0172, 0.0089, -0.0171, 0.0275, -0.0518, -0.184, -0.013, -0.0241, 0.0526, -0.028, 0.0051, 0.0163, -0.0165, 0.0161, 0.1237, 0.0804, -0.0789, 0.0386, -0.3892, 0.0157, -0.0246, 0.0477, -0.0045, -0.0214, 0.0173, -0.0191, -0.1382, -0.0111, 0.0712, 0.1514, 0.0291, 0.0555, -0.0039, 0.0028, -0.0277, -0.0275, -0.0177, -0.0338, -0.0372, 0.2071, 0.046, -0.0294, 0.0435, -0.0169, -0.0121, 0.0253, 0.0198, 0.0918, 0.0193, 0.0668, 0.0288, 0.004, -0.0439, -0.0302, 0.0064, 0.0364, 0.0543, -0.0338, 0.0159, 0.0617, -0.0941, -0.0086, -0.0092, 0.03, -0.0241, -0.035, -0.0621, 0.0175, 0.0374, 0.0034, 0.0344, 0.1286, -0.0267, 0.1861, 0.0489, -0.0032, 0.018, -0.0228, 0.2414, -0.0935, 0.0612, -0.0209, 0.0136, 0.0392, -0.0135, -0.0253, 0.0335, 0.0095, 0.0419, 0.0076, 0.4522, -0.0188, 0.0233, -0.0474, 0.0159, -0.009, 0.0265, 0.0336, 0.0221, 0.0472, 0.0048, 0.0962, 0.0344, -0.0515, -0.0087, -0.098, -0.0288, 0.0377, 0.0202, -0.2979, -0.0387, -0.0198, -0.0161, -0.0045, 0.0087, -0.0387, 0.0421, 0.0383, 0.0258, 0.0069, -0.0298, -0.0198, -0.0152, 0.0033, 0.0075, 0.0358, -0.0155, -0.0111, 0.076, -0.0452, 0.0697, 0.0299, -0.0029, -0.0348, -0.027, 0.0351, 0.0559, 0.0591, 0.1559, -0.0254, -0.0259], [-0.0455, 0.2169, -0.045, -0.1449, 0.0315, -0.0419, 0.2311, 0.2965, 0.0666, 0.0629, 0.0198, 0.113, -0.2449, 0.0175, 0.0883, 0.0854, 0.0701, -0.1096, -0.028, -0.1094, -0.3251, 0.0504, -0.0819, 0.0199, -0.0759, -0.0156, 0.0021, 0.2765, -0.0474, -0.1035, 0.0427, -0.0551, 0.0641, -0.0611, -0.0518, 0.1525, -0.0606, 0.0785, 0.0682, 0.1768, -0.0766, 0.0349, 0.0267, 0.0824, 0.0422, 0.0942, -0.0785, -0.075, 0.1675, -0.0544, 0.2628, 0.0023, -0.6394, 0.1559, 0.1765, 0.0651, -0.0255, 0.0073, 0.0251, 0.0821, -0.0506, 0.0795, 0.0042, -0.0591, -0.0546, 0.0919, -0.1361, 0.1442, -0.0495, -0.1155, 0.0202, 0.0177, 0.0534, 0.0331, -0.0241, -0.2127, 0.1117, -0.0832, -0.0365, 0.2927, -0.0237, 0.147, -0.0489, -0.2541, 0.2297, 0.0248, 0.0782, -0.2193, 0.072, -0.084, -0.1034, -0.1727, -0.0788, 0.0693, -0.0286, -0.1217, 0.0104, -0.0005, 0.0687, 0.0051, -0.1218, -0.1055, -0.103, 0.1064, -0.253, 0.074, -0.0645, 0.0713, 0.0915, 0.0479, -0.0029, 0.0341, -0.0275, -0.1296, -0.0062, -0.0279, -0.0489, -0.0853, 0.0262, -0.369, 0.0512, -0.2761, 0.0339, 0.0558, -0.0562, 0.1133, -0.123, 0.0184, 0.1064, -0.1874, -0.0688, 0.1317, -0.0355, -0.0461, -0.1048, 0.0028, -0.0095, -0.0501, 0.0169, -0.066, 0.0848, 0.0103, -0.1681, 0.0017, 0.0057, -0.1019, -0.0982, 0.0543, 0.1098, 0.2491, -0.0535, 0.0186, 0.0757, -0.0243, 0.1771, 0.0988, 0.0174, -0.0504, 0.1588, -0.1371, 0.1396, -0.2422, -0.0725, 0.0042, 0.016, 0.0036, 0.0601, -0.0851, -0.1195, -0.2016, -0.0865, 0.0556, -0.0251, 0.0856, -0.058, -0.0954, 0.2181, 0.0978, 0.1717, -0.048, 0.0292, 0.1121, -0.1031, -0.0349, 0.1859, 0.0179, -0.0794, 0.1225, -0.3095, 0.0048, -0.1176, -0.0507, -0.1813, 0.0849, 0.0489, 0.3092, 0.1104, 0.106, 0.1833, -0.1318, -0.1152, 0.0651, 0.2678, -0.0722, 0.0947, -0.1414, 0.0365, -0.1402, -0.009, 0.0396, -0.0218, -0.0268, 0.0875, 0.0372, -0.045, 0.1391, -0.0775, 0.1772, -0.3194, -0.083, 0.072, -0.0003, 0.075, -0.0854, 0.0564, 0.1475, -0.1642, 0.0939, -0.0234, -0.1082, 0.0726, 0.004, 0.2956, -0.07, -0.1524, -0.0799, 0.0644, 0.1934, -0.2858, 0.1708, 0.0498, -0.1527, -0.0686, 0.1355, 0.2039, -0.0426, 0.0943, 0.1789, 0.0321, 0.3679, 0.174, 0.1584, -0.1019, -0.1073, 0.4471, 0.2862, -0.0626, 0.0859, -0.1308, 0.2145, -0.0207, -0.0537, -0.079, 0.0873, -0.7401, 0.1595, -0.039, -0.0173, -0.0115, 0.0917, -0.1477, 0.006, 0.1513, -0.0682, -0.0532, 0.0731, -0.0389, -0.0997, -0.0045, 0.0345, -0.0181, 0.0577, -0.1213, -0.1275, -0.025, 0.0256, -0.0919, 0.1334, -0.0633, 0.1814, -0.0024, 0.0277, -0.0574, -0.2292, -0.0546, -0.0158, -0.1111, 0.0665, -0.0223, -0.115], [-0.0452, -0.1182, -0.0588, -0.0678, 0.0659, -0.0131, 0.0226, 0.0341, 0.0073, -0.0948, 0.0425, 0.1141, -0.0269, 0.0057, -0.0998, 0.0446, -0.0107, -0.046, -0.0858, 0.1738, -0.013, 0.0453, -0.1062, 0.1329, 0.0352, 0.0535, 0.0053, 0.1091, -0.0301, -0.0369, -0.0765, -0.0133, -0.0308, 0.1176, 0.0911, 0.0312, 0.049, -0.0843, 0.1036, -0.0138, 0.0216, -0.0034, 0.0214, -0.0578, -0.1358, 0.0589, 0.0092, 0.0299, 0.0109, -0.0453, 0.0458, 0.0886, -0.5181, 0.0412, 0.0049, -0.0017, 0.0471, -0.1153, -0.2281, -0.1125, 0.0473, -0.0311, -0.0006, -0.0075, -0.1309, -0.0087, 0.0068, 0.0634, 0.0376, 0.0001, 0.0125, -0.0212, -0.0006, -0.1096, -0.056, 0.0224, -0.0342, 0.06, -0.0298, 0.2647, 0.0342, 0.0083, -0.006, -0.3098, -0.0, 0.0341, -0.0454, 0.0164, -0.5405, 0.0691, -0.0402, 0.0763, -0.0947, -0.0367, -0.0085, 0.0024, 0.0071, -0.1708, -0.0697, -0.0473, -0.1802, -0.0478, 0.002, 0.0089, 0.0125, 0.0277, -0.1764, 0.0552, 0.0754, -0.0075, -0.0053, 0.0124, -0.013, 0.0183, -0.2257, -0.004, 0.1549, 0.0242, -0.0479, -0.2813, -0.0232, -0.0071, 0.0019, 0.0709, 0.0374, 0.2256, -0.1018, 0.232, -0.037, 0.0355, -0.0028, -0.0077, -0.0419, 0.0492, 0.0353, -0.2993, -0.0554, -0.0579, -0.0191, -0.0706, 0.006, -0.0416, 0.0982, 0.3701, 0.004, 0.1287, 0.1455, 0.0185, -0.0102, -0.1441, -0.1069, 0.0642, 0.087, 0.0936, 0.0824, -0.042, -0.1653, 0.0034, -0.0284, -0.0616, 0.0449, 0.0362, 0.0063, -0.1503, 0.1723, 0.014, 0.0642, 0.1124, 0.1234, 0.0643, -0.1091, 0.0408, 0.1279, -0.0234, 0.1254, 0.1637, 0.2048, -0.7466, 0.0269, -0.019, 0.0579, 0.0034, 0.093, 0.1473, 0.0041, -0.3608, -0.1192, -0.1101, 0.6456, 0.0752, -0.07, -0.055, 0.0097, 0.0175, 0.0034, 0.0171, -0.0259, -0.0431, 0.2949, 0.0442, -0.0568, -0.0246, -0.0825, -0.0895, -0.0354, 0.0302, -0.0512, -0.1258, -0.0261, 0.039, 0.0185, 0.0146, 0.0754, -0.0044, -0.0718, 0.0796, -0.0583, 0.0494, -0.0567, 0.0146, 0.0428, -0.0613, 0.0115, -0.0007, 0.0787, -0.0534, 0.0449, 0.1126, 0.0175, 0.0473, 0.2723, -0.0599, 0.2096, -0.0041, -0.0395, -0.4713, 0.0233, -0.0937, -0.2639, 0.0466, -0.1424, 0.001, -0.0245, -0.0115, -0.0302, 0.0034, -0.0401, 0.0055, 0.013, 0.356, 0.069, -0.0326, -0.06, 0.0033, 0.23, 0.0421, -0.1639, 0.0208, 0.0544, 0.0217, -0.049, -0.0096, -0.0298, 0.0323, -0.7646, -0.0145, 0.1061, -0.1425, -0.0049, -0.0913, 0.0375, -0.0735, -0.0046, 0.0877, 0.0725, -0.0258, -0.0401, -0.0095, -0.0906, -0.1003, 0.2793, -0.0684, 0.0113, -0.1215, -0.0354, -0.047, -0.0268, 0.0227, -0.1376, 0.0929, 0.0013, 0.0696, 0.1652, -0.068, 0.0175, -0.0667, -0.0633, 0.2623, 0.0747, 0.025], [-0.0326, 0.0195, 0.0364, -0.1522, -0.0004, -0.1664, -0.1484, -0.0419, -0.267, -0.0867, -0.0967, 0.1412, 0.049, -0.1076, 0.1205, 0.1043, 0.0353, 0.1153, -0.1742, -0.2729, -0.4423, -0.0395, 0.2611, -0.0571, -0.0127, -0.1157, 0.0721, -0.0517, -0.2769, -0.0741, 0.1076, 0.1005, 0.0459, 0.2092, -0.215, 0.1929, -0.0161, -0.2282, -0.0651, -0.0653, 0.1085, -0.1041, -0.0867, -0.033, 0.0878, -0.2261, -0.0212, -0.0315, -0.2479, 0.0607, -0.12, 0.0408, -0.8369, -0.1042, -0.1361, 0.0352, 0.0124, 0.1088, 0.0509, 0.0047, 0.0219, 0.0814, -0.0404, -0.0903, -0.0295, -0.0709, -0.0619, -0.0012, -0.1265, 0.1139, 0.0489, -0.0884, -0.0212, -0.0213, 0.2818, 0.0265, 0.0981, -0.5025, -0.2206, -0.0274, 0.0092, 0.0006, 0.1065, -0.282, 0.1825, -0.2823, -0.0556, 0.0994, 0.0301, 0.2853, -0.1554, -0.0155, 0.1182, -0.2357, 0.0372, 0.0412, -0.0469, -0.1763, -0.0437, -0.061, -0.1358, 0.0631, 0.0861, -0.0006, -0.0699, -0.0475, -0.1164, 0.1694, 0.0653, 0.1353, 0.0626, -0.056, -0.036, -0.0176, -0.2072, 0.1721, 0.093, 0.1898, -0.1265, -0.292, 0.1351, -0.1685, 0.0872, 0.0529, 0.1249, 0.1556, -0.0189, 0.1228, -0.0232, 0.0005, 0.0116, -0.0448, 0.1589, 0.0311, 0.2283, 0.2629, 0.1098, -0.2181, 0.0711, 0.0021, 0.0404, -0.0385, 0.1605, 0.1123, 0.021, -0.0397, -0.2564, -0.2977, 0.1308, -0.0305, -0.1551, 0.2805, -0.0386, -0.0466, -0.2589, 0.179, -0.0679, 0.1405, -0.212, 0.0176, -0.0757, -0.26, -0.0182, 0.0827, -0.1437, 0.03, -0.0128, -0.1292, 0.1008, 0.035, 0.2977, 0.2411, -0.0608, -0.1024, 0.1391, -0.1677, 0.3231, -0.1087, 0.0269, -0.0593, 0.1357, -0.0963, -0.1021, -0.1126, -0.0657, -0.4809, 0.0893, -0.1207, -0.1273, -0.0757, -0.0831, 0.2692, -0.0536, -0.1892, 0.0861, -0.1484, 0.0321, 0.1093, 0.0251, 0.1879, -0.0447, 0.0748, 0.1338, -0.0289, 0.0128, 0.0625, 0.173, -0.1717, -0.2312, 0.0046, 0.0477, 0.1302, 0.0672, -0.1898, -0.0803, 0.1801, -0.0964, 0.1186, 0.0623, 0.0322, 0.0902, -0.072, -0.0564, 0.1925, -0.2658, -0.1864, -0.105, 0.0615, -0.0711, 0.0126, 0.0044, 0.13, 0.1656, -0.1325, 0.0699, -0.0184, 0.0919, -0.0046, -0.1545, -0.0473, -0.0922, 0.1955, -0.0724, 0.4568, 0.0782, 0.2548, 0.0745, -0.1355, -0.0421, 0.372, 0.1428, 0.0172, 0.1141, -0.1431, 0.0519, -0.1439, 0.0232, 0.0016, -0.0564, 0.0573, 0.0371, 0.0915, 0.0967, -0.1458, -0.1127, -0.212, -0.0881, -0.1117, -0.1259, -0.0701, -0.0795, 0.1875, 0.0183, -0.0661, -0.0428, -0.0551, 0.0231, -0.297, -0.0825, -0.0378, 0.0166, -0.0777, -0.0733, 0.1054, -0.0045, 0.1439, 0.0198, 0.3949, -0.0875, -0.1, 0.1736, -0.0556, -0.0239, -0.1806, -0.2101, 0.1066, -0.1304, 0.0249, 0.0083, -0.1229], [-0.0657, 0.046, 0.0251, -0.0332, 0.0141, -0.221, -0.0121, -0.0384, 0.0709, -0.0542, 0.06, 0.0671, 0.0604, 0.1536, -0.0443, 0.0414, 0.0221, 0.0859, -0.0448, -0.0593, -0.0176, -0.036, 0.0366, -0.296, 0.0502, -0.2343, 0.047, -0.0906, 0.0382, -0.2112, 0.0204, -0.0113, 0.2169, -0.0023, 0.0375, 0.0031, 0.0218, 0.0073, -0.0524, 0.0786, 0.0447, -0.0189, -0.1872, 0.0252, -0.0965, -0.0778, -0.1135, -0.0199, 0.0901, 0.0191, -0.0134, -0.001, -0.6779, -0.0296, -0.0169, 0.0048, 0.0055, 0.0217, 0.0459, 0.1198, -0.1142, -0.0131, 0.1316, -0.034, 0.1073, -0.0355, -0.0246, 0.0521, -0.055, 0.0158, 0.03, 0.0168, 0.0335, 0.1986, 0.072, -0.0165, 0.0566, -0.1098, -0.0796, -0.0808, -0.0356, 0.0266, 0.1186, -0.2048, -0.0201, -0.1187, -0.0025, -0.0004, -0.0488, 0.0154, -0.0453, -0.0443, -0.0276, -0.1272, -0.0878, 0.007, 0.0324, -0.0797, -0.0007, -0.0803, -0.1543, -0.01, 0.0158, 0.0556, -0.1504, -0.0414, 0.216, -0.0864, 0.149, -0.0059, 0.0869, 0.2644, -0.023, 0.0089, -0.0612, 0.0043, 0.0118, 0.1146, -0.017, -0.3391, 0.117, 0.0525, 0.0654, 0.188, 0.0733, 0.1355, -0.0028, -0.0488, -0.1325, -0.0084, 0.0245, -0.0142, -0.0509, 0.0929, 0.0061, -0.2864, 0.1034, -0.0612, -0.0258, -0.0505, -0.0298, 0.0362, 0.0213, 0.2663, -0.0181, -0.059, 0.0939, -0.0895, 0.0385, -0.0212, 0.1066, 0.1416, 0.0303, 0.0044, -0.085, -0.0252, -0.0332, 0.0527, -0.032, -0.0311, -0.0576, 0.0001, 0.0041, 0.0065, -0.1985, 0.0077, 0.1188, 0.008, 0.0286, -0.0599, 0.1298, 0.0204, 0.0183, 0.1724, 0.0711, 0.0252, 0.1688, -0.0282, 0.0433, 0.0193, -0.0003, 0.0717, 0.0823, 0.0143, 0.0575, -0.2815, -0.0972, -0.1434, 0.0879, -0.0272, -0.041, 0.0076, 0.0508, 0.0021, 0.0043, 0.0017, -0.0012, -0.0067, 0.1853, 0.0582, 0.0609, -0.0207, 0.0806, 0.0253, 0.0604, 0.0744, -0.1165, -0.0971, -0.0967, 0.0329, 0.0521, 0.0893, 0.0125, -0.0923, -0.0302, 0.014, -0.1939, -0.0287, 0.0101, 0.1531, 0.0165, -0.1121, 0.0018, 0.1513, 0.0316, -0.094, -0.0563, -0.0422, 0.0181, 0.1168, -0.1048, 0.0031, 0.2662, 0.0742, 0.0081, 0.1066, -0.0511, 0.0307, -0.1781, 0.0602, -0.0484, 0.0319, 0.0762, 0.0056, -0.0201, -0.0665, -0.0476, 0.0214, -0.0166, 0.3347, 0.0167, 0.0944, -0.0148, -0.022, -0.2036, 0.0307, -0.1116, -0.0157, 0.0638, 0.017, -0.1067, 0.0272, -0.0475, -0.0247, -0.0591, -0.0323, 0.1388, 0.0192, -0.0207, 0.0467, 0.0291, -0.0942, -0.0741, -0.0043, -0.0332, 0.051, -0.0713, 0.0526, 0.004, -0.0196, -0.1, -0.1013, -0.0433, -0.07, -0.0107, 0.0362, -0.0375, 0.1955, -0.1428, 0.0313, -0.0333, 0.0039, 0.0837, 0.0162, -0.1025, 0.0986, -0.0369, 0.2393, 0.0885, -0.0732], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","printing it:    64\n","printing it:    64\n","maximum no. of cols:  300\n","maximum no. of rows:  64\n","bert_generator.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  all_ngram_embeddings = torch.tensor(ngram_embeddings_padded, dtype=torch.float)\n","03/28/2021 17:59:28 - INFO - bert_utils -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/28/2021 17:59:28 - INFO - bert_utils -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmphb0ui5lq\n","03/28/2021 17:59:31 - INFO - bert_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/28/2021 17:59:34 - INFO - bert_utils -   Weights of BertForNgramClassification not initialized from pretrained model: ['converter.weight', 'converter.bias']\n","03/28/2021 17:59:34 - INFO - bert_utils -   Weights from pretrained model not used in BertForNgramClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:104: UserWarning: \n","GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n","The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n","If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n","\n","  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n","Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n","Iteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"bert_generator.py\", line 432, in <module>\n","    main() \n","  File \"bert_generator.py\", line 315, in main\n","    loss = model(ngram_ids, ngram_masks, ngram_embeddings) \n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\", line 155, in forward\n","    \"them on device: {}\".format(self.src_device_obj, t.device))\n","RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k0xVqF0v64jp"},"source":["Generate attacks - DISP random attacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"id":"NBgKI3-O64jq","outputId":"ba42cb20-fd91-44d3-ecfe-5cac1249ead1"},"source":["!python bert_random_attacks.py \\\n","--task_name sst-2 \\\n","--do_lower_case \\\n","--data_dir data/sst-2/add_1/dev_attacks.tsv \\\n","--bert_model bert-base-uncased \\\n","--max_seq_length 128 \\\n","--output_dir ./data/sst-2/add_1/\\\n","#--output_dir_attacks ./data/sst-2/add_1/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","03/28/2021 20:01:47 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","03/28/2021 20:01:47 - INFO - bert_utils -   *** Example ***\n","03/28/2021 20:01:47 - INFO - bert_utils -   tokens: that loves its charcatres and communicates something rather handsome about human ntaure\n","03/28/2021 20:01:47 - INFO - bert_utils -   token_ids: 1 2 3 4 5 6 7 8 9 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 20:01:47 - INFO - bert_utils -   *** Example ***\n","03/28/2021 20:01:47 - INFO - bert_utils -   tokens: remains utterly satisfied to remain the smea throughout\n","03/28/2021 20:01:47 - INFO - bert_utils -   token_ids: 13 14 15 16 17 18 19 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/28/2021 20:01:47 - INFO - bert_utils -   Loading word embeddings for generating attacks... \n","output_file ./data/sst-2/add_1/disc_for_attacks_outputs.tsv\n","attacks:   0%|                                            | 0/5 [00:00<?, ?it/s]STEP: SBPLSHP:  0\n","examples:  [[13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  13\n","tok_id :  13\n","tok_id :  14\n","tok_id :  15\n","tok_id :  16\n","tok_id :  17\n","tok_id :  18\n","tok_id :  19\n","tok_id :  20\n","tok_id :  0\n","flaw_ids:  torch.Size([1, 128])\n","SBPLSHP all_tokens type :  <class 'list'>\n","SBPLSHP all_tokens Len :  5\n","SBPLSHP all_tokens step:  [ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0]\n","all_label_id : type :  <class 'list'>\n","all_label_id : Len :  5\n","all_label_id :  :  [1, 0, 0, 0, 1]\n","flaw_ids : type :  <class 'torch.Tensor'>\n","flaw_ids : Len :  1\n","flaw_ids : :  tensor([[  101,  3464, 12580,  8510,  2000,  3961,  1996, 15488,  5243,  2802,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]])\n","STEP: SBPLSHP:  1\n","examples:  [[21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  21\n","tok_id :  21\n","tok_id :  18\n","tok_id :  22\n","tok_id :  23\n","tok_id :  24\n","tok_id :  18\n","tok_id :  25\n","tok_id :  26\n","tok_id :  27\n","tok_id :  28\n","tok_id :  0\n","flaw_ids:  torch.Size([1, 128])\n","SBPLSHP all_tokens type :  <class 'list'>\n","SBPLSHP all_tokens Len :  5\n","SBPLSHP all_tokens step:  [13 14 15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0]\n","all_label_id : type :  <class 'list'>\n","all_label_id : Len :  5\n","all_label_id :  :  [1, 0, 0, 0, 1]\n","flaw_ids : type :  <class 'torch.Tensor'>\n","flaw_ids : Len :  1\n","flaw_ids : :  tensor([[  101,  2006,  1996,  2919,  7195,  1011,  1997,  1011,  1996,  1011,\n","         11265, 17811, 18856, 17322,  2015,  1996, 16587,  2071,  2852, 24225,\n","          2039,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]])\n","STEP: SBPLSHP:  2\n","examples:  [[37 38  1 18 39 40 34 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n","  58 59 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  37\n","tok_id :  37\n","tok_id :  38\n","tok_id :  1\n","tok_id :  18\n","tok_id :  39\n","tok_id :  40\n","tok_id :  34\n","tok_id :  41\n","tok_id :  42\n","tok_id :  43\n","tok_id :  44\n","tok_id :  45\n","tok_id :  46\n","tok_id :  47\n","tok_id :  48\n","tok_id :  49\n","tok_id :  50\n","tok_id :  51\n","tok_id :  52\n","tok_id :  53\n","tok_id :  54\n","tok_id :  55\n","tok_id :  56\n","tok_id :  57\n","tok_id :  58\n","tok_id :  59\n","tok_id :  37\n","tok_id :  0\n","flaw_ids:  torch.Size([1, 128])\n","SBPLSHP all_tokens type :  <class 'list'>\n","SBPLSHP all_tokens Len :  5\n","SBPLSHP all_tokens step:  [21 18 22 23 24 18 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0]\n","all_label_id : type :  <class 'list'>\n","all_label_id : Len :  5\n","all_label_id :  :  [1, 0, 0, 0, 1]\n","flaw_ids : type :  <class 'torch.Tensor'>\n","flaw_ids : Len :  1\n","flaw_ids : :  tensor([[  101,  1036,  1036, 16691,  2008,  1996,  2472,  1997, 10514,  6776,\n","          5365, 27858,  2015,  2004, 16419,  2998, 23616,  2319,  2145,  2735,\n","         27178,  1037,  2235,  1010,  3167,  2143,  2007,  2019,  6832,  2813,\n","          7361,  1012,  1036,  1036,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]])\n","STEP: SBPLSHP:  3\n","examples:  [[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  2\n","tok_id :  3\n","tok_id :  4\n","tok_id :  5\n","tok_id :  6\n","tok_id :  7\n","tok_id :  8\n","tok_id :  9\n","tok_id :  10\n","tok_id :  11\n","tok_id :  12\n","tok_id :  0\n","flaw_ids:  torch.Size([1, 128])\n","SBPLSHP all_tokens type :  <class 'list'>\n","SBPLSHP all_tokens Len :  5\n","SBPLSHP all_tokens step:  [ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0]\n","all_label_id : type :  <class 'list'>\n","all_label_id : Len :  5\n","all_label_id :  :  [1, 0, 0, 0, 1]\n","flaw_ids : type :  <class 'torch.Tensor'>\n","flaw_ids : Len :  1\n","flaw_ids : :  tensor([[  101,  2008,  7459,  2049, 25869, 11266,  6072,  1998, 10639,  2015,\n","         27941,  4160,  2738,  8502,  2055,  2529, 23961, 21159,  2063,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]])\n","STEP: SBPLSHP:  4\n","examples:  [[ 1 29 30 31 32 16 33 34 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0]]\n","example:  1\n","tok_id :  1\n","tok_id :  29\n","tok_id :  30\n","tok_id :  31\n","tok_id :  32\n","tok_id :  16\n","tok_id :  33\n","tok_id :  34\n","tok_id :  35\n","tok_id :  36\n","tok_id :  0\n","flaw_ids:  torch.Size([1, 128])\n","SBPLSHP all_tokens type :  <class 'list'>\n","SBPLSHP all_tokens Len :  5\n","SBPLSHP all_tokens step:  [37 38  1 18 39 40 34 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n"," 58 59 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0]\n","all_label_id : type :  <class 'list'>\n","all_label_id : Len :  5\n","all_label_id :  :  [1, 0, 0, 0, 1]\n","flaw_ids : type :  <class 'torch.Tensor'>\n","flaw_ids : Len :  1\n","flaw_ids : :  tensor([[  101,  2008,  1005,  1055,  2521,  2205, 13800,  2000,  7857,  2107,\n","          3937,  7438,  2213,  7159,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]])\n","attacks: 100%|███████████████████████████████████| 5/5 [00:00<00:00, 245.72it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zcjgzHI-64jq"},"source":["Testing"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"iHLmeOLx64jq"},"source":["Inference\n","We first attack the test data using 5 differernt methods to drop the model performance as much as possible. The codes related to attacking the test sets would be availble soon!\n","\n","During inference phase, we use the pre-trained discriminator to identify the words that have been attacked."]},{"cell_type":"code","metadata":{"tags":[],"id":"9pNTX7sk64jq"},"source":["# Modified - Run this : \n","# Discriminator do_eval or Testing \n","!python bert_discriminator.py \\\n","--task_name sst-2\\\n","--do_eval\\\n","--eval_batch_size 32\\\n","--do_lower_case\\\n","--data_dir data/sst-2/add_1/\\\n","--data_file data/sst-2/add_1/test.tsv\\\n","--bert_model bert-base-uncased\\\n","--max_seq_length 128\\\n","--train_batch_size 16\\\n","--learning_rate 2e-5\\\n","--num_train_epochs 5\\\n","--output_dir models/\\\n","--single\n","#--no_cuda"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"A3s3iNOf64jr"},"source":["Then, we recover the words with a pre-trained embedding estimator. Note that we use small-world-graph to conduct a KNN-based search for closest word in the embedding space."]},{"cell_type":"code","metadata":{"tags":[],"id":"DnDIFexA64jr"},"source":["!python bert_generator.py \\\n","--task_name sst-2\\\n","--do_eval\\\n","--do_lower_case\\\n","--data_dir data/sst-2/add_1/\\\n","--bert_model bert-base-uncased\\\n","--max_seq_length 64\\\n","--train_batch_size 8\\\n","--learning_rate 2e-5\\\n","--output_dir ./tmp/sst2-gnrt/\\\n","--num_eval_epochs 2\\\n","--no_cuda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"Ak09qXjy64jr"},"source":["Classification"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"8o40TCtz64jr"},"source":["After recovering the test instances, we can run a model to check the recovering effectiveness. The model in our settings is a sentiment classification model based on bert contextualized embeddings."]},{"cell_type":"code","metadata":{"tags":[],"id":"nyCYgnkC64jr"},"source":["python bert_classifier.py \n","--task_name sst-2 \n","--do_eval  \n","--do_lower_case   \n","--data_dir data/SST-2/add_1/  \n","--bert_model bert-base-uncased   \n","--max_seq_length 64   \n","--train_batch_size 8  \n","--learning_rate 2e-5   \n","--output_dir ./tmp/sst2-gnrt/ \n","--num_eval_epochs 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2IdaosB8lwC"},"source":["GAN2vec and RobGAN "]},{"cell_type":"code","metadata":{"id":"nuuFiJ0-8pHp"},"source":["!python GAN2vec_RobGAN_train.py  --sample developing  --task_name sst-2  --do_train  --data_dir data/sst-2/  --max_seq_length 128  --train_batch_size 8  --learning_rate 2e-5  --num_train_epochs 25  --output_dir ./tmp/disc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKBVhnDmgcCo","executionInfo":{"status":"ok","timestamp":1618020001444,"user_tz":240,"elapsed":399,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"1172c564-8ec6-43db-9eba-f070de811d17"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bert_classifier.py\t      bert_utils.py\t       models\n","bert_config.json\t      data\t\t       optimization.py\n","bert_defender_modified.ipynb  emb\t\t       __pycache__\n","bert_discriminator.py\t      enumerate_attacks.py     README.md\n","bert_eval_epoches.py\t      file_utils.py\t       RobGAN\n","bert_eval.py\t\t      GAN2vec\t\t       sample\n","bert_generator.py\t      GAN2vec_gen_dis.py       tmp\n","bert_model.py\t\t      GAN2vec_RobGAN_train.py  tokenization.py\n","bert_random_attacks.py\t      GAN2vec_train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YVxdhNDvw0aM","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1618247842163,"user_tz":240,"elapsed":306231,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"a92ae32f-aa9f-486e-b439-9853a235bf19"},"source":["!pip install allennlp"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/43/15d1c4ee4d24fd05ac283e76cc2287a32c02d439e821d8439471f6c869f2/allennlp-2.2.0-py3-none-any.whl (595kB)\n","\u001b[K     |████████████████████████████████| 604kB 9.2MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 16.9MB/s \n","\u001b[?25hRequirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.17.49)\n","Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n","Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n","Collecting overrides==3.1.0\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 36.0MB/s \n","\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n","Collecting transformers<4.5,>=4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 34.6MB/s \n","\u001b[?25hRequirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8.1+cu111)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n","Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.1+cu111)\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n","Collecting wandb<0.11.0,>=0.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/af/4cfe48fe55046181b992251933cff4ceb3bfd71a42838f5fe683683cd925/wandb-0.10.25-py2.py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 67.0MB/s \n","\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n","\u001b[K     |████████████████████████████████| 266kB 57.7MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.49 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (1.20.49)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.3.6)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (54.2.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp) (3.8.1)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 73.9MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 58.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (20.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.6.0->allennlp) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n","\u001b[K     |████████████████████████████████| 133kB 66.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp) (1.15.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Collecting subprocess32>=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n","Collecting pathtools\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.4.8)\n","Collecting GitPython>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n","\u001b[K     |████████████████████████████████| 163kB 56.5MB/s \n","\u001b[?25hCollecting sentry-sdk>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 76.2MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n","Collecting configparser>=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.5,>=4.1->allennlp) (2.4.7)\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n","\u001b[?25hCollecting smmap<5,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n","Building wheels for collected packages: overrides, jsonnet, sacremoses, subprocess32, pathtools\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=db085ca8673c729c867527aa55b8cbe46de4986918b46f24f53e3ebba1f43789\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388776 sha256=d738f761fccf635906f36d621482c30dc23ef2ee5812b96362f9c178cb223953\n","  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=5113b5f76805faaec27ee6c4a55b1d6eb1a93d197cb748ceda34e7c5119fed0c\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=64cc0cb0205f4ddbacc0c812fd94c0ce54b145f149f7b57928673e77c9b7d7a5\n","  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=4832528cbfcfbd22dd9f79657e397c1878a83230d5794669b976cf24e87b2ad5\n","  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","Successfully built overrides jsonnet sacremoses subprocess32 pathtools\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: sentencepiece, overrides, jsonpickle, tensorboardX, tokenizers, sacremoses, transformers, docker-pycreds, subprocess32, pathtools, smmap, gitdb, GitPython, urllib3, sentry-sdk, shortuuid, configparser, wandb, jsonnet, allennlp\n","  Found existing installation: urllib3 1.26.4\n","    Uninstalling urllib3-1.26.4:\n","      Successfully uninstalled urllib3-1.26.4\n","Successfully installed GitPython-3.1.14 allennlp-2.2.0 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 pathtools-0.1.2 sacremoses-0.0.44 sentencepiece-0.1.95 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboardX-2.2 tokenizers-0.10.2 transformers-4.4.2 urllib3-1.25.11 wandb-0.10.25\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkX_lJG6S3qQ","executionInfo":{"status":"ok","timestamp":1618247845972,"user_tz":240,"elapsed":308639,"user":{"displayName":"sriram sanjeev pratti","photoUrl":"https://lh3.googleusercontent.com/-x9kOW5PtcK0/AAAAAAAAAAI/AAAAAAAARGg/gGBrVWqYABU/s64/photo.jpg","userId":"07241238899528141574"}},"outputId":"e897dbac-079d-42af-acc8-69bc3fcf0e69"},"source":["!pip install dynet"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting dynet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e1/cdb9a7eb73ef636b10f15747782918a9b58ccd3932944909940f086aae17/dyNET-2.1.2-cp37-cp37m-manylinux1_x86_64.whl (4.4MB)\n","\u001b[K     |████████████████████████████████| 4.4MB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from dynet) (0.29.22)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dynet) (1.19.5)\n","Installing collected packages: dynet\n","Successfully installed dynet-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eL84s5UC8wyQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"155e84ab-6d89-4a7a-ad2e-899feeb6f597"},"source":["!python GAN2vec_RobGAN_train.py  --sample developing  --task_name sst-2  --do_train  --data_dir data/sst-2/  --max_seq_length 128  --train_batch_size 256  --learning_rate 2e-5  --num_train_epochs 100  --output_dir ./tmp/disc"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","[2] D Loss: 0.402  G Loss 1.301  (7.4s)\n","sentences:  <class 'list'>\n","sentences len:  256\n","seq: type  <class 'torch.Tensor'>\n","seq:  shape  torch.Size([256, 41, 128])\n","Seq_lens:  [12, 35, 20, 15, 19, 17, 13, 25, 6, 17, 19, 22, 12, 14, 18, 19, 24, 10, 18, 21, 6, 20, 39, 31, 23, 22, 3, 17, 7, 27, 4, 9, 10, 8, 7, 22, 25, 18, 10, 7, 16, 18, 27, 23, 9, 10, 28, 27, 3, 12, 38, 16, 8, 13, 5, 34, 13, 34, 26, 23, 12, 16, 40, 25, 31, 12, 19, 21, 25, 15, 6, 30, 23, 19, 7, 23, 30, 26, 16, 5, 31, 15, 16, 6, 13, 26, 13, 10, 9, 27, 14, 15, 17, 20, 6, 17, 35, 8, 10, 11, 17, 14, 23, 2, 8, 18, 23, 38, 8, 15, 28, 10, 7, 24, 17, 13, 23, 23, 22, 12, 9, 28, 30, 18, 17, 22, 17, 20, 25, 10, 9, 7, 10, 13, 29, 20, 25, 17, 6, 8, 6, 23, 23, 29, 25, 37, 10, 6, 19, 23, 8, 22, 17, 29, 24, 18, 20, 17, 30, 14, 20, 23, 13, 40, 22, 23, 11, 26, 6, 20, 12, 9, 21, 22, 13, 11, 13, 16, 21, 26, 11, 40, 23, 16, 24, 24, 23, 23, 19, 9, 25, 17, 33, 17, 13, 23, 26, 22, 10, 21, 12, 31, 10, 19, 10, 22, 31, 33, 27, 18, 16, 28, 21, 6, 17, 31, 27, 15, 32, 31, 18, 29, 28, 14, 38, 22, 14, 38, 9, 29, 21, 21, 24, 35, 21, 31, 24, 20, 12, 19, 22, 36, 21, 7, 14, 15, 41, 8, 21, 40, 19, 31, 11, 9, 24, 6]\n","real:  PackedSequence(data=tensor([[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-2.3125e-03, -1.4058e-03,  3.5940e-03,  ..., -4.8735e-02,\n","          6.1841e-03,  5.8689e-03],\n","        ...,\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01]]), batch_sizes=tensor([256, 256, 255, 253, 252, 250, 239, 232, 224, 215, 202, 197, 188, 177,\n","        170, 163, 155, 140, 131, 121, 112, 101,  89,  71,  63,  55,  49,  43,\n","         38,  33,  29,  20,  19,  17,  15,  12,  11,  10,   6,   5,   1]), sorted_indices=tensor([246,  62, 249, 163, 181,  22,  50, 107, 227, 224, 145, 241,   1,  96,\n","        233,  55,  57, 207, 192, 218,  64, 235,  80, 251, 219, 201, 215, 206,\n","         23,  71,  76, 122, 158, 229, 134, 143, 153, 221, 211, 222, 121, 110,\n","         46,  47, 216,  89,  42,  29, 208,  58,  85, 167,  77, 179, 196,  63,\n","        128,  68, 136, 144, 190,   7,  36, 154, 184, 185, 113, 236, 254,  16,\n","        232, 102, 165, 106,  43, 142, 161, 141, 182, 149, 186, 187,  75,  72,\n","        195,  24, 116, 117,  59, 205,  11,  35, 118, 225, 151, 125, 240,  25,\n","        197, 173, 164, 231, 199, 242, 212,  19, 172, 230, 248, 178, 234,  67,\n","         21, 127, 156, 135,   2, 169, 237,  93, 160, 188,  10, 148,  15, 203,\n","          4,  66, 239,  73, 250, 123,  14, 220, 105,  37,  18, 155, 209,  41,\n","        152, 124,  95, 157,   5,  92,  27, 126, 214, 100, 114, 137,   9, 191,\n","        193, 177,  78, 183,  51,  61,  40,  82, 210, 217, 245,  69,  91,  81,\n","        109,   3, 244, 223,  13, 101,  90, 226, 159, 133, 115, 194,   6,  53,\n","        162,  86, 176,  56, 174,  84,   0,  65, 200,  60, 170, 119,  12,  49,\n","        238,  99, 252, 180, 166, 175,  17,  32, 202,  87,  45,  98, 111, 129,\n","        132,  38, 146, 204, 198, 253,  88, 120, 189, 228, 130, 171,  31,  44,\n","        139, 247, 104,  97,  52, 150,  33, 108,  74, 243, 112,  39, 131,  28,\n","         34,  83,  70,  20,  94, 255,   8, 138, 140, 168, 147, 213,  54,  79,\n","         30,  48,  26, 103]), unsorted_indices=tensor([188,  12, 116, 169, 126, 144, 180,  61, 244, 152, 122,  90, 194, 172,\n","        132, 124,  69, 202, 136, 105, 241, 112,   5,  28,  85,  97, 254, 146,\n","        237,  47, 252, 222, 203, 230, 238,  91,  62, 135, 211, 235, 160, 139,\n","         46,  74, 223, 206,  42,  43, 253, 195,   6, 158, 228, 181, 250,  15,\n","        185,  16,  49,  88, 191, 159,   1,  55,  20, 189, 127, 111,  57, 165,\n","        240,  29,  83, 129, 232,  82,  30,  52, 156, 251,  22, 167, 161, 239,\n","        187,  50, 183, 205, 216,  45, 174, 166, 145, 119, 242, 142,  13, 227,\n","        207, 197, 149, 173,  71, 255, 226, 134,  73,   7, 231, 168,  41, 208,\n","        234,  66, 150, 178,  86,  87,  92, 193, 217,  40,  31, 131, 141,  95,\n","        147, 113,  56, 209, 220, 236, 210, 177,  34, 115,  58, 151, 245, 224,\n","        246,  77,  75,  35,  59,  10, 212, 248, 123,  79, 229,  94, 140,  36,\n","         63, 137, 114, 143,  32, 176, 120,  76, 182,   3, 100,  72, 200,  51,\n","        247, 117, 192, 221, 106,  99, 186, 201, 184, 155, 109,  53, 199,   4,\n","         78, 157,  64,  65,  80,  81, 121, 218,  60, 153,  18, 154, 179,  84,\n","         54,  98, 214, 102, 190,  25, 204, 125, 213,  89,  27,  17,  48, 138,\n","        162,  38, 104, 249, 148,  26,  44, 163,  19,  24, 133,  37,  39, 171,\n","          9,  93, 175,   8, 219,  33, 107, 101,  70,  14, 110,  21,  67, 118,\n","        196, 128,  96,  11, 103, 233, 170, 164,   0, 225, 108,   2, 130,  23,\n","        198, 215,  68, 243]))\n","real: type:   <class 'torch.nn.utils.rnn.PackedSequence'>\n","real: shape:   4\n","greal:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02]]])\n","greal: shape :  torch.Size([256, 1, 128])\n","greal: type :  <class 'torch.Tensor'>\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","fake:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03],\n","         [-1.6754e-01,  2.3807e-01, -4.5567e-02,  ..., -4.3464e-03,\n","          -1.7799e-01, -2.6081e-01],\n","         [-2.6488e-01,  3.3040e-01, -1.2965e-01,  ..., -7.1213e-02,\n","          -3.3291e-01, -6.3848e-01],\n","         [-4.8895e-01,  4.3272e-01, -2.9440e-01,  ..., -3.1354e-02,\n","          -5.0971e-01, -1.2023e+00],\n","         [-8.7660e-01,  3.9051e-01, -5.1590e-01,  ...,  1.1259e-01,\n","          -5.4731e-01, -1.7638e+00],\n","         [-1.2827e+00,  2.0403e-01, -7.2922e-01,  ...,  3.3639e-01,\n","          -4.6640e-01, -1.9623e+00]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04],\n","         [-2.2135e-01,  3.0748e-01, -1.5564e-01,  ..., -1.7664e-01,\n","           1.4654e-01, -1.5316e-02],\n","         [-4.4204e-01,  5.7324e-01, -2.1891e-01,  ..., -3.9116e-01,\n","           1.5082e-01, -1.0861e-01],\n","         [-7.8376e-01,  1.0125e+00, -3.1033e-01,  ..., -7.2600e-01,\n","           2.9135e-01, -3.1847e-01],\n","         [-1.2969e+00,  1.5136e+00, -3.5179e-01,  ..., -1.1337e+00,\n","           5.5395e-01, -5.9422e-01],\n","         [-1.7003e+00,  1.8028e+00, -2.7955e-01,  ..., -1.2993e+00,\n","           8.9491e-01, -7.6000e-01]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02],\n","         [-1.1551e-01,  1.8095e-01,  2.1846e-01,  ..., -3.0868e-01,\n","          -2.4297e-01, -2.9014e-02],\n","         [-1.7112e-01,  3.4108e-01, -4.7406e-03,  ..., -3.9363e-01,\n","          -2.3939e-01, -5.9175e-02],\n","         [-2.5151e-01,  5.5400e-01, -2.7303e-01,  ..., -4.4699e-01,\n","          -2.2835e-01, -1.2648e-01],\n","         [-3.0221e-01,  6.8865e-01, -6.5852e-01,  ..., -3.7842e-01,\n","          -1.5688e-01, -1.4865e-01],\n","         [-2.7343e-01,  6.6991e-01, -1.0186e+00,  ..., -2.4310e-01,\n","          -3.5428e-02, -1.3595e-01]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01],\n","         [ 2.0241e-01,  4.9919e-01,  1.8256e-01,  ..., -6.8717e-01,\n","          -3.2112e-01, -4.7279e-01],\n","         [ 1.6990e-01,  8.0448e-01,  5.2417e-01,  ..., -8.6106e-01,\n","          -7.6724e-01, -7.7342e-01],\n","         [ 1.4019e-01,  9.7761e-01,  8.2068e-01,  ..., -8.7083e-01,\n","          -1.3015e+00, -9.6119e-01],\n","         [ 1.5774e-01,  9.7194e-01,  1.0659e+00,  ..., -6.8095e-01,\n","          -1.7201e+00, -1.0375e+00],\n","         [ 2.7110e-01,  8.8240e-01,  1.2248e+00,  ..., -4.4997e-01,\n","          -2.0063e+00, -1.0417e+00]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01],\n","         [ 3.3716e-02,  3.7949e-01, -1.7672e-01,  ..., -7.1026e-01,\n","          -6.6069e-01, -1.1124e+00],\n","         [-7.8641e-02,  5.2321e-01,  2.2303e-02,  ..., -9.0006e-01,\n","          -1.1491e+00, -1.8020e+00],\n","         [-9.0003e-02,  7.0302e-01,  3.0894e-01,  ..., -1.0614e+00,\n","          -1.6509e+00, -2.4215e+00],\n","         [ 2.3824e-02,  7.9126e-01,  5.2509e-01,  ..., -1.0857e+00,\n","          -1.8385e+00, -2.6933e+00],\n","         [ 7.1282e-02,  7.9471e-01,  6.7508e-01,  ..., -9.5678e-01,\n","          -1.8692e+00, -2.7640e+00]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02],\n","         [-3.9032e-01,  1.8353e-01,  2.4280e-01,  ..., -6.3598e-01,\n","           1.3014e-02, -3.9987e-01],\n","         [-5.3291e-01,  4.4949e-01,  3.8504e-01,  ..., -1.0889e+00,\n","          -1.8084e-01, -8.8295e-01],\n","         [-6.2392e-01,  6.8029e-01,  6.8406e-01,  ..., -1.4251e+00,\n","          -5.5647e-01, -1.6574e+00],\n","         [-5.2330e-01,  7.7875e-01,  9.5929e-01,  ..., -1.4567e+00,\n","          -8.7848e-01, -2.1562e+00],\n","         [-4.1750e-01,  8.0704e-01,  1.1029e+00,  ..., -1.3935e+00,\n","          -1.0837e+00, -2.3591e+00]]], grad_fn=<CatBackward>)\n","fake: shape :  torch.Size([256, 6, 128])\n","fake: type :  <class 'torch.Tensor'>\n","D(real):  tensor([[0.6727],\n","        [0.7388],\n","        [0.9498],\n","        [0.9453],\n","        [0.8897],\n","        [0.9608],\n","        [0.9679],\n","        [0.9574],\n","        [0.8159],\n","        [0.9577],\n","        [0.8862],\n","        [0.9425],\n","        [0.8740],\n","        [0.9569],\n","        [0.8862],\n","        [0.6454],\n","        [0.9299],\n","        [0.8837],\n","        [0.8851],\n","        [0.9661],\n","        [0.7033],\n","        [0.9676],\n","        [0.9408],\n","        [0.9497],\n","        [0.8172],\n","        [0.9649],\n","        [0.2981],\n","        [0.9271],\n","        [0.3840],\n","        [0.9480],\n","        [0.2909],\n","        [0.9175],\n","        [0.8952],\n","        [0.9495],\n","        [0.9371],\n","        [0.9463],\n","        [0.9074],\n","        [0.9634],\n","        [0.9264],\n","        [0.9121],\n","        [0.9050],\n","        [0.7156],\n","        [0.8724],\n","        [0.9463],\n","        [0.5541],\n","        [0.8952],\n","        [0.9223],\n","        [0.7333],\n","        [0.2871],\n","        [0.7785],\n","        [0.8993],\n","        [0.9635],\n","        [0.9522],\n","        [0.9486],\n","        [0.3441],\n","        [0.9336],\n","        [0.8610],\n","        [0.9297],\n","        [0.9273],\n","        [0.9308],\n","        [0.8509],\n","        [0.9537],\n","        [0.8840],\n","        [0.8959],\n","        [0.9008],\n","        [0.8750],\n","        [0.4737],\n","        [0.8544],\n","        [0.9307],\n","        [0.6081],\n","        [0.7036],\n","        [0.9544],\n","        [0.9353],\n","        [0.9529],\n","        [0.9074],\n","        [0.9275],\n","        [0.9539],\n","        [0.9069],\n","        [0.6959],\n","        [0.5226],\n","        [0.8783],\n","        [0.8796],\n","        [0.9597],\n","        [0.7563],\n","        [0.9161],\n","        [0.9224],\n","        [0.9558],\n","        [0.8881],\n","        [0.8886],\n","        [0.9222],\n","        [0.8887],\n","        [0.9399],\n","        [0.9042],\n","        [0.8344],\n","        [0.3086],\n","        [0.9206],\n","        [0.9478],\n","        [0.8744],\n","        [0.9585],\n","        [0.8453],\n","        [0.9555],\n","        [0.9286],\n","        [0.8797],\n","        [0.3231],\n","        [0.6280],\n","        [0.4338],\n","        [0.8351],\n","        [0.9184],\n","        [0.9706],\n","        [0.7975],\n","        [0.8614],\n","        [0.9670],\n","        [0.8945],\n","        [0.9699],\n","        [0.9237],\n","        [0.9068],\n","        [0.5628],\n","        [0.9520],\n","        [0.8852],\n","        [0.9572],\n","        [0.5003],\n","        [0.9445],\n","        [0.5079],\n","        [0.9592],\n","        [0.4120],\n","        [0.7891],\n","        [0.8651],\n","        [0.9150],\n","        [0.8777],\n","        [0.9455],\n","        [0.9227],\n","        [0.9431],\n","        [0.5445],\n","        [0.9388],\n","        [0.7449],\n","        [0.9077],\n","        [0.9599],\n","        [0.9273],\n","        [0.8688],\n","        [0.8218],\n","        [0.8514],\n","        [0.9601],\n","        [0.7863],\n","        [0.9448],\n","        [0.8593],\n","        [0.8540],\n","        [0.9379],\n","        [0.3215],\n","        [0.9574],\n","        [0.9055],\n","        [0.9428],\n","        [0.7415],\n","        [0.9206],\n","        [0.9375],\n","        [0.9563],\n","        [0.9652],\n","        [0.7228],\n","        [0.9594],\n","        [0.8536],\n","        [0.9613],\n","        [0.9544],\n","        [0.9597],\n","        [0.9593],\n","        [0.9434],\n","        [0.9572],\n","        [0.7577],\n","        [0.7314],\n","        [0.9167],\n","        [0.8824],\n","        [0.8808],\n","        [0.9078],\n","        [0.9412],\n","        [0.7902],\n","        [0.9250],\n","        [0.9205],\n","        [0.9008],\n","        [0.9391],\n","        [0.9258],\n","        [0.9397],\n","        [0.8137],\n","        [0.9263],\n","        [0.8783],\n","        [0.9676],\n","        [0.9649],\n","        [0.9286],\n","        [0.9663],\n","        [0.9582],\n","        [0.9319],\n","        [0.9585],\n","        [0.3967],\n","        [0.9283],\n","        [0.9450],\n","        [0.7615],\n","        [0.8894],\n","        [0.9250],\n","        [0.9560],\n","        [0.9143],\n","        [0.6431],\n","        [0.9259],\n","        [0.9504],\n","        [0.7923],\n","        [0.9408],\n","        [0.9271],\n","        [0.9622],\n","        [0.8419],\n","        [0.8151],\n","        [0.9032],\n","        [0.9622],\n","        [0.8965],\n","        [0.5736],\n","        [0.3243],\n","        [0.7767],\n","        [0.9316],\n","        [0.2705],\n","        [0.9526],\n","        [0.6659],\n","        [0.8409],\n","        [0.8795],\n","        [0.4630],\n","        [0.9611],\n","        [0.9342],\n","        [0.8665],\n","        [0.9640],\n","        [0.9014],\n","        [0.9146],\n","        [0.8869],\n","        [0.9337],\n","        [0.9202],\n","        [0.7295],\n","        [0.9485],\n","        [0.2738],\n","        [0.5826],\n","        [0.3912],\n","        [0.9625],\n","        [0.8515],\n","        [0.9648],\n","        [0.9321],\n","        [0.5032],\n","        [0.9248],\n","        [0.9536],\n","        [0.9527],\n","        [0.8870],\n","        [0.9135],\n","        [0.9390],\n","        [0.9359],\n","        [0.9367],\n","        [0.9103],\n","        [0.9239],\n","        [0.8660],\n","        [0.7292],\n","        [0.9224],\n","        [0.7413],\n","        [0.8942],\n","        [0.9176],\n","        [0.9179],\n","        [0.9047]], grad_fn=<SigmoidBackward>)\n","t1 :  tensor([[0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000]])\n","t1 : shape  torch.Size([256, 1])\n","f1 :  tensor([[0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","[2] D Loss: 0.399  G Loss 1.319  (7.1s)\n","sentences:  <class 'list'>\n","sentences len:  256\n","seq: type  <class 'torch.Tensor'>\n","seq:  shape  torch.Size([256, 41, 128])\n","Seq_lens:  [12, 35, 20, 15, 19, 17, 13, 25, 6, 17, 19, 22, 12, 14, 18, 19, 24, 10, 18, 21, 6, 20, 39, 31, 23, 22, 3, 17, 7, 27, 4, 9, 10, 8, 7, 22, 25, 18, 10, 7, 16, 18, 27, 23, 9, 10, 28, 27, 3, 12, 38, 16, 8, 13, 5, 34, 13, 34, 26, 23, 12, 16, 40, 25, 31, 12, 19, 21, 25, 15, 6, 30, 23, 19, 7, 23, 30, 26, 16, 5, 31, 15, 16, 6, 13, 26, 13, 10, 9, 27, 14, 15, 17, 20, 6, 17, 35, 8, 10, 11, 17, 14, 23, 2, 8, 18, 23, 38, 8, 15, 28, 10, 7, 24, 17, 13, 23, 23, 22, 12, 9, 28, 30, 18, 17, 22, 17, 20, 25, 10, 9, 7, 10, 13, 29, 20, 25, 17, 6, 8, 6, 23, 23, 29, 25, 37, 10, 6, 19, 23, 8, 22, 17, 29, 24, 18, 20, 17, 30, 14, 20, 23, 13, 40, 22, 23, 11, 26, 6, 20, 12, 9, 21, 22, 13, 11, 13, 16, 21, 26, 11, 40, 23, 16, 24, 24, 23, 23, 19, 9, 25, 17, 33, 17, 13, 23, 26, 22, 10, 21, 12, 31, 10, 19, 10, 22, 31, 33, 27, 18, 16, 28, 21, 6, 17, 31, 27, 15, 32, 31, 18, 29, 28, 14, 38, 22, 14, 38, 9, 29, 21, 21, 24, 35, 21, 31, 24, 20, 12, 19, 22, 36, 21, 7, 14, 15, 41, 8, 21, 40, 19, 31, 11, 9, 24, 6]\n","real:  PackedSequence(data=tensor([[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-2.3125e-03, -1.4058e-03,  3.5940e-03,  ..., -4.8735e-02,\n","          6.1841e-03,  5.8689e-03],\n","        ...,\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01]]), batch_sizes=tensor([256, 256, 255, 253, 252, 250, 239, 232, 224, 215, 202, 197, 188, 177,\n","        170, 163, 155, 140, 131, 121, 112, 101,  89,  71,  63,  55,  49,  43,\n","         38,  33,  29,  20,  19,  17,  15,  12,  11,  10,   6,   5,   1]), sorted_indices=tensor([246,  62, 249, 163, 181,  22,  50, 107, 227, 224, 145, 241,   1,  96,\n","        233,  55,  57, 207, 192, 218,  64, 235,  80, 251, 219, 201, 215, 206,\n","         23,  71,  76, 122, 158, 229, 134, 143, 153, 221, 211, 222, 121, 110,\n","         46,  47, 216,  89,  42,  29, 208,  58,  85, 167,  77, 179, 196,  63,\n","        128,  68, 136, 144, 190,   7,  36, 154, 184, 185, 113, 236, 254,  16,\n","        232, 102, 165, 106,  43, 142, 161, 141, 182, 149, 186, 187,  75,  72,\n","        195,  24, 116, 117,  59, 205,  11,  35, 118, 225, 151, 125, 240,  25,\n","        197, 173, 164, 231, 199, 242, 212,  19, 172, 230, 248, 178, 234,  67,\n","         21, 127, 156, 135,   2, 169, 237,  93, 160, 188,  10, 148,  15, 203,\n","          4,  66, 239,  73, 250, 123,  14, 220, 105,  37,  18, 155, 209,  41,\n","        152, 124,  95, 157,   5,  92,  27, 126, 214, 100, 114, 137,   9, 191,\n","        193, 177,  78, 183,  51,  61,  40,  82, 210, 217, 245,  69,  91,  81,\n","        109,   3, 244, 223,  13, 101,  90, 226, 159, 133, 115, 194,   6,  53,\n","        162,  86, 176,  56, 174,  84,   0,  65, 200,  60, 170, 119,  12,  49,\n","        238,  99, 252, 180, 166, 175,  17,  32, 202,  87,  45,  98, 111, 129,\n","        132,  38, 146, 204, 198, 253,  88, 120, 189, 228, 130, 171,  31,  44,\n","        139, 247, 104,  97,  52, 150,  33, 108,  74, 243, 112,  39, 131,  28,\n","         34,  83,  70,  20,  94, 255,   8, 138, 140, 168, 147, 213,  54,  79,\n","         30,  48,  26, 103]), unsorted_indices=tensor([188,  12, 116, 169, 126, 144, 180,  61, 244, 152, 122,  90, 194, 172,\n","        132, 124,  69, 202, 136, 105, 241, 112,   5,  28,  85,  97, 254, 146,\n","        237,  47, 252, 222, 203, 230, 238,  91,  62, 135, 211, 235, 160, 139,\n","         46,  74, 223, 206,  42,  43, 253, 195,   6, 158, 228, 181, 250,  15,\n","        185,  16,  49,  88, 191, 159,   1,  55,  20, 189, 127, 111,  57, 165,\n","        240,  29,  83, 129, 232,  82,  30,  52, 156, 251,  22, 167, 161, 239,\n","        187,  50, 183, 205, 216,  45, 174, 166, 145, 119, 242, 142,  13, 227,\n","        207, 197, 149, 173,  71, 255, 226, 134,  73,   7, 231, 168,  41, 208,\n","        234,  66, 150, 178,  86,  87,  92, 193, 217,  40,  31, 131, 141,  95,\n","        147, 113,  56, 209, 220, 236, 210, 177,  34, 115,  58, 151, 245, 224,\n","        246,  77,  75,  35,  59,  10, 212, 248, 123,  79, 229,  94, 140,  36,\n","         63, 137, 114, 143,  32, 176, 120,  76, 182,   3, 100,  72, 200,  51,\n","        247, 117, 192, 221, 106,  99, 186, 201, 184, 155, 109,  53, 199,   4,\n","         78, 157,  64,  65,  80,  81, 121, 218,  60, 153,  18, 154, 179,  84,\n","         54,  98, 214, 102, 190,  25, 204, 125, 213,  89,  27,  17,  48, 138,\n","        162,  38, 104, 249, 148,  26,  44, 163,  19,  24, 133,  37,  39, 171,\n","          9,  93, 175,   8, 219,  33, 107, 101,  70,  14, 110,  21,  67, 118,\n","        196, 128,  96,  11, 103, 233, 170, 164,   0, 225, 108,   2, 130,  23,\n","        198, 215,  68, 243]))\n","real: type:   <class 'torch.nn.utils.rnn.PackedSequence'>\n","real: shape:   4\n","greal:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02]]])\n","greal: shape :  torch.Size([256, 1, 128])\n","greal: type :  <class 'torch.Tensor'>\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","fake:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03],\n","         [-1.6468e-01,  1.1825e-01,  4.8716e-02,  ..., -2.6453e-01,\n","          -4.0513e-02, -2.4730e-01],\n","         [-1.9914e-01,  2.1577e-01,  1.2482e-01,  ..., -4.4570e-01,\n","          -1.9737e-01, -4.3769e-01],\n","         [-2.5100e-01,  3.9383e-01,  2.5677e-01,  ..., -7.9660e-01,\n","          -5.1953e-01, -8.7200e-01],\n","         [-2.7102e-01,  6.3721e-01,  5.2223e-01,  ..., -1.2809e+00,\n","          -1.1097e+00, -1.6492e+00],\n","         [-1.5324e-01,  7.9987e-01,  7.9837e-01,  ..., -1.5764e+00,\n","          -1.6839e+00, -2.4006e+00]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04],\n","         [-2.2477e-01, -7.9760e-04,  1.4293e-01,  ..., -1.2161e-01,\n","          -6.2252e-02, -1.0862e-01],\n","         [-3.2555e-01,  1.5811e-01,  1.0007e-01,  ..., -2.2894e-01,\n","          -9.9153e-02, -2.4405e-01],\n","         [-5.4064e-01,  4.3476e-01,  9.5546e-02,  ..., -3.4934e-01,\n","          -1.3512e-01, -4.6623e-01],\n","         [-9.2806e-01,  8.2452e-01,  1.3679e-01,  ..., -4.7002e-01,\n","          -1.9948e-01, -8.4044e-01],\n","         [-1.3156e+00,  1.1623e+00,  2.1442e-01,  ..., -4.4295e-01,\n","          -2.8554e-01, -1.1942e+00]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02],\n","         [ 8.7298e-02, -5.2678e-02, -5.3403e-02,  ..., -3.8256e-02,\n","          -4.0154e-01, -4.7562e-01],\n","         [ 1.9609e-01,  9.8018e-04,  1.9205e-01,  ..., -7.0698e-02,\n","          -6.9600e-01, -7.0064e-01],\n","         [ 4.2519e-01,  5.4585e-02,  7.3421e-01,  ...,  5.3194e-02,\n","          -1.1712e+00, -1.0238e+00],\n","         [ 5.8746e-01,  4.9364e-02,  1.3942e+00,  ...,  3.4384e-01,\n","          -1.5936e+00, -1.2264e+00],\n","         [ 6.3153e-01,  2.6336e-02,  1.8710e+00,  ...,  6.6498e-01,\n","          -1.8173e+00, -1.1445e+00]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01],\n","         [-1.2279e-01,  4.6084e-01, -1.3890e-01,  ..., -1.2667e+00,\n","          -2.7421e-01, -7.5291e-01],\n","         [-9.4050e-02,  7.7579e-01, -6.4077e-02,  ..., -2.0905e+00,\n","          -5.5414e-01, -1.4477e+00],\n","         [-8.8456e-02,  9.6015e-01, -6.9800e-02,  ..., -2.5047e+00,\n","          -8.6426e-01, -2.0583e+00],\n","         [-1.0632e-01,  8.9306e-01, -5.0041e-02,  ..., -2.3574e+00,\n","          -1.1060e+00, -2.4220e+00],\n","         [-1.4129e-01,  7.7701e-01,  5.6926e-02,  ..., -2.0586e+00,\n","          -1.2945e+00, -2.6716e+00]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01],\n","         [-1.3801e-01,  4.9503e-01, -2.0621e-01,  ..., -1.5464e+00,\n","          -3.1545e-01, -1.2160e+00],\n","         [-1.9297e-01,  6.9442e-01, -2.5581e-01,  ..., -2.1055e+00,\n","          -7.5777e-01, -1.9801e+00],\n","         [-3.0135e-01,  6.7599e-01, -2.0975e-01,  ..., -2.2660e+00,\n","          -1.1835e+00, -2.4963e+00],\n","         [-3.7966e-01,  5.2328e-01, -6.5875e-02,  ..., -2.0727e+00,\n","          -1.4452e+00, -2.7820e+00],\n","         [-4.7911e-01,  3.7280e-01,  6.2998e-02,  ..., -1.7950e+00,\n","          -1.5447e+00, -2.8812e+00]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02],\n","         [-3.1840e-01, -6.5809e-02,  2.6806e-02,  ..., -2.8201e-01,\n","          -3.2792e-01, -4.6256e-01],\n","         [-4.3419e-01,  8.4599e-03, -5.3825e-03,  ..., -5.9369e-01,\n","          -5.1662e-01, -1.0721e+00],\n","         [-6.5234e-01,  7.5233e-02, -2.2531e-02,  ..., -8.3800e-01,\n","          -7.6830e-01, -1.7439e+00],\n","         [-8.0371e-01,  2.7929e-02,  1.5366e-02,  ..., -8.2816e-01,\n","          -9.1698e-01, -2.1215e+00],\n","         [-8.2607e-01, -2.1732e-02,  1.2796e-02,  ..., -7.4059e-01,\n","          -9.4484e-01, -2.2786e+00]]], grad_fn=<CatBackward>)\n","fake: shape :  torch.Size([256, 6, 128])\n","fake: type :  <class 'torch.Tensor'>\n","D(real):  tensor([[0.6639],\n","        [0.7270],\n","        [0.9458],\n","        [0.9419],\n","        [0.8826],\n","        [0.9580],\n","        [0.9648],\n","        [0.9512],\n","        [0.7894],\n","        [0.9532],\n","        [0.8811],\n","        [0.9374],\n","        [0.8707],\n","        [0.9531],\n","        [0.8772],\n","        [0.6357],\n","        [0.9236],\n","        [0.8736],\n","        [0.8764],\n","        [0.9621],\n","        [0.7018],\n","        [0.9655],\n","        [0.9364],\n","        [0.9443],\n","        [0.8121],\n","        [0.9604],\n","        [0.2922],\n","        [0.9237],\n","        [0.3943],\n","        [0.9422],\n","        [0.2832],\n","        [0.9089],\n","        [0.8902],\n","        [0.9475],\n","        [0.9376],\n","        [0.9412],\n","        [0.9024],\n","        [0.9583],\n","        [0.9250],\n","        [0.9014],\n","        [0.8984],\n","        [0.7075],\n","        [0.8662],\n","        [0.9415],\n","        [0.5465],\n","        [0.8810],\n","        [0.9219],\n","        [0.7249],\n","        [0.2816],\n","        [0.7694],\n","        [0.8917],\n","        [0.9591],\n","        [0.9493],\n","        [0.9458],\n","        [0.3423],\n","        [0.9272],\n","        [0.8476],\n","        [0.9245],\n","        [0.9230],\n","        [0.9245],\n","        [0.8493],\n","        [0.9496],\n","        [0.8710],\n","        [0.8921],\n","        [0.9015],\n","        [0.8662],\n","        [0.4670],\n","        [0.8555],\n","        [0.9250],\n","        [0.6077],\n","        [0.7171],\n","        [0.9493],\n","        [0.9299],\n","        [0.9502],\n","        [0.9135],\n","        [0.9223],\n","        [0.9510],\n","        [0.9003],\n","        [0.6910],\n","        [0.5027],\n","        [0.8683],\n","        [0.8777],\n","        [0.9550],\n","        [0.7540],\n","        [0.9126],\n","        [0.9201],\n","        [0.9519],\n","        [0.8746],\n","        [0.8833],\n","        [0.9169],\n","        [0.8846],\n","        [0.9378],\n","        [0.8976],\n","        [0.8304],\n","        [0.3102],\n","        [0.9141],\n","        [0.9434],\n","        [0.8664],\n","        [0.9554],\n","        [0.8336],\n","        [0.9506],\n","        [0.9223],\n","        [0.8667],\n","        [0.3186],\n","        [0.6316],\n","        [0.4320],\n","        [0.8272],\n","        [0.9120],\n","        [0.9642],\n","        [0.7892],\n","        [0.8533],\n","        [0.9623],\n","        [0.8845],\n","        [0.9670],\n","        [0.9195],\n","        [0.9065],\n","        [0.5536],\n","        [0.9496],\n","        [0.8825],\n","        [0.9511],\n","        [0.5037],\n","        [0.9433],\n","        [0.4975],\n","        [0.9556],\n","        [0.4051],\n","        [0.7979],\n","        [0.8514],\n","        [0.9065],\n","        [0.8697],\n","        [0.9395],\n","        [0.9131],\n","        [0.9373],\n","        [0.5319],\n","        [0.9335],\n","        [0.7378],\n","        [0.9003],\n","        [0.9571],\n","        [0.9242],\n","        [0.8606],\n","        [0.8176],\n","        [0.8561],\n","        [0.9560],\n","        [0.7767],\n","        [0.9409],\n","        [0.8544],\n","        [0.8482],\n","        [0.9338],\n","        [0.3251],\n","        [0.9514],\n","        [0.8999],\n","        [0.9371],\n","        [0.7229],\n","        [0.9146],\n","        [0.9345],\n","        [0.9523],\n","        [0.9615],\n","        [0.7072],\n","        [0.9543],\n","        [0.8629],\n","        [0.9567],\n","        [0.9496],\n","        [0.9582],\n","        [0.9544],\n","        [0.9418],\n","        [0.9534],\n","        [0.7558],\n","        [0.7249],\n","        [0.9124],\n","        [0.8725],\n","        [0.8784],\n","        [0.8976],\n","        [0.9341],\n","        [0.7827],\n","        [0.9191],\n","        [0.9165],\n","        [0.8920],\n","        [0.9357],\n","        [0.9208],\n","        [0.9354],\n","        [0.8190],\n","        [0.9216],\n","        [0.8677],\n","        [0.9652],\n","        [0.9596],\n","        [0.9213],\n","        [0.9619],\n","        [0.9534],\n","        [0.9243],\n","        [0.9536],\n","        [0.3985],\n","        [0.9227],\n","        [0.9413],\n","        [0.7527],\n","        [0.8823],\n","        [0.9162],\n","        [0.9514],\n","        [0.9069],\n","        [0.6361],\n","        [0.9238],\n","        [0.9451],\n","        [0.7808],\n","        [0.9354],\n","        [0.9184],\n","        [0.9579],\n","        [0.8423],\n","        [0.8083],\n","        [0.8940],\n","        [0.9574],\n","        [0.8977],\n","        [0.5637],\n","        [0.3190],\n","        [0.7646],\n","        [0.9239],\n","        [0.2676],\n","        [0.9471],\n","        [0.6562],\n","        [0.8423],\n","        [0.8693],\n","        [0.4585],\n","        [0.9578],\n","        [0.9302],\n","        [0.8713],\n","        [0.9612],\n","        [0.8945],\n","        [0.9144],\n","        [0.8802],\n","        [0.9287],\n","        [0.9134],\n","        [0.7243],\n","        [0.9418],\n","        [0.2668],\n","        [0.5677],\n","        [0.3847],\n","        [0.9605],\n","        [0.8408],\n","        [0.9609],\n","        [0.9306],\n","        [0.4923],\n","        [0.9190],\n","        [0.9464],\n","        [0.9481],\n","        [0.8840],\n","        [0.9028],\n","        [0.9257],\n","        [0.9315],\n","        [0.9312],\n","        [0.9029],\n","        [0.9186],\n","        [0.8577],\n","        [0.7209],\n","        [0.9204],\n","        [0.7368],\n","        [0.8889],\n","        [0.9089],\n","        [0.9096],\n","        [0.8892]], grad_fn=<SigmoidBackward>)\n","t1 :  tensor([[0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000]])\n","t1 : shape  torch.Size([256, 1])\n","f1 :  tensor([[0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","[2] D Loss: 0.397  G Loss 1.337  (7.1s)\n","sentences:  <class 'list'>\n","sentences len:  256\n","seq: type  <class 'torch.Tensor'>\n","seq:  shape  torch.Size([256, 41, 128])\n","Seq_lens:  [12, 35, 20, 15, 19, 17, 13, 25, 6, 17, 19, 22, 12, 14, 18, 19, 24, 10, 18, 21, 6, 20, 39, 31, 23, 22, 3, 17, 7, 27, 4, 9, 10, 8, 7, 22, 25, 18, 10, 7, 16, 18, 27, 23, 9, 10, 28, 27, 3, 12, 38, 16, 8, 13, 5, 34, 13, 34, 26, 23, 12, 16, 40, 25, 31, 12, 19, 21, 25, 15, 6, 30, 23, 19, 7, 23, 30, 26, 16, 5, 31, 15, 16, 6, 13, 26, 13, 10, 9, 27, 14, 15, 17, 20, 6, 17, 35, 8, 10, 11, 17, 14, 23, 2, 8, 18, 23, 38, 8, 15, 28, 10, 7, 24, 17, 13, 23, 23, 22, 12, 9, 28, 30, 18, 17, 22, 17, 20, 25, 10, 9, 7, 10, 13, 29, 20, 25, 17, 6, 8, 6, 23, 23, 29, 25, 37, 10, 6, 19, 23, 8, 22, 17, 29, 24, 18, 20, 17, 30, 14, 20, 23, 13, 40, 22, 23, 11, 26, 6, 20, 12, 9, 21, 22, 13, 11, 13, 16, 21, 26, 11, 40, 23, 16, 24, 24, 23, 23, 19, 9, 25, 17, 33, 17, 13, 23, 26, 22, 10, 21, 12, 31, 10, 19, 10, 22, 31, 33, 27, 18, 16, 28, 21, 6, 17, 31, 27, 15, 32, 31, 18, 29, 28, 14, 38, 22, 14, 38, 9, 29, 21, 21, 24, 35, 21, 31, 24, 20, 12, 19, 22, 36, 21, 7, 14, 15, 41, 8, 21, 40, 19, 31, 11, 9, 24, 6]\n","real:  PackedSequence(data=tensor([[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-2.3125e-03, -1.4058e-03,  3.5940e-03,  ..., -4.8735e-02,\n","          6.1841e-03,  5.8689e-03],\n","        ...,\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01]]), batch_sizes=tensor([256, 256, 255, 253, 252, 250, 239, 232, 224, 215, 202, 197, 188, 177,\n","        170, 163, 155, 140, 131, 121, 112, 101,  89,  71,  63,  55,  49,  43,\n","         38,  33,  29,  20,  19,  17,  15,  12,  11,  10,   6,   5,   1]), sorted_indices=tensor([246,  62, 249, 163, 181,  22,  50, 107, 227, 224, 145, 241,   1,  96,\n","        233,  55,  57, 207, 192, 218,  64, 235,  80, 251, 219, 201, 215, 206,\n","         23,  71,  76, 122, 158, 229, 134, 143, 153, 221, 211, 222, 121, 110,\n","         46,  47, 216,  89,  42,  29, 208,  58,  85, 167,  77, 179, 196,  63,\n","        128,  68, 136, 144, 190,   7,  36, 154, 184, 185, 113, 236, 254,  16,\n","        232, 102, 165, 106,  43, 142, 161, 141, 182, 149, 186, 187,  75,  72,\n","        195,  24, 116, 117,  59, 205,  11,  35, 118, 225, 151, 125, 240,  25,\n","        197, 173, 164, 231, 199, 242, 212,  19, 172, 230, 248, 178, 234,  67,\n","         21, 127, 156, 135,   2, 169, 237,  93, 160, 188,  10, 148,  15, 203,\n","          4,  66, 239,  73, 250, 123,  14, 220, 105,  37,  18, 155, 209,  41,\n","        152, 124,  95, 157,   5,  92,  27, 126, 214, 100, 114, 137,   9, 191,\n","        193, 177,  78, 183,  51,  61,  40,  82, 210, 217, 245,  69,  91,  81,\n","        109,   3, 244, 223,  13, 101,  90, 226, 159, 133, 115, 194,   6,  53,\n","        162,  86, 176,  56, 174,  84,   0,  65, 200,  60, 170, 119,  12,  49,\n","        238,  99, 252, 180, 166, 175,  17,  32, 202,  87,  45,  98, 111, 129,\n","        132,  38, 146, 204, 198, 253,  88, 120, 189, 228, 130, 171,  31,  44,\n","        139, 247, 104,  97,  52, 150,  33, 108,  74, 243, 112,  39, 131,  28,\n","         34,  83,  70,  20,  94, 255,   8, 138, 140, 168, 147, 213,  54,  79,\n","         30,  48,  26, 103]), unsorted_indices=tensor([188,  12, 116, 169, 126, 144, 180,  61, 244, 152, 122,  90, 194, 172,\n","        132, 124,  69, 202, 136, 105, 241, 112,   5,  28,  85,  97, 254, 146,\n","        237,  47, 252, 222, 203, 230, 238,  91,  62, 135, 211, 235, 160, 139,\n","         46,  74, 223, 206,  42,  43, 253, 195,   6, 158, 228, 181, 250,  15,\n","        185,  16,  49,  88, 191, 159,   1,  55,  20, 189, 127, 111,  57, 165,\n","        240,  29,  83, 129, 232,  82,  30,  52, 156, 251,  22, 167, 161, 239,\n","        187,  50, 183, 205, 216,  45, 174, 166, 145, 119, 242, 142,  13, 227,\n","        207, 197, 149, 173,  71, 255, 226, 134,  73,   7, 231, 168,  41, 208,\n","        234,  66, 150, 178,  86,  87,  92, 193, 217,  40,  31, 131, 141,  95,\n","        147, 113,  56, 209, 220, 236, 210, 177,  34, 115,  58, 151, 245, 224,\n","        246,  77,  75,  35,  59,  10, 212, 248, 123,  79, 229,  94, 140,  36,\n","         63, 137, 114, 143,  32, 176, 120,  76, 182,   3, 100,  72, 200,  51,\n","        247, 117, 192, 221, 106,  99, 186, 201, 184, 155, 109,  53, 199,   4,\n","         78, 157,  64,  65,  80,  81, 121, 218,  60, 153,  18, 154, 179,  84,\n","         54,  98, 214, 102, 190,  25, 204, 125, 213,  89,  27,  17,  48, 138,\n","        162,  38, 104, 249, 148,  26,  44, 163,  19,  24, 133,  37,  39, 171,\n","          9,  93, 175,   8, 219,  33, 107, 101,  70,  14, 110,  21,  67, 118,\n","        196, 128,  96,  11, 103, 233, 170, 164,   0, 225, 108,   2, 130,  23,\n","        198, 215,  68, 243]))\n","real: type:   <class 'torch.nn.utils.rnn.PackedSequence'>\n","real: shape:   4\n","greal:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02]]])\n","greal: shape :  torch.Size([256, 1, 128])\n","greal: type :  <class 'torch.Tensor'>\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","fake:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03],\n","         [ 7.3451e-02, -1.2480e-01,  1.5463e-01,  ..., -1.6001e-01,\n","          -2.2860e-01, -1.2385e-01],\n","         [ 9.5341e-02,  5.0240e-03,  2.3542e-01,  ..., -2.8298e-01,\n","          -5.3010e-01, -3.6476e-01],\n","         [ 2.1713e-01,  1.7122e-01,  4.4987e-01,  ..., -4.1665e-01,\n","          -1.0868e+00, -7.7806e-01],\n","         [ 4.2270e-01,  3.3420e-01,  7.4604e-01,  ..., -4.8641e-01,\n","          -1.8854e+00, -1.2981e+00],\n","         [ 5.5141e-01,  3.9755e-01,  9.2987e-01,  ..., -4.8438e-01,\n","          -2.5175e+00, -1.5969e+00]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04],\n","         [ 8.1404e-02,  4.8573e-02, -8.7925e-04,  ..., -2.4804e-01,\n","          -4.3339e-01, -5.0369e-01],\n","         [ 1.5141e-01,  9.9779e-02,  2.4383e-02,  ..., -4.4102e-01,\n","          -5.0837e-01, -6.0652e-01],\n","         [ 2.4963e-01,  1.9444e-01,  9.8067e-02,  ..., -8.4590e-01,\n","          -7.4851e-01, -1.0064e+00],\n","         [ 3.2436e-01,  3.1885e-01,  2.0384e-01,  ..., -1.5015e+00,\n","          -1.1447e+00, -1.7440e+00],\n","         [ 2.2056e-01,  4.1194e-01,  3.0528e-01,  ..., -2.0245e+00,\n","          -1.4244e+00, -2.4548e+00]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02],\n","         [ 6.6026e-02, -3.4354e-03, -1.5873e-01,  ..., -9.1847e-01,\n","           1.0178e-01,  7.1390e-02],\n","         [ 1.8310e-01,  4.0924e-01, -5.9317e-02,  ..., -1.6106e+00,\n","           3.6716e-02, -5.1880e-02],\n","         [ 2.3618e-01,  8.3137e-01,  1.8324e-01,  ..., -2.4791e+00,\n","          -1.6332e-01, -2.0249e-01],\n","         [ 1.7206e-01,  1.0822e+00,  4.1728e-01,  ..., -2.9507e+00,\n","          -3.6275e-01, -2.7580e-01],\n","         [ 1.0825e-01,  1.1939e+00,  4.7737e-01,  ..., -3.1215e+00,\n","          -4.5720e-01, -3.5672e-01]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01],\n","         [ 2.5637e-01,  3.0023e-01,  1.6799e-01,  ..., -1.4504e+00,\n","          -4.7145e-01, -8.4290e-01],\n","         [ 3.1954e-01,  6.5962e-01,  4.2673e-01,  ..., -2.3452e+00,\n","          -8.1966e-01, -1.2238e+00],\n","         [ 4.1862e-01,  9.4778e-01,  7.6486e-01,  ..., -2.8339e+00,\n","          -1.1186e+00, -1.1609e+00],\n","         [ 5.0575e-01,  1.0394e+00,  1.0315e+00,  ..., -2.9214e+00,\n","          -1.2903e+00, -9.0396e-01],\n","         [ 5.5112e-01,  9.9703e-01,  1.1678e+00,  ..., -2.8688e+00,\n","          -1.3316e+00, -7.0315e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01],\n","         [-3.2501e-02, -1.4442e-01, -1.6433e-02,  ..., -5.3357e-01,\n","          -7.6458e-01, -1.1134e+00],\n","         [ 3.6241e-02,  5.8171e-02,  2.3958e-01,  ..., -6.9212e-01,\n","          -1.3056e+00, -1.8445e+00],\n","         [ 1.8579e-01,  3.0262e-01,  7.0633e-01,  ..., -7.3066e-01,\n","          -1.9651e+00, -2.3884e+00],\n","         [ 3.2801e-01,  5.1284e-01,  1.1219e+00,  ..., -7.5739e-01,\n","          -2.3393e+00, -2.4797e+00],\n","         [ 4.7219e-01,  6.0915e-01,  1.3272e+00,  ..., -8.4550e-01,\n","          -2.4882e+00, -2.4398e+00]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02],\n","         [-2.3664e-01,  3.4442e-01, -1.9455e-01,  ..., -7.4948e-01,\n","          -8.6498e-02, -5.5521e-01],\n","         [-5.0455e-01,  5.4516e-01, -5.6654e-01,  ..., -9.8211e-01,\n","          -8.5064e-02, -1.1105e+00],\n","         [-9.0218e-01,  7.3457e-01, -9.9823e-01,  ..., -1.0778e+00,\n","           2.9807e-03, -1.6007e+00],\n","         [-1.2954e+00,  8.2626e-01, -1.2244e+00,  ..., -9.0902e-01,\n","           9.5185e-02, -1.7385e+00],\n","         [-1.4885e+00,  8.7569e-01, -1.2303e+00,  ..., -7.2563e-01,\n","           1.0267e-01, -1.6746e+00]]], grad_fn=<CatBackward>)\n","fake: shape :  torch.Size([256, 6, 128])\n","fake: type :  <class 'torch.Tensor'>\n","D(real):  tensor([[0.6686],\n","        [0.7299],\n","        [0.9485],\n","        [0.9459],\n","        [0.8855],\n","        [0.9603],\n","        [0.9661],\n","        [0.9511],\n","        [0.7827],\n","        [0.9548],\n","        [0.8845],\n","        [0.9403],\n","        [0.8763],\n","        [0.9553],\n","        [0.8800],\n","        [0.6402],\n","        [0.9273],\n","        [0.8739],\n","        [0.8784],\n","        [0.9634],\n","        [0.7150],\n","        [0.9678],\n","        [0.9398],\n","        [0.9457],\n","        [0.8206],\n","        [0.9611],\n","        [0.2869],\n","        [0.9285],\n","        [0.4121],\n","        [0.9440],\n","        [0.2768],\n","        [0.9104],\n","        [0.8945],\n","        [0.9521],\n","        [0.9431],\n","        [0.9441],\n","        [0.9065],\n","        [0.9588],\n","        [0.9306],\n","        [0.9037],\n","        [0.9004],\n","        [0.7141],\n","        [0.8713],\n","        [0.9445],\n","        [0.5494],\n","        [0.8787],\n","        [0.9291],\n","        [0.7317],\n","        [0.2767],\n","        [0.7743],\n","        [0.8951],\n","        [0.9600],\n","        [0.9518],\n","        [0.9494],\n","        [0.3437],\n","        [0.9288],\n","        [0.8458],\n","        [0.9279],\n","        [0.9273],\n","        [0.9268],\n","        [0.8591],\n","        [0.9518],\n","        [0.8687],\n","        [0.8976],\n","        [0.9098],\n","        [0.8683],\n","        [0.4704],\n","        [0.8643],\n","        [0.9282],\n","        [0.6172],\n","        [0.7391],\n","        [0.9503],\n","        [0.9328],\n","        [0.9539],\n","        [0.9230],\n","        [0.9251],\n","        [0.9542],\n","        [0.9030],\n","        [0.7015],\n","        [0.4943],\n","        [0.8684],\n","        [0.8856],\n","        [0.9563],\n","        [0.7654],\n","        [0.9181],\n","        [0.9256],\n","        [0.9542],\n","        [0.8741],\n","        [0.8886],\n","        [0.9204],\n","        [0.8901],\n","        [0.9433],\n","        [0.9007],\n","        [0.8388],\n","        [0.3160],\n","        [0.9165],\n","        [0.9468],\n","        [0.8691],\n","        [0.9574],\n","        [0.8351],\n","        [0.9524],\n","        [0.9252],\n","        [0.8647],\n","        [0.3145],\n","        [0.6474],\n","        [0.4371],\n","        [0.8325],\n","        [0.9141],\n","        [0.9627],\n","        [0.7935],\n","        [0.8564],\n","        [0.9625],\n","        [0.8886],\n","        [0.9684],\n","        [0.9221],\n","        [0.9151],\n","        [0.5585],\n","        [0.9531],\n","        [0.8879],\n","        [0.9517],\n","        [0.5186],\n","        [0.9488],\n","        [0.4994],\n","        [0.9579],\n","        [0.4063],\n","        [0.8161],\n","        [0.8508],\n","        [0.9064],\n","        [0.8724],\n","        [0.9417],\n","        [0.9138],\n","        [0.9387],\n","        [0.5330],\n","        [0.9360],\n","        [0.7453],\n","        [0.9032],\n","        [0.9598],\n","        [0.9295],\n","        [0.8655],\n","        [0.8276],\n","        [0.8693],\n","        [0.9577],\n","        [0.7808],\n","        [0.9437],\n","        [0.8612],\n","        [0.8519],\n","        [0.9377],\n","        [0.3331],\n","        [0.9519],\n","        [0.9037],\n","        [0.9392],\n","        [0.7193],\n","        [0.9176],\n","        [0.9389],\n","        [0.9542],\n","        [0.9631],\n","        [0.7061],\n","        [0.9554],\n","        [0.8810],\n","        [0.9576],\n","        [0.9517],\n","        [0.9614],\n","        [0.9556],\n","        [0.9460],\n","        [0.9549],\n","        [0.7657],\n","        [0.7323],\n","        [0.9191],\n","        [0.8761],\n","        [0.8846],\n","        [0.8969],\n","        [0.9367],\n","        [0.7891],\n","        [0.9234],\n","        [0.9211],\n","        [0.8930],\n","        [0.9402],\n","        [0.9240],\n","        [0.9393],\n","        [0.8356],\n","        [0.9260],\n","        [0.8689],\n","        [0.9672],\n","        [0.9595],\n","        [0.9233],\n","        [0.9626],\n","        [0.9550],\n","        [0.9258],\n","        [0.9548],\n","        [0.4080],\n","        [0.9255],\n","        [0.9445],\n","        [0.7582],\n","        [0.8854],\n","        [0.9171],\n","        [0.9533],\n","        [0.9082],\n","        [0.6444],\n","        [0.9297],\n","        [0.9472],\n","        [0.7836],\n","        [0.9386],\n","        [0.9201],\n","        [0.9595],\n","        [0.8532],\n","        [0.8153],\n","        [0.8937],\n","        [0.9585],\n","        [0.9082],\n","        [0.5660],\n","        [0.3168],\n","        [0.7668],\n","        [0.9251],\n","        [0.2664],\n","        [0.9485],\n","        [0.6605],\n","        [0.8542],\n","        [0.8710],\n","        [0.4635],\n","        [0.9598],\n","        [0.9350],\n","        [0.8862],\n","        [0.9633],\n","        [0.8979],\n","        [0.9221],\n","        [0.8846],\n","        [0.9311],\n","        [0.9161],\n","        [0.7324],\n","        [0.9427],\n","        [0.2617],\n","        [0.5681],\n","        [0.3850],\n","        [0.9631],\n","        [0.8420],\n","        [0.9623],\n","        [0.9358],\n","        [0.4926],\n","        [0.9226],\n","        [0.9461],\n","        [0.9497],\n","        [0.8905],\n","        [0.9041],\n","        [0.9244],\n","        [0.9357],\n","        [0.9332],\n","        [0.9057],\n","        [0.9242],\n","        [0.8597],\n","        [0.7274],\n","        [0.9264],\n","        [0.7455],\n","        [0.8919],\n","        [0.9102],\n","        [0.9107],\n","        [0.8856]], grad_fn=<SigmoidBackward>)\n","t1 :  tensor([[0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000]])\n","t1 : shape  torch.Size([256, 1])\n","f1 :  tensor([[0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","[2] D Loss: 0.395  G Loss 1.360  (7.1s)\n","sentences:  <class 'list'>\n","sentences len:  256\n","seq: type  <class 'torch.Tensor'>\n","seq:  shape  torch.Size([256, 41, 128])\n","Seq_lens:  [12, 35, 20, 15, 19, 17, 13, 25, 6, 17, 19, 22, 12, 14, 18, 19, 24, 10, 18, 21, 6, 20, 39, 31, 23, 22, 3, 17, 7, 27, 4, 9, 10, 8, 7, 22, 25, 18, 10, 7, 16, 18, 27, 23, 9, 10, 28, 27, 3, 12, 38, 16, 8, 13, 5, 34, 13, 34, 26, 23, 12, 16, 40, 25, 31, 12, 19, 21, 25, 15, 6, 30, 23, 19, 7, 23, 30, 26, 16, 5, 31, 15, 16, 6, 13, 26, 13, 10, 9, 27, 14, 15, 17, 20, 6, 17, 35, 8, 10, 11, 17, 14, 23, 2, 8, 18, 23, 38, 8, 15, 28, 10, 7, 24, 17, 13, 23, 23, 22, 12, 9, 28, 30, 18, 17, 22, 17, 20, 25, 10, 9, 7, 10, 13, 29, 20, 25, 17, 6, 8, 6, 23, 23, 29, 25, 37, 10, 6, 19, 23, 8, 22, 17, 29, 24, 18, 20, 17, 30, 14, 20, 23, 13, 40, 22, 23, 11, 26, 6, 20, 12, 9, 21, 22, 13, 11, 13, 16, 21, 26, 11, 40, 23, 16, 24, 24, 23, 23, 19, 9, 25, 17, 33, 17, 13, 23, 26, 22, 10, 21, 12, 31, 10, 19, 10, 22, 31, 33, 27, 18, 16, 28, 21, 6, 17, 31, 27, 15, 32, 31, 18, 29, 28, 14, 38, 22, 14, 38, 9, 29, 21, 21, 24, 35, 21, 31, 24, 20, 12, 19, 22, 36, 21, 7, 14, 15, 41, 8, 21, 40, 19, 31, 11, 9, 24, 6]\n","real:  PackedSequence(data=tensor([[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-2.3125e-03, -1.4058e-03,  3.5940e-03,  ..., -4.8735e-02,\n","          6.1841e-03,  5.8689e-03],\n","        ...,\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01]]), batch_sizes=tensor([256, 256, 255, 253, 252, 250, 239, 232, 224, 215, 202, 197, 188, 177,\n","        170, 163, 155, 140, 131, 121, 112, 101,  89,  71,  63,  55,  49,  43,\n","         38,  33,  29,  20,  19,  17,  15,  12,  11,  10,   6,   5,   1]), sorted_indices=tensor([246,  62, 249, 163, 181,  22,  50, 107, 227, 224, 145, 241,   1,  96,\n","        233,  55,  57, 207, 192, 218,  64, 235,  80, 251, 219, 201, 215, 206,\n","         23,  71,  76, 122, 158, 229, 134, 143, 153, 221, 211, 222, 121, 110,\n","         46,  47, 216,  89,  42,  29, 208,  58,  85, 167,  77, 179, 196,  63,\n","        128,  68, 136, 144, 190,   7,  36, 154, 184, 185, 113, 236, 254,  16,\n","        232, 102, 165, 106,  43, 142, 161, 141, 182, 149, 186, 187,  75,  72,\n","        195,  24, 116, 117,  59, 205,  11,  35, 118, 225, 151, 125, 240,  25,\n","        197, 173, 164, 231, 199, 242, 212,  19, 172, 230, 248, 178, 234,  67,\n","         21, 127, 156, 135,   2, 169, 237,  93, 160, 188,  10, 148,  15, 203,\n","          4,  66, 239,  73, 250, 123,  14, 220, 105,  37,  18, 155, 209,  41,\n","        152, 124,  95, 157,   5,  92,  27, 126, 214, 100, 114, 137,   9, 191,\n","        193, 177,  78, 183,  51,  61,  40,  82, 210, 217, 245,  69,  91,  81,\n","        109,   3, 244, 223,  13, 101,  90, 226, 159, 133, 115, 194,   6,  53,\n","        162,  86, 176,  56, 174,  84,   0,  65, 200,  60, 170, 119,  12,  49,\n","        238,  99, 252, 180, 166, 175,  17,  32, 202,  87,  45,  98, 111, 129,\n","        132,  38, 146, 204, 198, 253,  88, 120, 189, 228, 130, 171,  31,  44,\n","        139, 247, 104,  97,  52, 150,  33, 108,  74, 243, 112,  39, 131,  28,\n","         34,  83,  70,  20,  94, 255,   8, 138, 140, 168, 147, 213,  54,  79,\n","         30,  48,  26, 103]), unsorted_indices=tensor([188,  12, 116, 169, 126, 144, 180,  61, 244, 152, 122,  90, 194, 172,\n","        132, 124,  69, 202, 136, 105, 241, 112,   5,  28,  85,  97, 254, 146,\n","        237,  47, 252, 222, 203, 230, 238,  91,  62, 135, 211, 235, 160, 139,\n","         46,  74, 223, 206,  42,  43, 253, 195,   6, 158, 228, 181, 250,  15,\n","        185,  16,  49,  88, 191, 159,   1,  55,  20, 189, 127, 111,  57, 165,\n","        240,  29,  83, 129, 232,  82,  30,  52, 156, 251,  22, 167, 161, 239,\n","        187,  50, 183, 205, 216,  45, 174, 166, 145, 119, 242, 142,  13, 227,\n","        207, 197, 149, 173,  71, 255, 226, 134,  73,   7, 231, 168,  41, 208,\n","        234,  66, 150, 178,  86,  87,  92, 193, 217,  40,  31, 131, 141,  95,\n","        147, 113,  56, 209, 220, 236, 210, 177,  34, 115,  58, 151, 245, 224,\n","        246,  77,  75,  35,  59,  10, 212, 248, 123,  79, 229,  94, 140,  36,\n","         63, 137, 114, 143,  32, 176, 120,  76, 182,   3, 100,  72, 200,  51,\n","        247, 117, 192, 221, 106,  99, 186, 201, 184, 155, 109,  53, 199,   4,\n","         78, 157,  64,  65,  80,  81, 121, 218,  60, 153,  18, 154, 179,  84,\n","         54,  98, 214, 102, 190,  25, 204, 125, 213,  89,  27,  17,  48, 138,\n","        162,  38, 104, 249, 148,  26,  44, 163,  19,  24, 133,  37,  39, 171,\n","          9,  93, 175,   8, 219,  33, 107, 101,  70,  14, 110,  21,  67, 118,\n","        196, 128,  96,  11, 103, 233, 170, 164,   0, 225, 108,   2, 130,  23,\n","        198, 215,  68, 243]))\n","real: type:   <class 'torch.nn.utils.rnn.PackedSequence'>\n","real: shape:   4\n","greal:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02]]])\n","greal: shape :  torch.Size([256, 1, 128])\n","greal: type :  <class 'torch.Tensor'>\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","fake:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03],\n","         [ 1.3562e-01,  2.3480e-01, -5.0177e-02,  ..., -2.6992e-01,\n","          -4.4370e-02, -3.5999e-02],\n","         [ 1.7648e-01,  4.0674e-01, -3.0819e-03,  ..., -4.4728e-01,\n","          -2.0200e-01, -2.9559e-02],\n","         [ 3.4749e-01,  7.0490e-01,  3.9481e-03,  ..., -7.3125e-01,\n","          -4.1429e-01, -5.7983e-02],\n","         [ 5.8833e-01,  1.0640e+00, -3.4804e-02,  ..., -1.0901e+00,\n","          -6.3450e-01, -1.0402e-01],\n","         [ 7.5423e-01,  1.3131e+00, -1.4055e-01,  ..., -1.3647e+00,\n","          -7.8922e-01, -1.5315e-01]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04],\n","         [-1.9467e-01,  5.3281e-02,  9.3082e-02,  ..., -1.9617e-02,\n","           1.0793e-01, -9.6710e-02],\n","         [-2.1687e-01,  1.3354e-01,  1.7702e-01,  ..., -1.2190e-01,\n","           8.3866e-02, -6.0150e-02],\n","         [-3.2786e-01,  2.5140e-01,  3.3892e-01,  ..., -2.8840e-01,\n","           9.6027e-02, -9.7182e-02],\n","         [-5.8137e-01,  3.4849e-01,  6.0466e-01,  ..., -5.1571e-01,\n","           1.0838e-01, -2.3213e-01],\n","         [-8.9788e-01,  3.0613e-01,  8.8595e-01,  ..., -7.1002e-01,\n","           1.3139e-01, -4.3940e-01]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02],\n","         [ 1.5109e-01, -3.0566e-02,  1.6197e-01,  ..., -7.8573e-01,\n","          -3.0717e-01, -4.0012e-01],\n","         [ 7.3922e-02,  6.2798e-02,  3.0566e-01,  ..., -1.3126e+00,\n","          -6.8008e-01, -1.0642e+00],\n","         [ 8.7943e-02,  2.5244e-01,  4.7646e-01,  ..., -1.8334e+00,\n","          -1.2232e+00, -1.8600e+00],\n","         [ 2.3088e-01,  4.0761e-01,  6.8097e-01,  ..., -2.0142e+00,\n","          -1.6488e+00, -2.3719e+00],\n","         [ 3.7505e-01,  5.0235e-01,  8.8512e-01,  ..., -2.0299e+00,\n","          -1.9284e+00, -2.5410e+00]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01],\n","         [-1.3939e-01,  4.1204e-01,  1.8668e-01,  ..., -5.2012e-01,\n","          -2.1285e-01, -8.8947e-01],\n","         [-4.6646e-01,  5.6094e-01,  1.9097e-01,  ..., -3.7694e-01,\n","          -4.6224e-01, -1.2787e+00],\n","         [-8.2243e-01,  6.5194e-01,  2.8408e-01,  ..., -3.7423e-02,\n","          -8.7756e-01, -1.4893e+00],\n","         [-9.9657e-01,  6.5286e-01,  4.3931e-01,  ...,  3.7576e-01,\n","          -1.1414e+00, -1.4487e+00],\n","         [-1.0180e+00,  5.9147e-01,  5.8753e-01,  ...,  7.1073e-01,\n","          -1.2879e+00, -1.2519e+00]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01],\n","         [-7.0568e-01,  1.8289e-01,  4.1346e-02,  ..., -7.9920e-01,\n","          -1.7248e-01, -6.6730e-01],\n","         [-9.2891e-01,  2.7244e-01,  1.5622e-01,  ..., -1.0143e+00,\n","          -3.9962e-01, -1.1995e+00],\n","         [-1.0186e+00,  3.4102e-01,  2.5819e-01,  ..., -1.0347e+00,\n","          -7.6732e-01, -1.5419e+00],\n","         [-9.8277e-01,  3.4660e-01,  3.7885e-01,  ..., -9.1718e-01,\n","          -1.0708e+00, -1.6695e+00],\n","         [-9.0358e-01,  3.3651e-01,  5.6011e-01,  ..., -7.2806e-01,\n","          -1.2587e+00, -1.7666e+00]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02],\n","         [-2.9226e-01,  1.3761e-01,  4.2637e-03,  ..., -1.4466e-01,\n","          -2.8901e-01, -4.7739e-01],\n","         [-5.3030e-01,  1.9568e-01, -6.6098e-03,  ..., -4.7271e-02,\n","          -3.9898e-01, -8.4052e-01],\n","         [-9.5623e-01,  2.4239e-01, -1.5946e-01,  ...,  1.0319e-01,\n","          -5.6002e-01, -1.2146e+00],\n","         [-1.3326e+00,  2.4977e-01, -3.6334e-01,  ...,  3.3577e-01,\n","          -5.7485e-01, -1.2680e+00],\n","         [-1.5381e+00,  2.5696e-01, -3.5041e-01,  ...,  6.3084e-01,\n","          -5.1128e-01, -1.0435e+00]]], grad_fn=<CatBackward>)\n","fake: shape :  torch.Size([256, 6, 128])\n","fake: type :  <class 'torch.Tensor'>\n","D(real):  tensor([[0.6727],\n","        [0.7315],\n","        [0.9502],\n","        [0.9490],\n","        [0.8871],\n","        [0.9618],\n","        [0.9667],\n","        [0.9501],\n","        [0.7803],\n","        [0.9555],\n","        [0.8863],\n","        [0.9423],\n","        [0.8804],\n","        [0.9567],\n","        [0.8812],\n","        [0.6450],\n","        [0.9299],\n","        [0.8719],\n","        [0.8791],\n","        [0.9639],\n","        [0.7323],\n","        [0.9694],\n","        [0.9425],\n","        [0.9461],\n","        [0.8266],\n","        [0.9608],\n","        [0.2818],\n","        [0.9320],\n","        [0.4335],\n","        [0.9447],\n","        [0.2709],\n","        [0.9119],\n","        [0.8973],\n","        [0.9560],\n","        [0.9483],\n","        [0.9460],\n","        [0.9087],\n","        [0.9584],\n","        [0.9343],\n","        [0.9067],\n","        [0.9014],\n","        [0.7199],\n","        [0.8746],\n","        [0.9465],\n","        [0.5507],\n","        [0.8747],\n","        [0.9344],\n","        [0.7378],\n","        [0.2720],\n","        [0.7780],\n","        [0.8970],\n","        [0.9601],\n","        [0.9537],\n","        [0.9521],\n","        [0.3471],\n","        [0.9288],\n","        [0.8425],\n","        [0.9303],\n","        [0.9303],\n","        [0.9285],\n","        [0.8670],\n","        [0.9532],\n","        [0.8647],\n","        [0.9013],\n","        [0.9160],\n","        [0.8689],\n","        [0.4741],\n","        [0.8705],\n","        [0.9306],\n","        [0.6241],\n","        [0.7630],\n","        [0.9502],\n","        [0.9350],\n","        [0.9567],\n","        [0.9310],\n","        [0.9265],\n","        [0.9563],\n","        [0.9047],\n","        [0.7108],\n","        [0.4907],\n","        [0.8671],\n","        [0.8912],\n","        [0.9566],\n","        [0.7789],\n","        [0.9223],\n","        [0.9300],\n","        [0.9554],\n","        [0.8715],\n","        [0.8926],\n","        [0.9223],\n","        [0.8933],\n","        [0.9474],\n","        [0.9025],\n","        [0.8449],\n","        [0.3243],\n","        [0.9176],\n","        [0.9493],\n","        [0.8702],\n","        [0.9584],\n","        [0.8354],\n","        [0.9534],\n","        [0.9276],\n","        [0.8613],\n","        [0.3105],\n","        [0.6625],\n","        [0.4427],\n","        [0.8365],\n","        [0.9151],\n","        [0.9613],\n","        [0.7965],\n","        [0.8584],\n","        [0.9620],\n","        [0.8937],\n","        [0.9691],\n","        [0.9232],\n","        [0.9214],\n","        [0.5634],\n","        [0.9555],\n","        [0.8917],\n","        [0.9515],\n","        [0.5351],\n","        [0.9530],\n","        [0.5011],\n","        [0.9595],\n","        [0.4079],\n","        [0.8303],\n","        [0.8482],\n","        [0.9052],\n","        [0.8742],\n","        [0.9429],\n","        [0.9134],\n","        [0.9405],\n","        [0.5344],\n","        [0.9380],\n","        [0.7519],\n","        [0.9047],\n","        [0.9615],\n","        [0.9334],\n","        [0.8733],\n","        [0.8370],\n","        [0.8841],\n","        [0.9588],\n","        [0.7833],\n","        [0.9459],\n","        [0.8666],\n","        [0.8543],\n","        [0.9409],\n","        [0.3441],\n","        [0.9514],\n","        [0.9057],\n","        [0.9416],\n","        [0.7136],\n","        [0.9196],\n","        [0.9420],\n","        [0.9550],\n","        [0.9639],\n","        [0.7030],\n","        [0.9556],\n","        [0.8954],\n","        [0.9576],\n","        [0.9530],\n","        [0.9636],\n","        [0.9561],\n","        [0.9487],\n","        [0.9552],\n","        [0.7740],\n","        [0.7384],\n","        [0.9246],\n","        [0.8828],\n","        [0.8887],\n","        [0.8947],\n","        [0.9388],\n","        [0.7945],\n","        [0.9262],\n","        [0.9244],\n","        [0.8931],\n","        [0.9436],\n","        [0.9263],\n","        [0.9424],\n","        [0.8497],\n","        [0.9294],\n","        [0.8686],\n","        [0.9684],\n","        [0.9585],\n","        [0.9250],\n","        [0.9627],\n","        [0.9558],\n","        [0.9261],\n","        [0.9554],\n","        [0.4196],\n","        [0.9270],\n","        [0.9466],\n","        [0.7628],\n","        [0.8873],\n","        [0.9164],\n","        [0.9546],\n","        [0.9085],\n","        [0.6522],\n","        [0.9344],\n","        [0.9484],\n","        [0.7851],\n","        [0.9405],\n","        [0.9209],\n","        [0.9603],\n","        [0.8610],\n","        [0.8197],\n","        [0.8925],\n","        [0.9588],\n","        [0.9166],\n","        [0.5682],\n","        [0.3148],\n","        [0.7686],\n","        [0.9250],\n","        [0.2664],\n","        [0.9491],\n","        [0.6648],\n","        [0.8633],\n","        [0.8709],\n","        [0.4679],\n","        [0.9610],\n","        [0.9387],\n","        [0.8979],\n","        [0.9645],\n","        [0.8994],\n","        [0.9279],\n","        [0.8874],\n","        [0.9321],\n","        [0.9179],\n","        [0.7385],\n","        [0.9425],\n","        [0.2568],\n","        [0.5681],\n","        [0.3856],\n","        [0.9647],\n","        [0.8419],\n","        [0.9629],\n","        [0.9393],\n","        [0.4928],\n","        [0.9248],\n","        [0.9448],\n","        [0.9503],\n","        [0.8956],\n","        [0.9040],\n","        [0.9248],\n","        [0.9387],\n","        [0.9344],\n","        [0.9074],\n","        [0.9295],\n","        [0.8603],\n","        [0.7316],\n","        [0.9308],\n","        [0.7526],\n","        [0.8941],\n","        [0.9109],\n","        [0.9109],\n","        [0.8855]], grad_fn=<SigmoidBackward>)\n","t1 :  tensor([[0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000]])\n","t1 : shape  torch.Size([256, 1])\n","f1 :  tensor([[0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","[2] D Loss: 0.393  G Loss 1.373  (7.0s)\n","sentences:  <class 'list'>\n","sentences len:  256\n","seq: type  <class 'torch.Tensor'>\n","seq:  shape  torch.Size([256, 41, 128])\n","Seq_lens:  [12, 35, 20, 15, 19, 17, 13, 25, 6, 17, 19, 22, 12, 14, 18, 19, 24, 10, 18, 21, 6, 20, 39, 31, 23, 22, 3, 17, 7, 27, 4, 9, 10, 8, 7, 22, 25, 18, 10, 7, 16, 18, 27, 23, 9, 10, 28, 27, 3, 12, 38, 16, 8, 13, 5, 34, 13, 34, 26, 23, 12, 16, 40, 25, 31, 12, 19, 21, 25, 15, 6, 30, 23, 19, 7, 23, 30, 26, 16, 5, 31, 15, 16, 6, 13, 26, 13, 10, 9, 27, 14, 15, 17, 20, 6, 17, 35, 8, 10, 11, 17, 14, 23, 2, 8, 18, 23, 38, 8, 15, 28, 10, 7, 24, 17, 13, 23, 23, 22, 12, 9, 28, 30, 18, 17, 22, 17, 20, 25, 10, 9, 7, 10, 13, 29, 20, 25, 17, 6, 8, 6, 23, 23, 29, 25, 37, 10, 6, 19, 23, 8, 22, 17, 29, 24, 18, 20, 17, 30, 14, 20, 23, 13, 40, 22, 23, 11, 26, 6, 20, 12, 9, 21, 22, 13, 11, 13, 16, 21, 26, 11, 40, 23, 16, 24, 24, 23, 23, 19, 9, 25, 17, 33, 17, 13, 23, 26, 22, 10, 21, 12, 31, 10, 19, 10, 22, 31, 33, 27, 18, 16, 28, 21, 6, 17, 31, 27, 15, 32, 31, 18, 29, 28, 14, 38, 22, 14, 38, 9, 29, 21, 21, 24, 35, 21, 31, 24, 20, 12, 19, 22, 36, 21, 7, 14, 15, 41, 8, 21, 40, 19, 31, 11, 9, 24, 6]\n","real:  PackedSequence(data=tensor([[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","          2.3435e-01,  1.2447e-01],\n","        [-2.3125e-03, -1.4058e-03,  3.5940e-03,  ..., -4.8735e-02,\n","          6.1841e-03,  5.8689e-03],\n","        ...,\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01],\n","        [-1.5283e-01, -3.6613e-02,  2.1768e-01,  ..., -2.8116e+00,\n","          2.9199e-01,  1.4563e-01]]), batch_sizes=tensor([256, 256, 255, 253, 252, 250, 239, 232, 224, 215, 202, 197, 188, 177,\n","        170, 163, 155, 140, 131, 121, 112, 101,  89,  71,  63,  55,  49,  43,\n","         38,  33,  29,  20,  19,  17,  15,  12,  11,  10,   6,   5,   1]), sorted_indices=tensor([246,  62, 249, 163, 181,  22,  50, 107, 227, 224, 145, 241,   1,  96,\n","        233,  55,  57, 207, 192, 218,  64, 235,  80, 251, 219, 201, 215, 206,\n","         23,  71,  76, 122, 158, 229, 134, 143, 153, 221, 211, 222, 121, 110,\n","         46,  47, 216,  89,  42,  29, 208,  58,  85, 167,  77, 179, 196,  63,\n","        128,  68, 136, 144, 190,   7,  36, 154, 184, 185, 113, 236, 254,  16,\n","        232, 102, 165, 106,  43, 142, 161, 141, 182, 149, 186, 187,  75,  72,\n","        195,  24, 116, 117,  59, 205,  11,  35, 118, 225, 151, 125, 240,  25,\n","        197, 173, 164, 231, 199, 242, 212,  19, 172, 230, 248, 178, 234,  67,\n","         21, 127, 156, 135,   2, 169, 237,  93, 160, 188,  10, 148,  15, 203,\n","          4,  66, 239,  73, 250, 123,  14, 220, 105,  37,  18, 155, 209,  41,\n","        152, 124,  95, 157,   5,  92,  27, 126, 214, 100, 114, 137,   9, 191,\n","        193, 177,  78, 183,  51,  61,  40,  82, 210, 217, 245,  69,  91,  81,\n","        109,   3, 244, 223,  13, 101,  90, 226, 159, 133, 115, 194,   6,  53,\n","        162,  86, 176,  56, 174,  84,   0,  65, 200,  60, 170, 119,  12,  49,\n","        238,  99, 252, 180, 166, 175,  17,  32, 202,  87,  45,  98, 111, 129,\n","        132,  38, 146, 204, 198, 253,  88, 120, 189, 228, 130, 171,  31,  44,\n","        139, 247, 104,  97,  52, 150,  33, 108,  74, 243, 112,  39, 131,  28,\n","         34,  83,  70,  20,  94, 255,   8, 138, 140, 168, 147, 213,  54,  79,\n","         30,  48,  26, 103]), unsorted_indices=tensor([188,  12, 116, 169, 126, 144, 180,  61, 244, 152, 122,  90, 194, 172,\n","        132, 124,  69, 202, 136, 105, 241, 112,   5,  28,  85,  97, 254, 146,\n","        237,  47, 252, 222, 203, 230, 238,  91,  62, 135, 211, 235, 160, 139,\n","         46,  74, 223, 206,  42,  43, 253, 195,   6, 158, 228, 181, 250,  15,\n","        185,  16,  49,  88, 191, 159,   1,  55,  20, 189, 127, 111,  57, 165,\n","        240,  29,  83, 129, 232,  82,  30,  52, 156, 251,  22, 167, 161, 239,\n","        187,  50, 183, 205, 216,  45, 174, 166, 145, 119, 242, 142,  13, 227,\n","        207, 197, 149, 173,  71, 255, 226, 134,  73,   7, 231, 168,  41, 208,\n","        234,  66, 150, 178,  86,  87,  92, 193, 217,  40,  31, 131, 141,  95,\n","        147, 113,  56, 209, 220, 236, 210, 177,  34, 115,  58, 151, 245, 224,\n","        246,  77,  75,  35,  59,  10, 212, 248, 123,  79, 229,  94, 140,  36,\n","         63, 137, 114, 143,  32, 176, 120,  76, 182,   3, 100,  72, 200,  51,\n","        247, 117, 192, 221, 106,  99, 186, 201, 184, 155, 109,  53, 199,   4,\n","         78, 157,  64,  65,  80,  81, 121, 218,  60, 153,  18, 154, 179,  84,\n","         54,  98, 214, 102, 190,  25, 204, 125, 213,  89,  27,  17,  48, 138,\n","        162,  38, 104, 249, 148,  26,  44, 163,  19,  24, 133,  37,  39, 171,\n","          9,  93, 175,   8, 219,  33, 107, 101,  70,  14, 110,  21,  67, 118,\n","        196, 128,  96,  11, 103, 233, 170, 164,   0, 225, 108,   2, 130,  23,\n","        198, 215,  68, 243]))\n","real: type:   <class 'torch.nn.utils.rnn.PackedSequence'>\n","real: shape:   4\n","greal:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02]]])\n","greal: shape :  torch.Size([256, 1, 128])\n","greal: type :  <class 'torch.Tensor'>\n","one_hot_size type:  <class 'int'>\n"," one_hot_size shape :  17\n","out_size type:  <class 'int'>\n"," out_size shape :  128\n","h_n type:  <class 'torch.Tensor'>\n","h_n shape :  torch.Size([1, 256, 145])\n","c_n type:  <class 'torch.Tensor'>\n","c_n shape :  torch.Size([1, 256, 145])\n","batch type:  <class 'torch.Tensor'>\n","batch shape :  torch.Size([256, 1, 128])\n","one_hot shape:  torch.Size([256, 1, 17])\n","one_hot shape: after  torch.Size([256, 1, 17])\n","x_n shape :  torch.Size([256, 1, 145])\n","sentence : Type  <class 'list'>\n","sentence : Len  1\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n","sentence_len:  5\n"," h_n shape :  torch.Size([256, 6, 128])\n","fake:  tensor([[[-5.7667e-03,  1.0972e-03,  9.9342e-03,  ..., -1.5081e-01,\n","           1.8071e-02,  7.2120e-03],\n","         [ 1.0319e-01, -1.7591e-01,  2.2725e-01,  ..., -1.2894e-01,\n","          -3.7037e-01, -1.6777e-01],\n","         [ 3.5528e-01, -9.7841e-02,  5.2592e-01,  ..., -3.2340e-01,\n","          -6.4382e-01, -2.2624e-01],\n","         [ 7.7930e-01, -5.5183e-02,  9.1193e-01,  ..., -6.4406e-01,\n","          -1.0541e+00, -3.9216e-01],\n","         [ 1.3048e+00, -1.8514e-02,  1.3554e+00,  ..., -1.0806e+00,\n","          -1.4883e+00, -5.5747e-01],\n","         [ 1.6826e+00, -3.0151e-03,  1.5940e+00,  ..., -1.4563e+00,\n","          -1.6846e+00, -6.2706e-01]],\n","\n","        [[-4.1601e-03, -1.8149e-03,  3.5406e-03,  ..., -7.8308e-02,\n","           1.0556e-02,  7.2568e-04],\n","         [ 1.9579e-01,  1.2535e-02, -2.1430e-02,  ...,  8.4874e-03,\n","          -2.4402e-01, -1.4886e-01],\n","         [ 2.2334e-01, -1.7549e-02, -1.0816e-02,  ...,  1.5805e-03,\n","          -3.5224e-01, -1.3794e-01],\n","         [ 3.3104e-01,  5.5305e-03, -3.4660e-02,  ..., -3.2316e-02,\n","          -5.2368e-01, -2.1241e-01],\n","         [ 4.8212e-01,  5.3886e-02, -7.2607e-02,  ..., -4.7689e-02,\n","          -7.3653e-01, -3.3350e-01],\n","         [ 6.1781e-01,  1.2540e-01, -1.1051e-01,  ..., -4.6736e-02,\n","          -8.9945e-01, -4.5923e-01]],\n","\n","        [[-4.3799e-02, -1.5880e-02,  6.6912e-02,  ..., -8.8892e-01,\n","           9.4974e-02,  4.9282e-02],\n","         [-3.1816e-01,  1.8276e-01, -3.2796e-01,  ..., -4.4319e-01,\n","           1.3230e-01, -4.4468e-01],\n","         [-6.6385e-01,  4.0600e-01, -5.0550e-01,  ..., -4.8543e-01,\n","           1.6369e-01, -8.4919e-01],\n","         [-1.1880e+00,  5.8777e-01, -7.4863e-01,  ..., -4.9137e-01,\n","           3.4550e-01, -1.2894e+00],\n","         [-1.7531e+00,  6.5911e-01, -8.3230e-01,  ..., -3.6815e-01,\n","           4.9753e-01, -1.4954e+00],\n","         [-2.1888e+00,  5.8990e-01, -7.4474e-01,  ..., -2.7734e-01,\n","           5.9259e-01, -1.4441e+00]],\n","\n","        ...,\n","\n","        [[-1.1657e-01, -2.2561e-02,  1.5979e-01,  ..., -2.1489e+00,\n","           2.3435e-01,  1.2447e-01],\n","         [-3.5728e-01, -1.4631e-01, -1.9834e-01,  ..., -8.0750e-01,\n","          -6.9742e-01, -1.5078e+00],\n","         [-4.3566e-01, -2.3855e-01, -1.6245e-01,  ..., -7.9009e-01,\n","          -1.2840e+00, -2.3393e+00],\n","         [-4.8739e-01, -3.9166e-01, -1.9644e-02,  ..., -6.4562e-01,\n","          -1.8134e+00, -2.8696e+00],\n","         [-5.2430e-01, -4.8416e-01,  8.4675e-02,  ..., -5.9918e-01,\n","          -2.0631e+00, -3.0164e+00],\n","         [-5.5094e-01, -4.9326e-01,  1.2098e-01,  ..., -6.1857e-01,\n","          -2.1578e+00, -2.9991e+00]],\n","\n","        [[-1.2480e-01, -6.2331e-02,  1.4169e-01,  ..., -2.4589e+00,\n","           2.5898e-01,  1.5619e-01],\n","         [-1.2176e-01,  6.0814e-02, -1.1392e-01,  ..., -8.6047e-01,\n","          -3.3605e-01, -1.0628e+00],\n","         [-1.3559e-01,  4.7577e-02,  4.9858e-02,  ..., -1.3330e+00,\n","          -6.4602e-01, -1.8451e+00],\n","         [-1.2647e-01, -1.3501e-02,  2.6353e-01,  ..., -1.4744e+00,\n","          -1.0030e+00, -2.4333e+00],\n","         [-2.3563e-01, -5.7063e-02,  3.8702e-01,  ..., -1.3494e+00,\n","          -1.2444e+00, -2.6495e+00],\n","         [-3.2519e-01, -1.0114e-01,  4.3196e-01,  ..., -1.2168e+00,\n","          -1.3420e+00, -2.6759e+00]],\n","\n","        [[-6.3028e-02, -1.3355e-02,  7.3182e-02,  ..., -1.1417e+00,\n","           1.2054e-01,  6.7849e-02],\n","         [-2.2928e-01,  1.8052e-01, -1.9684e-01,  ..., -4.2829e-01,\n","           1.0590e-01, -3.6747e-01],\n","         [-5.2014e-01,  3.5216e-01, -2.9060e-01,  ..., -7.2670e-01,\n","           2.5177e-01, -6.0172e-01],\n","         [-8.7593e-01,  5.0804e-01, -3.8461e-01,  ..., -1.1182e+00,\n","           3.6383e-01, -1.0053e+00],\n","         [-1.0456e+00,  4.9318e-01, -3.3181e-01,  ..., -1.2387e+00,\n","           3.6686e-01, -1.3285e+00],\n","         [-1.0353e+00,  3.3313e-01, -2.2028e-01,  ..., -1.1301e+00,\n","           2.5642e-01, -1.5215e+00]]], grad_fn=<CatBackward>)\n","fake: shape :  torch.Size([256, 6, 128])\n","fake: type :  <class 'torch.Tensor'>\n","D(real):  tensor([[0.6699],\n","        [0.7252],\n","        [0.9478],\n","        [0.9478],\n","        [0.8824],\n","        [0.9600],\n","        [0.9643],\n","        [0.9445],\n","        [0.7744],\n","        [0.9522],\n","        [0.8823],\n","        [0.9396],\n","        [0.8788],\n","        [0.9545],\n","        [0.8753],\n","        [0.6436],\n","        [0.9269],\n","        [0.8614],\n","        [0.8726],\n","        [0.9609],\n","        [0.7454],\n","        [0.9682],\n","        [0.9410],\n","        [0.9421],\n","        [0.8238],\n","        [0.9569],\n","        [0.2769],\n","        [0.9306],\n","        [0.4551],\n","        [0.9406],\n","        [0.2652],\n","        [0.9084],\n","        [0.8945],\n","        [0.9560],\n","        [0.9506],\n","        [0.9433],\n","        [0.9047],\n","        [0.9542],\n","        [0.9327],\n","        [0.9053],\n","        [0.8972],\n","        [0.7182],\n","        [0.8706],\n","        [0.9439],\n","        [0.5448],\n","        [0.8632],\n","        [0.9348],\n","        [0.7364],\n","        [0.2674],\n","        [0.7737],\n","        [0.8927],\n","        [0.9566],\n","        [0.9519],\n","        [0.9508],\n","        [0.3514],\n","        [0.9228],\n","        [0.8318],\n","        [0.9275],\n","        [0.9281],\n","        [0.9255],\n","        [0.8675],\n","        [0.9504],\n","        [0.8530],\n","        [0.8995],\n","        [0.9168],\n","        [0.8627],\n","        [0.4733],\n","        [0.8700],\n","        [0.9277],\n","        [0.6228],\n","        [0.7834],\n","        [0.9458],\n","        [0.9326],\n","        [0.9558],\n","        [0.9352],\n","        [0.9224],\n","        [0.9546],\n","        [0.9014],\n","        [0.7125],\n","        [0.4868],\n","        [0.8590],\n","        [0.8897],\n","        [0.9530],\n","        [0.7863],\n","        [0.9211],\n","        [0.9301],\n","        [0.9527],\n","        [0.8591],\n","        [0.8897],\n","        [0.9184],\n","        [0.8897],\n","        [0.9473],\n","        [0.8982],\n","        [0.8426],\n","        [0.3340],\n","        [0.9125],\n","        [0.9474],\n","        [0.8630],\n","        [0.9559],\n","        [0.8275],\n","        [0.9502],\n","        [0.9248],\n","        [0.8507],\n","        [0.3066],\n","        [0.6702],\n","        [0.4444],\n","        [0.8334],\n","        [0.9107],\n","        [0.9569],\n","        [0.7924],\n","        [0.8539],\n","        [0.9579],\n","        [0.8932],\n","        [0.9668],\n","        [0.9194],\n","        [0.9216],\n","        [0.5621],\n","        [0.9539],\n","        [0.8900],\n","        [0.9470],\n","        [0.5478],\n","        [0.9529],\n","        [0.4969],\n","        [0.9577],\n","        [0.4063],\n","        [0.8361],\n","        [0.8368],\n","        [0.8989],\n","        [0.8699],\n","        [0.9391],\n","        [0.9060],\n","        [0.9393],\n","        [0.5300],\n","        [0.9360],\n","        [0.7506],\n","        [0.9002],\n","        [0.9597],\n","        [0.9325],\n","        [0.8767],\n","        [0.8398],\n","        [0.8950],\n","        [0.9565],\n","        [0.7783],\n","        [0.9442],\n","        [0.8652],\n","        [0.8505],\n","        [0.9401],\n","        [0.3567],\n","        [0.9466],\n","        [0.9010],\n","        [0.9401],\n","        [0.6982],\n","        [0.9159],\n","        [0.9406],\n","        [0.9517],\n","        [0.9615],\n","        [0.6911],\n","        [0.9519],\n","        [0.9023],\n","        [0.9538],\n","        [0.9504],\n","        [0.9626],\n","        [0.9526],\n","        [0.9473],\n","        [0.9516],\n","        [0.7753],\n","        [0.7360],\n","        [0.9245],\n","        [0.8855],\n","        [0.8863],\n","        [0.8860],\n","        [0.9360],\n","        [0.7926],\n","        [0.9229],\n","        [0.9225],\n","        [0.8876],\n","        [0.9425],\n","        [0.9240],\n","        [0.9411],\n","        [0.8557],\n","        [0.9276],\n","        [0.8608],\n","        [0.9666],\n","        [0.9534],\n","        [0.9215],\n","        [0.9591],\n","        [0.9527],\n","        [0.9206],\n","        [0.9521],\n","        [0.4298],\n","        [0.9232],\n","        [0.9444],\n","        [0.7597],\n","        [0.8829],\n","        [0.9091],\n","        [0.9521],\n","        [0.9035],\n","        [0.6526],\n","        [0.9341],\n","        [0.9450],\n","        [0.7780],\n","        [0.9373],\n","        [0.9157],\n","        [0.9574],\n","        [0.8600],\n","        [0.8151],\n","        [0.8855],\n","        [0.9554],\n","        [0.9193],\n","        [0.5642],\n","        [0.3116],\n","        [0.7627],\n","        [0.9192],\n","        [0.2676],\n","        [0.9452],\n","        [0.6624],\n","        [0.8652],\n","        [0.8635],\n","        [0.4670],\n","        [0.9589],\n","        [0.9373],\n","        [0.9026],\n","        [0.9624],\n","        [0.8935],\n","        [0.9283],\n","        [0.8841],\n","        [0.9279],\n","        [0.9140],\n","        [0.7355],\n","        [0.9374],\n","        [0.2519],\n","        [0.5611],\n","        [0.3833],\n","        [0.9632],\n","        [0.8349],\n","        [0.9600],\n","        [0.9382],\n","        [0.4878],\n","        [0.9211],\n","        [0.9385],\n","        [0.9468],\n","        [0.8950],\n","        [0.8960],\n","        [0.9215],\n","        [0.9368],\n","        [0.9311],\n","        [0.9030],\n","        [0.9300],\n","        [0.8544],\n","        [0.7264],\n","        [0.9302],\n","        [0.7522],\n","        [0.8913],\n","        [0.9071],\n","        [0.9057],\n","        [0.8823]], grad_fn=<SigmoidBackward>)\n","t1 :  tensor([[0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000],\n","        [0.9000]])\n","t1 : shape  torch.Size([256, 1])\n","f1 :  tensor([[0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000],\n","        [0.1000]])\n","f1 : shape  torch.Size([256, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V1Rp_5jFXMWS"},"source":[""],"execution_count":null,"outputs":[]}]}